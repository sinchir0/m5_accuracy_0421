{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_CASE = \"case8\"\n",
    "CASE = \"case26\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from local_para import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed固定\n",
    "import os, sys, gc, warnings, random, datetime\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "SEED = 2020\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/input/train.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_ORG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/input/test.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_ORG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_ORG_PATH)\n",
    "test = pd.read_csv(TEST_ORG_PATH)\n",
    "\n",
    "SAME_PATH = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/\"\n",
    "category = pd.read_csv(f\"{SAME_PATH}\" + \"Users/td017/kaggle-pipeline/input/category.csv\")\n",
    "jan = pd.read_csv(f\"{SAME_PATH}\" + \"Users/td017/kaggle-pipeline/input/jan.csv\")\n",
    "meta = pd.read_csv(f\"{SAME_PATH}\" + \"Users/td017/kaggle-pipeline/input/meta.csv\")\n",
    "purchase_log = pd.read_csv(f\"{SAME_PATH}\" + \"Users/td017/kaggle-pipeline/input/purchase_log.csv\")\n",
    "sample_submission = pd.read_csv(f\"{SAME_PATH}\" + \"Users/td017/kaggle-pipeline/input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_id</th>\n",
       "      <th>130123</th>\n",
       "      <th>130125</th>\n",
       "      <th>130129</th>\n",
       "      <th>130131</th>\n",
       "      <th>140307</th>\n",
       "      <th>140313</th>\n",
       "      <th>140316</th>\n",
       "      <th>140317</th>\n",
       "      <th>140321</th>\n",
       "      <th>140501</th>\n",
       "      <th>140505</th>\n",
       "      <th>140641</th>\n",
       "      <th>140691</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>njibeyLPrsnu4HCopjBihW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FqBfZgvrWVNMsCqGmZMdv3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KYE5JJ4y6zJBipkCKobwVg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tC5JqjsVxsKxQ8Ykk9S7fg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pigjc37smwP2E3Z4VtKinB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>YCHTGgFS6shv3GCMB7sB5j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nQsjoHBDtiJvKNUxzUoR4d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LXdStjW5USNHWpcXHd4E5j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9qw6QFmtqjjbPS9pMrwi7S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e6qaQuMJ3xsZJ96QmgCnxN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              purchase_id  130123  130125  130129  130131  140307  140313  \\\n",
       "0  njibeyLPrsnu4HCopjBihW       0       0       0       0       0       0   \n",
       "1  FqBfZgvrWVNMsCqGmZMdv3       0       0       1       0       0       0   \n",
       "2  KYE5JJ4y6zJBipkCKobwVg       0       0       0       0       0       0   \n",
       "3  tC5JqjsVxsKxQ8Ykk9S7fg       0       0       0       1       0       0   \n",
       "4  Pigjc37smwP2E3Z4VtKinB       0       0       0       0       0       0   \n",
       "5  YCHTGgFS6shv3GCMB7sB5j       0       0       0       1       0       0   \n",
       "6  nQsjoHBDtiJvKNUxzUoR4d       0       0       0       0       0       0   \n",
       "7  LXdStjW5USNHWpcXHd4E5j       0       0       0       0       0       0   \n",
       "8  9qw6QFmtqjjbPS9pMrwi7S       0       0       1       0       0       0   \n",
       "9  e6qaQuMJ3xsZJ96QmgCnxN       0       0       0       0       0       0   \n",
       "\n",
       "   140316  140317  140321  140501  140505  140641  140691  \n",
       "0       0       0       0       0       0       0       0  \n",
       "1       0       0       0       0       0       0       0  \n",
       "2       0       0       0       0       0       0       0  \n",
       "3       0       0       0       0       0       0       0  \n",
       "4       0       0       0       0       1       0       0  \n",
       "5       0       0       0       0       0       0       0  \n",
       "6       0       0       0       0       0       0       0  \n",
       "7       1       0       0       0       0       0       0  \n",
       "8       0       0       0       0       0       0       0  \n",
       "9       0       0       0       0       0       0       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3rcdjjRyw9qSh6NcZMKSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y56uzwqQzynHYZ4bDfLPp5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xDMdFERmC7CD9yFvyvKJnh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzZENdjz7SvUQkGZV45afF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zFWkhHbLYJ9Fh5kUvCrx4g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              purchase_id\n",
       "0  C3rcdjjRyw9qSh6NcZMKSX\n",
       "1  Y56uzwqQzynHYZ4bDfLPp5\n",
       "2  xDMdFERmC7CD9yFvyvKJnh\n",
       "3  zzZENdjz7SvUQkGZV45afF\n",
       "4  zFWkhHbLYJ9Fh5kUvCrx4g"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ccl_jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "purchase_log[\"ccl_jan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccl_jan_list = []\n",
    "\n",
    "for i in purchase_log[\"ccl_jan\"].tolist():\n",
    "    if len(str(i)) == 13:\n",
    "        ccl_jan_list.append(str(i)[:7])\n",
    "    else:\n",
    "        ccl_jan_list.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_log[\"ccl_jan_company\"] = ccl_jan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_log[\"ccl_jan_company\"] .value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_merge = purchase_log.merge(jan,on=['ccl_jan','ccl_category_cd4'])\n",
    "purchase_merge = purchase_merge.merge(category,on='ccl_category_cd4')\n",
    "purchase_merge = purchase_merge.merge(meta,on='purchase_id')\n",
    "#train_df = train.merge(purchase_merge,on='purchase_id')\n",
    "#test_df = test.merge(purchase_merge,on='purchase_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_merge = purchase_merge[~purchase_merge['ccl_category_cd4'].isin(sample_submission.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_merge.to_pickle(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/purchase_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = train.merge(meta,on='purchase_id')\n",
    "test_meta = test.merge(meta,on='purchase_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = train_meta.copy()\n",
    "test_tmp = test_meta.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jpholiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref\n",
    "# https://www.guruguru.ml/competitions/9/discussions/5e86691f-1f18-4860-a52a-df79e7d6697b\n",
    "\n",
    "import datetime\n",
    "import jpholiday\n",
    "\n",
    "def isHoliday(date):\n",
    "    if date.weekday() >= 5 or jpholiday.is_holiday(date):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def isSatSun(date):\n",
    "    if date.weekday() >= 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "date = pd.Series(pd.to_datetime(meta[\"p_date\"].value_counts().sort_index().index))\n",
    "\n",
    "#特徴量生成\n",
    "years = date.apply(lambda x: x.year)\n",
    "months = date.apply(lambda x: x.month)\n",
    "days = date.apply(lambda x: x.day)\n",
    "weekdays = date.apply(lambda x: x.weekday())\n",
    "holidays = date.apply(lambda x: isHoliday(x))\n",
    "satsuns = date.apply(lambda x: isSatSun(x))\n",
    "specials = date.apply(lambda x: jpholiday.is_holiday(x)).astype(int)\n",
    "\n",
    "#大型連休フラグ\n",
    "\n",
    "#バレンタインフラグ\n",
    "\n",
    "time_df = pd.concat([date, years, months, days, weekdays, holidays, satsuns, specials], axis=1)\n",
    "time_df.columns = [\"p_date\", \"years\",\"months\",\"days\",\"weekdays\",\"holiday_flg\",\"satsun_flg\", \"special_flg\"]\n",
    "time_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "End_Start_year_list = [\n",
    "    #年末年始\n",
    "    \"2017-01-02\",\n",
    "    \"2017-12-30\",\"2017-12-31\",\n",
    "    \"2018-01-01\",\"2018-01-02\",\"2018-01-03\",\"2018-01-04\",\"2018-01-05\",\"2018-01-06\",\"2018-01-07\",\n",
    "    \"2018-12-29\",\"2018-12-30\",\"2018-12-31\"\n",
    "]\n",
    "\n",
    "GW_list = [\n",
    "    #GW\n",
    "    \"2017-04-29\",\"2017-04-30\",\n",
    "    \"2017-05-01\",\"2017-05-02\",\"2017-05-03\",\"2017-05-04\",\"2017-05-05\",\"2017-05-06\",\"2017-05-07\",\n",
    "    \"2018-04-27\",\"2018-04-28\",\"2018-04-29\",\"2018-04-30\",\n",
    "    \"2018-05-01\",\"2018-05-02\",\"2018-05-03\",\"2018-05-04\",\"2018-05-05\",\"2018-05-06\"\n",
    "    ]\n",
    "\n",
    "#多分、ハロウィーンまでの残り時間の逆数みたいな変数を作ったほうがいい気がする\n",
    "Halloween_list = [\n",
    "    #ハロウィン\n",
    "    \"2017-10-31\",\n",
    "    \"2018-10-31\",\n",
    "]\n",
    "\n",
    "#多分、クリスマスをピークに前30日後１４日で値が入るような変数を作ったほうがいい気がする\n",
    "Christmas_list = [\n",
    "#クリスマス\n",
    "\"2017-12-24\",\"2017-12-25\",\n",
    "\"2018-12-24\",\"2018-12-25\",\n",
    "]\n",
    "\n",
    "#多分、バレンタインをピークに前30日後１４日で値が入るような変数を作ったほうがいい気がする\n",
    "Valentine_list = [\n",
    "    \"2017-02-14\",\n",
    "    \"2018-02-14\",\n",
    "]\n",
    "\n",
    "#多分、ホワイトデーまでの残り時間の逆数みたいな変数を作ったほうがいい気がする\n",
    "Whiteday_list = [\n",
    "    \"2017-03-14\",\n",
    "    \"2018-03-14\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df['End_Start_year_flg'] = date.isin(End_Start_year_list).astype(\"int\")\n",
    "time_df['GW_flg'] = date.isin(GW_list).astype(\"int\")\n",
    "time_df['Halloween_flg'] = date.isin(Halloween_list).astype(\"int\")\n",
    "time_df['Christmas_flg'] = date.isin(Christmas_list).astype(\"int\")\n",
    "time_df['Valentine_flg'] = date.isin(Valentine_list).astype(\"int\")\n",
    "time_df['Whiteday_flg'] = date.isin(Whiteday_list).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_time_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggr_numeric(train,test,cols,key_col,agg_list):\n",
    "    train_after = train.copy()\n",
    "    test_after = test.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        for agg_type in agg_list:\n",
    "            new_col_name = key_col + \"_\" + col +\"_\"+ agg_type\n",
    "            temp_df =  purchase_merge.groupby(key_col)[col].agg([agg_type]).reset_index().rename(columns={agg_type : new_col_name})\n",
    "            temp_df.index = list(temp_df[key_col])\n",
    "            temp_df = temp_df[new_col_name].to_dict()   \n",
    "            \n",
    "            train_after[new_col_name] = train[key_col].map(temp_df)\n",
    "            test_after[new_col_name]  = test[key_col].map(temp_df)\n",
    "    \n",
    "    return train_after,test_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"purchase_id\"\n",
    "cols = [\"amount\",\"total\",\"p_time\"]\n",
    "agg_list = ['sum','max','min','mean','std','median']\n",
    "\n",
    "train_tmp2,test_tmp2 = aggr_numeric(train_tmp,test_tmp,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"mpno\"\n",
    "\n",
    "train_tmp3,test_tmp3 = aggr_numeric(train_tmp2,test_tmp2,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"mstr\"\n",
    "\n",
    "train_tmp4,test_tmp4 = aggr_numeric(train_tmp3,test_tmp3,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp4.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_train.pkl\")\n",
    "test_tmp4.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp4 = train_after.copy()\n",
    "test_tmp4 = test_after.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggr_category(train,test,cols,key_col,agg_list):\n",
    "    from scipy.stats import mode\n",
    "    def mod(arr):\n",
    "        print(\"lol\")\n",
    "        print(mode(arr))\n",
    "        return mode(arr)[0][0]   \n",
    "    \n",
    "    train_after = train.copy()\n",
    "    test_after = test.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        for agg_type in agg_list:\n",
    "            new_col_name = key_col + \"_\" + col +\"_\"+ agg_type\n",
    "            if agg_type == \"mode\":\n",
    "                temp_df =  purchase_merge.groupby(key_col)[col].agg(mod).reset_index().rename(columns={col : new_col_name})\n",
    "            elif agg_type == \"nunique\":\n",
    "                temp_df =  purchase_merge.groupby(key_col)[col].nunique().reset_index().rename(columns={col : new_col_name})\n",
    "            else:\n",
    "                temp_df =  purchase_merge.groupby(key_col)[col].agg([agg_type]).reset_index().rename(columns={agg_type : new_col_name})\n",
    "            temp_df.index = list(temp_df[key_col])\n",
    "            temp_df = temp_df[new_col_name].to_dict()   \n",
    "            \n",
    "            train_after[new_col_name] = train[key_col].map(temp_df)\n",
    "            test_after[new_col_name]  = test[key_col].map(temp_df)\n",
    "    \n",
    "    return train_after,test_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"purchase_id\"\n",
    "cols = ['ccl_category_cd4',\n",
    "        'ccl_jan','jan_name',\n",
    "        'ccl_category_cd1',\n",
    "        'ccl_category_cd2',\n",
    "        'ccl_category_cd3',\n",
    "       'ccl_category_name1',\n",
    "       'ccl_category_name2',\n",
    "       'ccl_category_name3',\n",
    "       'ccl_category_name4',\n",
    "        'mpno',\n",
    "        'mstr'\n",
    "       ]\n",
    "agg_list = ['mode','nunique']\n",
    "\n",
    "train_tmp5,test_tmp5 = aggr_category(train_tmp4,test_tmp4,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"mpno\"\n",
    "cols.remove(key_col)\n",
    "# cols = [\"ccl_category_cd4\"]\n",
    "# agg_list = ['mode']\n",
    "\n",
    "train_tmp6,test_tmp6 = aggr_category(train_tmp5,test_tmp5,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"mstr\"\n",
    "cols.remove(key_col)\n",
    "# cols = [\"ccl_category_cd4\"]\n",
    "# agg_list = ['mode']\n",
    "\n",
    "train_tmp7,test_tmp7 = aggr_category(train_tmp6,test_tmp6,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_after.to_csv(\n",
    "#     \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir0/code/Users/td017/kaggle-pipeline/input/train_after.csv\",\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "# test_after.to_csv(\n",
    "#     \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir0/code/Users/td017/kaggle-pipeline/input/test_after.csv\",\n",
    "#     index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp7.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_train.pkl\")\n",
    "test_tmp7.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load_and_pickle_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp7 = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/case8_train.pkl\")\n",
    "test_tmp7 = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/case8_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_df = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_time_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tmp7['p_date'] = pd.to_datetime(train_tmp7['p_date'] )\n",
    "# train_tmp8 = pd.merge(train_tmp7,time_df,on='p_date')\n",
    "\n",
    "# test_tmp7['p_date'] = pd.to_datetime(test_tmp7['p_date'] )\n",
    "# test_tmp8 = pd.merge(test_tmp7,time_df,on='p_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp7['p_date'] = pd.to_datetime(train_tmp7['p_date'] )\n",
    "train_tmp8 = pd.merge(train_tmp7,time_df[[\"p_date\",\"Christmas_flg\"]],on='p_date')\n",
    "\n",
    "test_tmp7['p_date'] = pd.to_datetime(test_tmp7['p_date'] )\n",
    "test_tmp8 = pd.merge(test_tmp7,time_df[[\"p_date\",\"Christmas_flg\"]],on='p_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tmp8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp8.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_train.pkl\")\n",
    "test_tmp8.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME_AGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp8 = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_train.pkl\")\n",
    "test_tmp8 = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tmp8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_merge = pd.read_pickle(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/purchase_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/case9_time_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_merge['p_date'] = pd.to_datetime(purchase_merge['p_date'])\n",
    "\n",
    "purchase_merge = purchase_merge.merge(time_df,on='p_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"months\"\n",
    "cols = [\"amount\",\"total\",\"p_time\"]\n",
    "agg_list = ['sum','max','min','mean','std','median']\n",
    "\n",
    "train_tmp9,test_tmp9 = aggr_numeric(train_tmp8,test_tmp8,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"weekdays\"\n",
    "\n",
    "train_tmp10,test_tmp10 = aggr_numeric(train_tmp9,test_tmp9,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"days\"\n",
    "\n",
    "train_tmp11,test_tmp11 = aggr_numeric(train_tmp10,test_tmp10,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_col = \"holiday_flg\"\n",
    "\n",
    "train_tmp12,test_tmp12 = aggr_numeric(train_tmp11,test_tmp11,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp12.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_train.pkl\")\n",
    "test_tmp12.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_col = \"months\"\n",
    "# cols = [\n",
    "#     #'ccl_category_cd4',\n",
    "#     #'ccl_jan',\n",
    "#     'jan_name',\n",
    "#     #'ccl_category_cd1',\n",
    "#     #'ccl_category_cd2',\n",
    "#     #'ccl_category_cd3',\n",
    "#     'ccl_category_name1',\n",
    "#     'ccl_category_name2',\n",
    "#     'ccl_category_name3',\n",
    "#     'ccl_category_name4',\n",
    "#     #'mpno',\n",
    "#     #'mstr'\n",
    "# ]\n",
    "# agg_list = ['mode','nunique']\n",
    "\n",
    "# train_tmp13,test_tmp13 = aggr_category(train_tmp12,test_tmp12,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_col = \"weekdays\"\n",
    "\n",
    "# train_tmp14,test_tmp14 = aggr_category(train_tmp13,test_tmp13,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_col = \"days\"\n",
    "\n",
    "# train_tmp15,test_tmp15 = aggr_category(train_tmp14,test_tmp14,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_col = \"holiday_flg\"\n",
    "\n",
    "# train_tmp16,test_tmp16 = aggr_category(train_tmp15,test_tmp15,cols,key_col,agg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tmp16.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir0/code/Users/td017/kaggle-pipeline/tmp/{CASE}_train.pkl\")\n",
    "# test_tmp16.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir0/code/Users/td017/kaggle-pipeline/tmp/{CASE}_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_after = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/case9_train.pkl\")\n",
    "test_after = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/case9_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = train_after.copy()\n",
    "test_tmp = test_after.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_target_per_min_max = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/month_per_min_max_df.pkl\").add_suffix(\"_per_months\")\n",
    "day_target_per_min_max = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/day_per_min_max_df.pkl\").add_suffix(\"_per_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_per_months</th>\n",
       "      <th>スナック_per_months</th>\n",
       "      <th>チョコレート_per_months</th>\n",
       "      <th>RTD_per_months</th>\n",
       "      <th>コーヒードリンク_per_months</th>\n",
       "      <th>米菓_per_months</th>\n",
       "      <th>新ジャンル_per_months</th>\n",
       "      <th>日本茶・麦茶ドリンク_per_months</th>\n",
       "      <th>ビール_per_months</th>\n",
       "      <th>発泡酒_per_months</th>\n",
       "      <th>チューインガム_per_months</th>\n",
       "      <th>水_per_months</th>\n",
       "      <th>その他茶ドリンク_per_months</th>\n",
       "      <th>炭酸水_per_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.638197</td>\n",
       "      <td>0.556225</td>\n",
       "      <td>0.099330</td>\n",
       "      <td>0.031246</td>\n",
       "      <td>0.430260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125129</td>\n",
       "      <td>0.018580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.754691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.255688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886637</td>\n",
       "      <td>0.517630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189315</td>\n",
       "      <td>0.175762</td>\n",
       "      <td>0.157159</td>\n",
       "      <td>0.077859</td>\n",
       "      <td>0.222124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.626955</td>\n",
       "      <td>0.506733</td>\n",
       "      <td>0.404150</td>\n",
       "      <td>0.088989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708028</td>\n",
       "      <td>0.431775</td>\n",
       "      <td>0.575860</td>\n",
       "      <td>0.212208</td>\n",
       "      <td>0.378681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165016</td>\n",
       "      <td>0.091367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.876234</td>\n",
       "      <td>0.453102</td>\n",
       "      <td>0.367034</td>\n",
       "      <td>0.278157</td>\n",
       "      <td>0.410407</td>\n",
       "      <td>0.711879</td>\n",
       "      <td>0.325582</td>\n",
       "      <td>0.873317</td>\n",
       "      <td>0.868953</td>\n",
       "      <td>0.557202</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.195202</td>\n",
       "      <td>0.344214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.078425</td>\n",
       "      <td>0.243080</td>\n",
       "      <td>0.523541</td>\n",
       "      <td>0.522735</td>\n",
       "      <td>0.387490</td>\n",
       "      <td>0.904890</td>\n",
       "      <td>0.538690</td>\n",
       "      <td>0.884929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396753</td>\n",
       "      <td>0.730982</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   months_per_months  スナック_per_months  チョコレート_per_months  RTD_per_months  \\\n",
       "0                  1         0.638197           0.556225        0.099330   \n",
       "1                  2         0.754691           1.000000        0.255688   \n",
       "2                  3         0.626955           0.506733        0.404150   \n",
       "3                  4         0.876234           0.453102        0.367034   \n",
       "4                  5         0.078425           0.243080        0.523541   \n",
       "\n",
       "   コーヒードリンク_per_months  米菓_per_months  新ジャンル_per_months  \\\n",
       "0             0.031246       0.430260          0.000000   \n",
       "1             0.000000       0.886637          0.517630   \n",
       "2             0.088989       1.000000          0.708028   \n",
       "3             0.278157       0.410407          0.711879   \n",
       "4             0.522735       0.387490          0.904890   \n",
       "\n",
       "   日本茶・麦茶ドリンク_per_months  ビール_per_months  発泡酒_per_months  チューインガム_per_months  \\\n",
       "0               0.041853        0.000000        0.000000            0.125129   \n",
       "1               0.000000        0.189315        0.175762            0.157159   \n",
       "2               0.431775        0.575860        0.212208            0.378681   \n",
       "3               0.325582        0.873317        0.868953            0.557202   \n",
       "4               0.538690        0.884929        1.000000            0.000000   \n",
       "\n",
       "   水_per_months  その他茶ドリンク_per_months  炭酸水_per_months  \n",
       "0      0.018580             0.000000        0.078538  \n",
       "1      0.077859             0.222124        0.000000  \n",
       "2      0.000000             0.165016        0.091367  \n",
       "3      0.072100             0.195202        0.344214  \n",
       "4      0.396753             0.730982        1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_target_per_min_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = pd.merge(train_tmp,month_target_per_min_max, left_on = \"months\",right_on=\"months_per_months\")\n",
    "test_tmp = pd.merge(test_tmp,month_target_per_min_max, left_on = \"months\",right_on=\"months_per_months\")\n",
    "\n",
    "train_tmp = pd.merge(train_tmp,day_target_per_min_max, left_on = \"days\",right_on=\"days_per_days\")\n",
    "test_tmp = pd.merge(test_tmp,day_target_per_min_max, left_on = \"days\", right_on=\"days_per_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstr_target_per_min_max = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/case13_mstr_per.pkl\").add_suffix(\"_per_mstr\").reset_index().rename(columns={\"index\":\"mstr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = pd.merge(train_tmp,mstr_target_per_min_max, on=\"mstr\")\n",
    "test_tmp = pd.merge(test_tmp,mstr_target_per_min_max, on=\"mstr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128376, 260)\n",
      "(96505, 247)\n"
     ]
    }
   ],
   "source": [
    "print(train_tmp.shape)\n",
    "print(test_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 気象庁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisyoutyou = pd.read_csv(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/kisyoutyou_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisyoutyou['p_date'] = pd.to_datetime(kisyoutyou['p_date'])\n",
    "train_tmp = pd.merge(train_tmp,kisyoutyou, on=\"p_date\")\n",
    "test_tmp = pd.merge(test_tmp,kisyoutyou, on=\"p_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_after = train_tmp.copy()\n",
    "test_after = test_tmp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128376, 266)\n",
      "(96505, 253)\n"
     ]
    }
   ],
   "source": [
    "print(train_after.shape)\n",
    "print(test_after.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykakasi import kakasi\n",
    "\n",
    "kakasi = kakasi()\n",
    "\n",
    "kakasi.setMode('H', 'a')\n",
    "kakasi.setMode('K', 'a')\n",
    "kakasi.setMode('J', 'a')\n",
    "\n",
    "conv = kakasi.getConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list_train = []\n",
    "\n",
    "for i in train_after.columns.tolist():\n",
    "    columns_list_train.append(conv.do(i))\n",
    "\n",
    "train_after.columns = columns_list_train\n",
    "\n",
    "columns_list_test = []\n",
    "\n",
    "for i in test_after.columns.tolist():\n",
    "    columns_list_test.append(conv.do(i))\n",
    "\n",
    "test_after.columns = columns_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_after.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_after.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "class Base_Model(object):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, target,categoricals=[], n_splits=5, verbose=True):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = target\n",
    "        self.cv = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        self.params = self.get_params()\n",
    "        self.y_pred, self.score, self.model,self.oof_pred = self.fit()\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_cv(self):\n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        return cv.split(self.train_df, self.train_df[self.target])\n",
    "    \n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "        \n",
    "    def fit(self):\n",
    "        oof_pred = np.zeros((len(self.train_df), ))\n",
    "        y_pred = np.zeros((len(self.test_df), ))\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n",
    "            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model = self.train_model(train_set, val_set)\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "            print('Partial score of fold {} is: {}'.format(fold, roc_auc_score(y_val, oof_pred[val_idx])))\n",
    "            score = roc_auc_score(self.train_df[self.target], oof_pred)\n",
    "        if self.verbose:\n",
    "            print('AUC score is: ', score)\n",
    "        return y_pred, score, model, oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lgb_Model(Base_Model):\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'n_estimators':5000,\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'auc',\n",
    "                    'subsample': 0.75,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.05,\n",
    "                    'feature_fraction': 0.9,\n",
    "                    'max_depth': 7,\n",
    "                    'lambda_l1': 1,  \n",
    "                    'lambda_l2': 1,\n",
    "                    'early_stopping_rounds': 100\n",
    "                    }\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = [ \n",
    " '130123',\n",
    " '130125',\n",
    " '130129',\n",
    " '130131',\n",
    " '140307',\n",
    " '140313',\n",
    " '140316',\n",
    " '140317',\n",
    " '140321',\n",
    " '140501',\n",
    " '140505',\n",
    " '140641',\n",
    " '140691'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = [\n",
    "    'purchase_id',\n",
    "    'years',\n",
    "    'p_date'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_after.columns.tolist()\n",
    "\n",
    "for i in target_list:\n",
    "    features.remove(i)\n",
    "    \n",
    "for i in remove_list:\n",
    "    features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mpno',\n",
       " 'mstr',\n",
       " 'p_time',\n",
       " 'purchase_id_amount_sum',\n",
       " 'purchase_id_amount_max',\n",
       " 'purchase_id_amount_min',\n",
       " 'purchase_id_amount_mean',\n",
       " 'purchase_id_amount_std',\n",
       " 'purchase_id_amount_median',\n",
       " 'purchase_id_total_sum',\n",
       " 'purchase_id_total_max',\n",
       " 'purchase_id_total_min',\n",
       " 'purchase_id_total_mean',\n",
       " 'purchase_id_total_std',\n",
       " 'purchase_id_total_median',\n",
       " 'purchase_id_p_time_sum',\n",
       " 'purchase_id_p_time_max',\n",
       " 'purchase_id_p_time_min',\n",
       " 'purchase_id_p_time_mean',\n",
       " 'purchase_id_p_time_std',\n",
       " 'purchase_id_p_time_median',\n",
       " 'mpno_amount_sum',\n",
       " 'mpno_amount_max',\n",
       " 'mpno_amount_min',\n",
       " 'mpno_amount_mean',\n",
       " 'mpno_amount_std',\n",
       " 'mpno_amount_median',\n",
       " 'mpno_total_sum',\n",
       " 'mpno_total_max',\n",
       " 'mpno_total_min',\n",
       " 'mpno_total_mean',\n",
       " 'mpno_total_std',\n",
       " 'mpno_total_median',\n",
       " 'mpno_p_time_sum',\n",
       " 'mpno_p_time_max',\n",
       " 'mpno_p_time_min',\n",
       " 'mpno_p_time_mean',\n",
       " 'mpno_p_time_std',\n",
       " 'mpno_p_time_median',\n",
       " 'mstr_amount_sum',\n",
       " 'mstr_amount_max',\n",
       " 'mstr_amount_min',\n",
       " 'mstr_amount_mean',\n",
       " 'mstr_amount_std',\n",
       " 'mstr_amount_median',\n",
       " 'mstr_total_sum',\n",
       " 'mstr_total_max',\n",
       " 'mstr_total_min',\n",
       " 'mstr_total_mean',\n",
       " 'mstr_total_std',\n",
       " 'mstr_total_median',\n",
       " 'mstr_p_time_sum',\n",
       " 'mstr_p_time_max',\n",
       " 'mstr_p_time_min',\n",
       " 'mstr_p_time_mean',\n",
       " 'mstr_p_time_std',\n",
       " 'mstr_p_time_median',\n",
       " 'purchase_id_ccl_category_cd4_mode',\n",
       " 'purchase_id_ccl_category_cd4_nunique',\n",
       " 'purchase_id_ccl_jan_mode',\n",
       " 'purchase_id_ccl_jan_nunique',\n",
       " 'purchase_id_jan_name_mode',\n",
       " 'purchase_id_jan_name_nunique',\n",
       " 'purchase_id_ccl_category_cd1_mode',\n",
       " 'purchase_id_ccl_category_cd1_nunique',\n",
       " 'purchase_id_ccl_category_cd2_mode',\n",
       " 'purchase_id_ccl_category_cd2_nunique',\n",
       " 'purchase_id_ccl_category_cd3_mode',\n",
       " 'purchase_id_ccl_category_cd3_nunique',\n",
       " 'purchase_id_ccl_category_name1_mode',\n",
       " 'purchase_id_ccl_category_name1_nunique',\n",
       " 'purchase_id_ccl_category_name2_mode',\n",
       " 'purchase_id_ccl_category_name2_nunique',\n",
       " 'purchase_id_ccl_category_name3_mode',\n",
       " 'purchase_id_ccl_category_name3_nunique',\n",
       " 'purchase_id_ccl_category_name4_mode',\n",
       " 'purchase_id_ccl_category_name4_nunique',\n",
       " 'purchase_id_mpno_mode',\n",
       " 'purchase_id_mpno_nunique',\n",
       " 'purchase_id_mstr_mode',\n",
       " 'purchase_id_mstr_nunique',\n",
       " 'mpno_ccl_category_cd4_mode',\n",
       " 'mpno_ccl_category_cd4_nunique',\n",
       " 'mpno_ccl_jan_mode',\n",
       " 'mpno_ccl_jan_nunique',\n",
       " 'mpno_jan_name_mode',\n",
       " 'mpno_jan_name_nunique',\n",
       " 'mpno_ccl_category_cd1_mode',\n",
       " 'mpno_ccl_category_cd1_nunique',\n",
       " 'mpno_ccl_category_cd2_mode',\n",
       " 'mpno_ccl_category_cd2_nunique',\n",
       " 'mpno_ccl_category_cd3_mode',\n",
       " 'mpno_ccl_category_cd3_nunique',\n",
       " 'mpno_ccl_category_name1_mode',\n",
       " 'mpno_ccl_category_name1_nunique',\n",
       " 'mpno_ccl_category_name2_mode',\n",
       " 'mpno_ccl_category_name2_nunique',\n",
       " 'mpno_ccl_category_name3_mode',\n",
       " 'mpno_ccl_category_name3_nunique',\n",
       " 'mpno_ccl_category_name4_mode',\n",
       " 'mpno_ccl_category_name4_nunique',\n",
       " 'mpno_mstr_mode',\n",
       " 'mpno_mstr_nunique',\n",
       " 'mstr_ccl_category_cd4_mode',\n",
       " 'mstr_ccl_category_cd4_nunique',\n",
       " 'mstr_ccl_jan_mode',\n",
       " 'mstr_ccl_jan_nunique',\n",
       " 'mstr_jan_name_mode',\n",
       " 'mstr_jan_name_nunique',\n",
       " 'mstr_ccl_category_cd1_mode',\n",
       " 'mstr_ccl_category_cd1_nunique',\n",
       " 'mstr_ccl_category_cd2_mode',\n",
       " 'mstr_ccl_category_cd2_nunique',\n",
       " 'mstr_ccl_category_cd3_mode',\n",
       " 'mstr_ccl_category_cd3_nunique',\n",
       " 'mstr_ccl_category_name1_mode',\n",
       " 'mstr_ccl_category_name1_nunique',\n",
       " 'mstr_ccl_category_name2_mode',\n",
       " 'mstr_ccl_category_name2_nunique',\n",
       " 'mstr_ccl_category_name3_mode',\n",
       " 'mstr_ccl_category_name3_nunique',\n",
       " 'mstr_ccl_category_name4_mode',\n",
       " 'mstr_ccl_category_name4_nunique',\n",
       " 'months',\n",
       " 'days',\n",
       " 'weekdays',\n",
       " 'holiday_flg',\n",
       " 'satsun_flg',\n",
       " 'special_flg',\n",
       " 'End_Start_year_flg',\n",
       " 'GW_flg',\n",
       " 'Halloween_flg',\n",
       " 'Valentine_flg',\n",
       " 'Whiteday_flg',\n",
       " 'Christmas_flg',\n",
       " 'months_amount_sum',\n",
       " 'months_amount_max',\n",
       " 'months_amount_min',\n",
       " 'months_amount_mean',\n",
       " 'months_amount_std',\n",
       " 'months_amount_median',\n",
       " 'months_total_sum',\n",
       " 'months_total_max',\n",
       " 'months_total_min',\n",
       " 'months_total_mean',\n",
       " 'months_total_std',\n",
       " 'months_total_median',\n",
       " 'months_p_time_sum',\n",
       " 'months_p_time_max',\n",
       " 'months_p_time_min',\n",
       " 'months_p_time_mean',\n",
       " 'months_p_time_std',\n",
       " 'months_p_time_median',\n",
       " 'weekdays_amount_sum',\n",
       " 'weekdays_amount_max',\n",
       " 'weekdays_amount_min',\n",
       " 'weekdays_amount_mean',\n",
       " 'weekdays_amount_std',\n",
       " 'weekdays_amount_median',\n",
       " 'weekdays_total_sum',\n",
       " 'weekdays_total_max',\n",
       " 'weekdays_total_min',\n",
       " 'weekdays_total_mean',\n",
       " 'weekdays_total_std',\n",
       " 'weekdays_total_median',\n",
       " 'weekdays_p_time_sum',\n",
       " 'weekdays_p_time_max',\n",
       " 'weekdays_p_time_min',\n",
       " 'weekdays_p_time_mean',\n",
       " 'weekdays_p_time_std',\n",
       " 'weekdays_p_time_median',\n",
       " 'days_amount_sum',\n",
       " 'days_amount_max',\n",
       " 'days_amount_min',\n",
       " 'days_amount_mean',\n",
       " 'days_amount_std',\n",
       " 'days_amount_median',\n",
       " 'days_total_sum',\n",
       " 'days_total_max',\n",
       " 'days_total_min',\n",
       " 'days_total_mean',\n",
       " 'days_total_std',\n",
       " 'days_total_median',\n",
       " 'days_p_time_sum',\n",
       " 'days_p_time_max',\n",
       " 'days_p_time_min',\n",
       " 'days_p_time_mean',\n",
       " 'days_p_time_std',\n",
       " 'days_p_time_median',\n",
       " 'holiday_flg_amount_sum',\n",
       " 'holiday_flg_amount_max',\n",
       " 'holiday_flg_amount_min',\n",
       " 'holiday_flg_amount_mean',\n",
       " 'holiday_flg_amount_std',\n",
       " 'holiday_flg_amount_median',\n",
       " 'holiday_flg_total_sum',\n",
       " 'holiday_flg_total_max',\n",
       " 'holiday_flg_total_min',\n",
       " 'holiday_flg_total_mean',\n",
       " 'holiday_flg_total_std',\n",
       " 'holiday_flg_total_median',\n",
       " 'holiday_flg_p_time_sum',\n",
       " 'holiday_flg_p_time_max',\n",
       " 'holiday_flg_p_time_min',\n",
       " 'holiday_flg_p_time_mean',\n",
       " 'holiday_flg_p_time_std',\n",
       " 'holiday_flg_p_time_median',\n",
       " 'months_per_months',\n",
       " 'sunakku_per_months',\n",
       " 'chokoreeto_per_months',\n",
       " 'RTD_per_months',\n",
       " 'koohiidorinku_per_months',\n",
       " 'komeka_per_months',\n",
       " 'shinjanru_per_months',\n",
       " 'nihoncha.mugichadorinku_per_months',\n",
       " 'biiru_per_months',\n",
       " 'happousake_per_months',\n",
       " 'chuuingamu_per_months',\n",
       " 'mizu_per_months',\n",
       " 'sonohokachadorinku_per_months',\n",
       " 'tansansui_per_months',\n",
       " 'days_per_days',\n",
       " 'sunakku_per_days',\n",
       " 'chokoreeto_per_days',\n",
       " 'RTD_per_days',\n",
       " 'koohiidorinku_per_days',\n",
       " 'komeka_per_days',\n",
       " 'shinjanru_per_days',\n",
       " 'nihoncha.mugichadorinku_per_days',\n",
       " 'biiru_per_days',\n",
       " 'happousake_per_days',\n",
       " 'sonohokachadorinku_per_days',\n",
       " 'chuuingamu_per_days',\n",
       " 'mizu_per_days',\n",
       " 'tansansui_per_days',\n",
       " 'sunakku_per_mstr',\n",
       " 'chokoreeto_per_mstr',\n",
       " 'koohiidorinku_per_mstr',\n",
       " 'shinjanru_per_mstr',\n",
       " 'nihoncha.mugichadorinku_per_mstr',\n",
       " 'mizu_per_mstr',\n",
       " 'happousake_per_mstr',\n",
       " 'sonohokachadorinku_per_mstr',\n",
       " 'tansansui_per_mstr',\n",
       " 'mean_tmp',\n",
       " 'max_tmp',\n",
       " 'min_tmp',\n",
       " 'sum_precipitation',\n",
       " 'sunshine_hours',\n",
       " 'mean_windspeed']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Enc\n",
    "def LabelEnc(train,test):\n",
    "    train_after = train.copy()\n",
    "    test_after = test.copy()\n",
    "    \n",
    "    train[\"type\"] = \"train\"\n",
    "    test[\"type\"] = \"test\"\n",
    "    \n",
    "    # データを全結合\n",
    "    all_tmp = pd.concat([train,test],sort=False)\n",
    "    \n",
    "    # Labelencを行うリストを取得\n",
    "    obj_lost = all_tmp.select_dtypes(\"O\").columns.tolist()\n",
    "    obj_lost.remove(\"type\")\n",
    "    \n",
    "    ### LabelEncを実施\n",
    "    import category_encoders as ce\n",
    "    #ce_oe = ce.OrdinalEncoder(cols=obj_lost,handle_unknown='impute')\n",
    "    ce_oe = ce.OrdinalEncoder(cols=obj_lost)\n",
    "    all_tmp_2 = ce_oe.fit_transform(all_tmp)\n",
    "    all_tmp_2_train = all_tmp_2[all_tmp_2[\"type\"] == \"train\"]\n",
    "    all_tmp_2_test = all_tmp_2[all_tmp_2[\"type\"] == \"test\"]\n",
    "    #文字を序数に変換\n",
    "    for i in obj_lost:\n",
    "        train_after[i] = all_tmp_2_train[i] - 1\n",
    "        test_after[i] = all_tmp_2_test[i] - 1\n",
    "    \n",
    "    return train_after,test_after\n",
    "\n",
    "train_after,test_after = LabelEnc(train_after,test_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### 1 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.771811\tvalid_1's auc: 0.732927\n",
      "[200]\ttraining's auc: 0.810026\tvalid_1's auc: 0.746359\n",
      "[300]\ttraining's auc: 0.838041\tvalid_1's auc: 0.749858\n",
      "[400]\ttraining's auc: 0.860632\tvalid_1's auc: 0.751393\n",
      "[500]\ttraining's auc: 0.880201\tvalid_1's auc: 0.753106\n",
      "[600]\ttraining's auc: 0.896402\tvalid_1's auc: 0.752851\n",
      "Early stopping, best iteration is:\n",
      "[531]\ttraining's auc: 0.885487\tvalid_1's auc: 0.753323\n",
      "Partial score of fold 0 is: 0.7533234808346417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.771633\tvalid_1's auc: 0.73243\n",
      "[200]\ttraining's auc: 0.810016\tvalid_1's auc: 0.743455\n",
      "[300]\ttraining's auc: 0.838145\tvalid_1's auc: 0.747615\n",
      "[400]\ttraining's auc: 0.860852\tvalid_1's auc: 0.749261\n",
      "[500]\ttraining's auc: 0.880081\tvalid_1's auc: 0.74943\n",
      "Early stopping, best iteration is:\n",
      "[483]\ttraining's auc: 0.877024\tvalid_1's auc: 0.749879\n",
      "Partial score of fold 1 is: 0.7498790527943615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.771374\tvalid_1's auc: 0.731443\n",
      "[200]\ttraining's auc: 0.810991\tvalid_1's auc: 0.743174\n",
      "[300]\ttraining's auc: 0.838464\tvalid_1's auc: 0.747196\n",
      "[400]\ttraining's auc: 0.861376\tvalid_1's auc: 0.749032\n",
      "[500]\ttraining's auc: 0.880832\tvalid_1's auc: 0.749239\n",
      "[600]\ttraining's auc: 0.897665\tvalid_1's auc: 0.748865\n",
      "Early stopping, best iteration is:\n",
      "[518]\ttraining's auc: 0.884338\tvalid_1's auc: 0.749325\n",
      "Partial score of fold 2 is: 0.7493249851279229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.771286\tvalid_1's auc: 0.736592\n",
      "[200]\ttraining's auc: 0.810226\tvalid_1's auc: 0.747922\n",
      "[300]\ttraining's auc: 0.838009\tvalid_1's auc: 0.75095\n",
      "[400]\ttraining's auc: 0.861666\tvalid_1's auc: 0.752999\n",
      "[500]\ttraining's auc: 0.880665\tvalid_1's auc: 0.754089\n",
      "[600]\ttraining's auc: 0.896762\tvalid_1's auc: 0.753899\n",
      "Early stopping, best iteration is:\n",
      "[537]\ttraining's auc: 0.886855\tvalid_1's auc: 0.754199\n",
      "Partial score of fold 3 is: 0.7541990416783935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.770453\tvalid_1's auc: 0.735264\n",
      "[200]\ttraining's auc: 0.808873\tvalid_1's auc: 0.74601\n",
      "[300]\ttraining's auc: 0.838068\tvalid_1's auc: 0.750206\n",
      "[400]\ttraining's auc: 0.86037\tvalid_1's auc: 0.750899\n",
      "[500]\ttraining's auc: 0.879563\tvalid_1's auc: 0.751175\n",
      "Early stopping, best iteration is:\n",
      "[412]\ttraining's auc: 0.862903\tvalid_1's auc: 0.751491\n",
      "Partial score of fold 4 is: 0.7514907645940281\n",
      "AUC score is:  0.7516361499218029\n",
      "################### 2 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.926944\tvalid_1's auc: 0.8009\n",
      "[200]\ttraining's auc: 0.969613\tvalid_1's auc: 0.805289\n",
      "Early stopping, best iteration is:\n",
      "[184]\ttraining's auc: 0.964868\tvalid_1's auc: 0.806264\n",
      "Partial score of fold 0 is: 0.8062644644723744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.927998\tvalid_1's auc: 0.819217\n",
      "[200]\ttraining's auc: 0.970171\tvalid_1's auc: 0.819421\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttraining's auc: 0.954897\tvalid_1's auc: 0.823474\n",
      "Partial score of fold 1 is: 0.8234738576571912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.927967\tvalid_1's auc: 0.820788\n",
      "[200]\ttraining's auc: 0.970615\tvalid_1's auc: 0.826606\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttraining's auc: 0.953372\tvalid_1's auc: 0.827809\n",
      "Partial score of fold 2 is: 0.8278094812100049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.926361\tvalid_1's auc: 0.826014\n",
      "[200]\ttraining's auc: 0.969282\tvalid_1's auc: 0.828645\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's auc: 0.95749\tvalid_1's auc: 0.829271\n",
      "Partial score of fold 3 is: 0.8292709914461973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.92837\tvalid_1's auc: 0.784103\n",
      "[200]\ttraining's auc: 0.970005\tvalid_1's auc: 0.792145\n",
      "[300]\ttraining's auc: 0.986328\tvalid_1's auc: 0.793247\n",
      "Early stopping, best iteration is:\n",
      "[291]\ttraining's auc: 0.985156\tvalid_1's auc: 0.794119\n",
      "Partial score of fold 4 is: 0.7941194136577302\n",
      "AUC score is:  0.8143444472271061\n",
      "################### 3 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.80045\tvalid_1's auc: 0.720787\n",
      "[200]\ttraining's auc: 0.854214\tvalid_1's auc: 0.730844\n",
      "[300]\ttraining's auc: 0.892346\tvalid_1's auc: 0.733866\n",
      "[400]\ttraining's auc: 0.91842\tvalid_1's auc: 0.73643\n",
      "[500]\ttraining's auc: 0.938125\tvalid_1's auc: 0.736487\n",
      "[600]\ttraining's auc: 0.953047\tvalid_1's auc: 0.737339\n",
      "[700]\ttraining's auc: 0.964313\tvalid_1's auc: 0.738334\n",
      "Early stopping, best iteration is:\n",
      "[682]\ttraining's auc: 0.962637\tvalid_1's auc: 0.738679\n",
      "Partial score of fold 0 is: 0.7386786874991991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.801911\tvalid_1's auc: 0.716794\n",
      "[200]\ttraining's auc: 0.855608\tvalid_1's auc: 0.728255\n",
      "[300]\ttraining's auc: 0.89167\tvalid_1's auc: 0.731242\n",
      "[400]\ttraining's auc: 0.919123\tvalid_1's auc: 0.733634\n",
      "[500]\ttraining's auc: 0.938951\tvalid_1's auc: 0.733117\n",
      "Early stopping, best iteration is:\n",
      "[404]\ttraining's auc: 0.91971\tvalid_1's auc: 0.733837\n",
      "Partial score of fold 1 is: 0.7338365794052262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.798327\tvalid_1's auc: 0.739014\n",
      "[200]\ttraining's auc: 0.853931\tvalid_1's auc: 0.75047\n",
      "[300]\ttraining's auc: 0.890847\tvalid_1's auc: 0.752656\n",
      "[400]\ttraining's auc: 0.918968\tvalid_1's auc: 0.752942\n",
      "Early stopping, best iteration is:\n",
      "[306]\ttraining's auc: 0.893106\tvalid_1's auc: 0.75306\n",
      "Partial score of fold 2 is: 0.7530599052047438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.798558\tvalid_1's auc: 0.720199\n",
      "[200]\ttraining's auc: 0.851865\tvalid_1's auc: 0.727559\n",
      "[300]\ttraining's auc: 0.88905\tvalid_1's auc: 0.729117\n",
      "[400]\ttraining's auc: 0.91555\tvalid_1's auc: 0.73098\n",
      "[500]\ttraining's auc: 0.936325\tvalid_1's auc: 0.731297\n",
      "[600]\ttraining's auc: 0.951747\tvalid_1's auc: 0.732029\n",
      "[700]\ttraining's auc: 0.963427\tvalid_1's auc: 0.7314\n",
      "Early stopping, best iteration is:\n",
      "[627]\ttraining's auc: 0.955142\tvalid_1's auc: 0.732594\n",
      "Partial score of fold 3 is: 0.7325943412338729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.800518\tvalid_1's auc: 0.736466\n",
      "[200]\ttraining's auc: 0.855742\tvalid_1's auc: 0.745762\n",
      "[300]\ttraining's auc: 0.89214\tvalid_1's auc: 0.747345\n",
      "[400]\ttraining's auc: 0.917243\tvalid_1's auc: 0.747822\n",
      "[500]\ttraining's auc: 0.937796\tvalid_1's auc: 0.748792\n",
      "Early stopping, best iteration is:\n",
      "[494]\ttraining's auc: 0.936672\tvalid_1's auc: 0.748855\n",
      "Partial score of fold 4 is: 0.7488549271165179\n",
      "AUC score is:  0.7408489989012229\n",
      "################### 4 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.764472\tvalid_1's auc: 0.731757\n",
      "[200]\ttraining's auc: 0.803445\tvalid_1's auc: 0.748178\n",
      "[300]\ttraining's auc: 0.829973\tvalid_1's auc: 0.75331\n",
      "[400]\ttraining's auc: 0.851676\tvalid_1's auc: 0.754821\n",
      "[500]\ttraining's auc: 0.870138\tvalid_1's auc: 0.75537\n",
      "[600]\ttraining's auc: 0.886052\tvalid_1's auc: 0.756673\n",
      "[700]\ttraining's auc: 0.899784\tvalid_1's auc: 0.757083\n",
      "[800]\ttraining's auc: 0.911993\tvalid_1's auc: 0.757658\n",
      "[900]\ttraining's auc: 0.922323\tvalid_1's auc: 0.757594\n",
      "[1000]\ttraining's auc: 0.931799\tvalid_1's auc: 0.757459\n",
      "Early stopping, best iteration is:\n",
      "[934]\ttraining's auc: 0.925706\tvalid_1's auc: 0.757806\n",
      "Partial score of fold 0 is: 0.757806069571661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.764075\tvalid_1's auc: 0.727514\n",
      "[200]\ttraining's auc: 0.802387\tvalid_1's auc: 0.743687\n",
      "[300]\ttraining's auc: 0.828886\tvalid_1's auc: 0.74908\n",
      "[400]\ttraining's auc: 0.850826\tvalid_1's auc: 0.751305\n",
      "[500]\ttraining's auc: 0.86962\tvalid_1's auc: 0.752197\n",
      "[600]\ttraining's auc: 0.885411\tvalid_1's auc: 0.752368\n",
      "[700]\ttraining's auc: 0.899253\tvalid_1's auc: 0.753515\n",
      "[800]\ttraining's auc: 0.911866\tvalid_1's auc: 0.753051\n",
      "Early stopping, best iteration is:\n",
      "[701]\ttraining's auc: 0.899415\tvalid_1's auc: 0.753577\n",
      "Partial score of fold 1 is: 0.7535773836552357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.765672\tvalid_1's auc: 0.726643\n",
      "[200]\ttraining's auc: 0.803279\tvalid_1's auc: 0.740484\n",
      "[300]\ttraining's auc: 0.830122\tvalid_1's auc: 0.744867\n",
      "[400]\ttraining's auc: 0.851762\tvalid_1's auc: 0.746134\n",
      "[500]\ttraining's auc: 0.86993\tvalid_1's auc: 0.747102\n",
      "[600]\ttraining's auc: 0.885871\tvalid_1's auc: 0.747643\n",
      "[700]\ttraining's auc: 0.899944\tvalid_1's auc: 0.748756\n",
      "[800]\ttraining's auc: 0.91199\tvalid_1's auc: 0.748835\n",
      "Early stopping, best iteration is:\n",
      "[726]\ttraining's auc: 0.903246\tvalid_1's auc: 0.749198\n",
      "Partial score of fold 2 is: 0.7491982534280603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.763904\tvalid_1's auc: 0.732184\n",
      "[200]\ttraining's auc: 0.802047\tvalid_1's auc: 0.745374\n",
      "[300]\ttraining's auc: 0.828605\tvalid_1's auc: 0.750491\n",
      "[400]\ttraining's auc: 0.851115\tvalid_1's auc: 0.751651\n",
      "[500]\ttraining's auc: 0.869824\tvalid_1's auc: 0.753438\n",
      "[600]\ttraining's auc: 0.885768\tvalid_1's auc: 0.754418\n",
      "[700]\ttraining's auc: 0.899814\tvalid_1's auc: 0.755214\n",
      "Early stopping, best iteration is:\n",
      "[683]\ttraining's auc: 0.897388\tvalid_1's auc: 0.755392\n",
      "Partial score of fold 3 is: 0.7553921490351378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.762825\tvalid_1's auc: 0.73353\n",
      "[200]\ttraining's auc: 0.801769\tvalid_1's auc: 0.750188\n",
      "[300]\ttraining's auc: 0.828226\tvalid_1's auc: 0.756565\n",
      "[400]\ttraining's auc: 0.850185\tvalid_1's auc: 0.758591\n",
      "[500]\ttraining's auc: 0.868475\tvalid_1's auc: 0.759644\n",
      "[600]\ttraining's auc: 0.884444\tvalid_1's auc: 0.760899\n",
      "[700]\ttraining's auc: 0.898391\tvalid_1's auc: 0.761253\n",
      "[800]\ttraining's auc: 0.910892\tvalid_1's auc: 0.761192\n",
      "Early stopping, best iteration is:\n",
      "[751]\ttraining's auc: 0.904762\tvalid_1's auc: 0.761578\n",
      "Partial score of fold 4 is: 0.7615782992425381\n",
      "AUC score is:  0.7555031839744257\n",
      "################### 5 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.83934\tvalid_1's auc: 0.803158\n",
      "[200]\ttraining's auc: 0.876265\tvalid_1's auc: 0.822359\n",
      "[300]\ttraining's auc: 0.899536\tvalid_1's auc: 0.828979\n",
      "[400]\ttraining's auc: 0.917957\tvalid_1's auc: 0.832589\n",
      "[500]\ttraining's auc: 0.932204\tvalid_1's auc: 0.833427\n",
      "Early stopping, best iteration is:\n",
      "[486]\ttraining's auc: 0.930383\tvalid_1's auc: 0.833767\n",
      "Partial score of fold 0 is: 0.8337672975627175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.839781\tvalid_1's auc: 0.804782\n",
      "[200]\ttraining's auc: 0.876566\tvalid_1's auc: 0.825484\n",
      "[300]\ttraining's auc: 0.899741\tvalid_1's auc: 0.832545\n",
      "[400]\ttraining's auc: 0.917519\tvalid_1's auc: 0.835125\n",
      "[500]\ttraining's auc: 0.931828\tvalid_1's auc: 0.835627\n",
      "[600]\ttraining's auc: 0.943331\tvalid_1's auc: 0.836545\n",
      "[700]\ttraining's auc: 0.952923\tvalid_1's auc: 0.837091\n",
      "[800]\ttraining's auc: 0.961127\tvalid_1's auc: 0.836821\n",
      "Early stopping, best iteration is:\n",
      "[718]\ttraining's auc: 0.95466\tvalid_1's auc: 0.837293\n",
      "Partial score of fold 1 is: 0.8372933964229384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.839604\tvalid_1's auc: 0.801976\n",
      "[200]\ttraining's auc: 0.878211\tvalid_1's auc: 0.822964\n",
      "[300]\ttraining's auc: 0.901232\tvalid_1's auc: 0.828682\n",
      "[400]\ttraining's auc: 0.918328\tvalid_1's auc: 0.832181\n",
      "[500]\ttraining's auc: 0.933114\tvalid_1's auc: 0.832824\n",
      "[600]\ttraining's auc: 0.944462\tvalid_1's auc: 0.834791\n",
      "[700]\ttraining's auc: 0.953938\tvalid_1's auc: 0.83488\n",
      "[800]\ttraining's auc: 0.96193\tvalid_1's auc: 0.834535\n",
      "Early stopping, best iteration is:\n",
      "[731]\ttraining's auc: 0.956561\tvalid_1's auc: 0.835028\n",
      "Partial score of fold 2 is: 0.8350279364937655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.839467\tvalid_1's auc: 0.809786\n",
      "[200]\ttraining's auc: 0.876991\tvalid_1's auc: 0.829737\n",
      "[300]\ttraining's auc: 0.900433\tvalid_1's auc: 0.836355\n",
      "[400]\ttraining's auc: 0.918125\tvalid_1's auc: 0.839982\n",
      "[500]\ttraining's auc: 0.932034\tvalid_1's auc: 0.840715\n",
      "[600]\ttraining's auc: 0.943614\tvalid_1's auc: 0.840918\n",
      "[700]\ttraining's auc: 0.953062\tvalid_1's auc: 0.840899\n",
      "Early stopping, best iteration is:\n",
      "[650]\ttraining's auc: 0.948453\tvalid_1's auc: 0.841326\n",
      "Partial score of fold 3 is: 0.8413257566065692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.839055\tvalid_1's auc: 0.80334\n",
      "[200]\ttraining's auc: 0.876187\tvalid_1's auc: 0.824028\n",
      "[300]\ttraining's auc: 0.899735\tvalid_1's auc: 0.830869\n",
      "[400]\ttraining's auc: 0.917976\tvalid_1's auc: 0.83429\n",
      "[500]\ttraining's auc: 0.93208\tvalid_1's auc: 0.835232\n",
      "[600]\ttraining's auc: 0.943802\tvalid_1's auc: 0.836347\n",
      "[700]\ttraining's auc: 0.953049\tvalid_1's auc: 0.836494\n",
      "[800]\ttraining's auc: 0.960932\tvalid_1's auc: 0.836417\n",
      "Early stopping, best iteration is:\n",
      "[727]\ttraining's auc: 0.955327\tvalid_1's auc: 0.836768\n",
      "Partial score of fold 4 is: 0.8367679228122898\n",
      "AUC score is:  0.8367614554684554\n",
      "################### 6 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.872205\tvalid_1's auc: 0.823121\n",
      "[200]\ttraining's auc: 0.914228\tvalid_1's auc: 0.83502\n",
      "[300]\ttraining's auc: 0.940463\tvalid_1's auc: 0.840078\n",
      "[400]\ttraining's auc: 0.958052\tvalid_1's auc: 0.841454\n",
      "[500]\ttraining's auc: 0.970399\tvalid_1's auc: 0.842669\n",
      "[600]\ttraining's auc: 0.979453\tvalid_1's auc: 0.841392\n",
      "Early stopping, best iteration is:\n",
      "[502]\ttraining's auc: 0.970588\tvalid_1's auc: 0.842671\n",
      "Partial score of fold 0 is: 0.8426713991948379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.872059\tvalid_1's auc: 0.809253\n",
      "[200]\ttraining's auc: 0.914008\tvalid_1's auc: 0.823875\n",
      "[300]\ttraining's auc: 0.940223\tvalid_1's auc: 0.827576\n",
      "[400]\ttraining's auc: 0.95695\tvalid_1's auc: 0.829168\n",
      "[500]\ttraining's auc: 0.969097\tvalid_1's auc: 0.829936\n",
      "[600]\ttraining's auc: 0.978291\tvalid_1's auc: 0.831053\n",
      "[700]\ttraining's auc: 0.98475\tvalid_1's auc: 0.8313\n",
      "[800]\ttraining's auc: 0.989496\tvalid_1's auc: 0.829847\n",
      "Early stopping, best iteration is:\n",
      "[705]\ttraining's auc: 0.985042\tvalid_1's auc: 0.831424\n",
      "Partial score of fold 1 is: 0.8314244021175339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.872116\tvalid_1's auc: 0.802646\n",
      "[200]\ttraining's auc: 0.913488\tvalid_1's auc: 0.821758\n",
      "[300]\ttraining's auc: 0.940126\tvalid_1's auc: 0.825756\n",
      "[400]\ttraining's auc: 0.957769\tvalid_1's auc: 0.828535\n",
      "[500]\ttraining's auc: 0.96979\tvalid_1's auc: 0.828622\n",
      "[600]\ttraining's auc: 0.979069\tvalid_1's auc: 0.828888\n",
      "Early stopping, best iteration is:\n",
      "[512]\ttraining's auc: 0.971246\tvalid_1's auc: 0.829179\n",
      "Partial score of fold 2 is: 0.8291785822527826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.876209\tvalid_1's auc: 0.79368\n",
      "[200]\ttraining's auc: 0.916409\tvalid_1's auc: 0.805846\n",
      "[300]\ttraining's auc: 0.941748\tvalid_1's auc: 0.811562\n",
      "[400]\ttraining's auc: 0.959476\tvalid_1's auc: 0.813793\n",
      "[500]\ttraining's auc: 0.971436\tvalid_1's auc: 0.814209\n",
      "[600]\ttraining's auc: 0.979744\tvalid_1's auc: 0.814737\n",
      "Early stopping, best iteration is:\n",
      "[572]\ttraining's auc: 0.977689\tvalid_1's auc: 0.815109\n",
      "Partial score of fold 3 is: 0.8151094135704744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.872858\tvalid_1's auc: 0.808971\n",
      "[200]\ttraining's auc: 0.915419\tvalid_1's auc: 0.818667\n",
      "[300]\ttraining's auc: 0.941473\tvalid_1's auc: 0.822535\n",
      "[400]\ttraining's auc: 0.95842\tvalid_1's auc: 0.822618\n",
      "Early stopping, best iteration is:\n",
      "[313]\ttraining's auc: 0.944031\tvalid_1's auc: 0.823536\n",
      "Partial score of fold 4 is: 0.8235359597822648\n",
      "AUC score is:  0.8280569385165586\n",
      "################### 7 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.934108\tvalid_1's auc: 0.790538\n",
      "[200]\ttraining's auc: 0.976837\tvalid_1's auc: 0.805553\n",
      "[300]\ttraining's auc: 0.990505\tvalid_1's auc: 0.809141\n",
      "[400]\ttraining's auc: 0.995997\tvalid_1's auc: 0.810182\n",
      "[500]\ttraining's auc: 0.99838\tvalid_1's auc: 0.811601\n",
      "Early stopping, best iteration is:\n",
      "[488]\ttraining's auc: 0.998202\tvalid_1's auc: 0.811754\n",
      "Partial score of fold 0 is: 0.8117543764828019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.937235\tvalid_1's auc: 0.798813\n",
      "[200]\ttraining's auc: 0.977176\tvalid_1's auc: 0.816285\n",
      "[300]\ttraining's auc: 0.990728\tvalid_1's auc: 0.818198\n",
      "[400]\ttraining's auc: 0.996082\tvalid_1's auc: 0.818556\n",
      "[500]\ttraining's auc: 0.998414\tvalid_1's auc: 0.818831\n",
      "Early stopping, best iteration is:\n",
      "[427]\ttraining's auc: 0.996896\tvalid_1's auc: 0.819727\n",
      "Partial score of fold 1 is: 0.8197272113431798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.933351\tvalid_1's auc: 0.804439\n",
      "[200]\ttraining's auc: 0.97615\tvalid_1's auc: 0.816715\n",
      "[300]\ttraining's auc: 0.990154\tvalid_1's auc: 0.822747\n",
      "[400]\ttraining's auc: 0.99612\tvalid_1's auc: 0.819258\n",
      "Early stopping, best iteration is:\n",
      "[302]\ttraining's auc: 0.990473\tvalid_1's auc: 0.822854\n",
      "Partial score of fold 2 is: 0.8228537625767517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.932573\tvalid_1's auc: 0.793294\n",
      "[200]\ttraining's auc: 0.975091\tvalid_1's auc: 0.807013\n",
      "[300]\ttraining's auc: 0.989503\tvalid_1's auc: 0.806275\n",
      "Early stopping, best iteration is:\n",
      "[231]\ttraining's auc: 0.980479\tvalid_1's auc: 0.808004\n",
      "Partial score of fold 3 is: 0.8080035725660111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.932812\tvalid_1's auc: 0.799338\n",
      "[200]\ttraining's auc: 0.973556\tvalid_1's auc: 0.811893\n",
      "[300]\ttraining's auc: 0.989269\tvalid_1's auc: 0.815704\n",
      "[400]\ttraining's auc: 0.995519\tvalid_1's auc: 0.816216\n",
      "[500]\ttraining's auc: 0.998145\tvalid_1's auc: 0.817312\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttraining's auc: 0.99741\tvalid_1's auc: 0.818436\n",
      "Partial score of fold 4 is: 0.818436096536777\n",
      "AUC score is:  0.8148541893826009\n",
      "################### 8 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.949229\tvalid_1's auc: 0.839163\n",
      "[200]\ttraining's auc: 0.981712\tvalid_1's auc: 0.843448\n",
      "Early stopping, best iteration is:\n",
      "[189]\ttraining's auc: 0.979562\tvalid_1's auc: 0.843883\n",
      "Partial score of fold 0 is: 0.8438834584970686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.952769\tvalid_1's auc: 0.837373\n",
      "[200]\ttraining's auc: 0.982212\tvalid_1's auc: 0.838697\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's auc: 0.968434\tvalid_1's auc: 0.84001\n",
      "Partial score of fold 1 is: 0.8400098243587323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.947122\tvalid_1's auc: 0.826963\n",
      "[200]\ttraining's auc: 0.980373\tvalid_1's auc: 0.836363\n",
      "[300]\ttraining's auc: 0.992197\tvalid_1's auc: 0.841016\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's auc: 0.990294\tvalid_1's auc: 0.842004\n",
      "Partial score of fold 2 is: 0.8420036689131368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.949305\tvalid_1's auc: 0.850401\n",
      "[200]\ttraining's auc: 0.979871\tvalid_1's auc: 0.858074\n",
      "[300]\ttraining's auc: 0.991584\tvalid_1's auc: 0.858767\n",
      "[400]\ttraining's auc: 0.996538\tvalid_1's auc: 0.859332\n",
      "[500]\ttraining's auc: 0.998679\tvalid_1's auc: 0.857761\n",
      "Early stopping, best iteration is:\n",
      "[405]\ttraining's auc: 0.996688\tvalid_1's auc: 0.859908\n",
      "Partial score of fold 3 is: 0.8599083685327696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.952166\tvalid_1's auc: 0.823305\n",
      "[200]\ttraining's auc: 0.981188\tvalid_1's auc: 0.82868\n",
      "Early stopping, best iteration is:\n",
      "[156]\ttraining's auc: 0.972754\tvalid_1's auc: 0.829629\n",
      "Partial score of fold 4 is: 0.8296290931433757\n",
      "AUC score is:  0.8418859569958494\n",
      "################### 9 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.975629\tvalid_1's auc: 0.890506\n",
      "[200]\ttraining's auc: 0.994138\tvalid_1's auc: 0.895447\n",
      "Early stopping, best iteration is:\n",
      "[174]\ttraining's auc: 0.991569\tvalid_1's auc: 0.896672\n",
      "Partial score of fold 0 is: 0.8966715737154047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.974899\tvalid_1's auc: 0.854625\n",
      "[200]\ttraining's auc: 0.993709\tvalid_1's auc: 0.862124\n",
      "[300]\ttraining's auc: 0.998265\tvalid_1's auc: 0.863301\n",
      "Early stopping, best iteration is:\n",
      "[290]\ttraining's auc: 0.997987\tvalid_1's auc: 0.864077\n",
      "Partial score of fold 1 is: 0.8640765590055651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.973743\tvalid_1's auc: 0.874224\n",
      "[200]\ttraining's auc: 0.99421\tvalid_1's auc: 0.878438\n",
      "[300]\ttraining's auc: 0.998665\tvalid_1's auc: 0.880738\n",
      "[400]\ttraining's auc: 0.999749\tvalid_1's auc: 0.882234\n",
      "[500]\ttraining's auc: 0.99997\tvalid_1's auc: 0.880781\n",
      "Early stopping, best iteration is:\n",
      "[439]\ttraining's auc: 0.999881\tvalid_1's auc: 0.882887\n",
      "Partial score of fold 2 is: 0.8828874301339926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.976109\tvalid_1's auc: 0.881011\n",
      "[200]\ttraining's auc: 0.994556\tvalid_1's auc: 0.888587\n",
      "Early stopping, best iteration is:\n",
      "[192]\ttraining's auc: 0.993796\tvalid_1's auc: 0.890548\n",
      "Partial score of fold 3 is: 0.8905481547413907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.974019\tvalid_1's auc: 0.862851\n",
      "[200]\ttraining's auc: 0.993537\tvalid_1's auc: 0.865535\n",
      "[300]\ttraining's auc: 0.998411\tvalid_1's auc: 0.868514\n",
      "[400]\ttraining's auc: 0.99964\tvalid_1's auc: 0.867957\n",
      "Early stopping, best iteration is:\n",
      "[359]\ttraining's auc: 0.999339\tvalid_1's auc: 0.869456\n",
      "Partial score of fold 4 is: 0.8694562657771913\n",
      "AUC score is:  0.8757060088536831\n",
      "################### 10 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.92716\tvalid_1's auc: 0.896927\n",
      "[200]\ttraining's auc: 0.951558\tvalid_1's auc: 0.906495\n",
      "[300]\ttraining's auc: 0.966022\tvalid_1's auc: 0.910113\n",
      "[400]\ttraining's auc: 0.975615\tvalid_1's auc: 0.911783\n",
      "[500]\ttraining's auc: 0.982509\tvalid_1's auc: 0.913376\n",
      "[600]\ttraining's auc: 0.987325\tvalid_1's auc: 0.914415\n",
      "[700]\ttraining's auc: 0.991091\tvalid_1's auc: 0.914894\n",
      "Early stopping, best iteration is:\n",
      "[680]\ttraining's auc: 0.990453\tvalid_1's auc: 0.915191\n",
      "Partial score of fold 0 is: 0.915191152910787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.926744\tvalid_1's auc: 0.894499\n",
      "[200]\ttraining's auc: 0.952083\tvalid_1's auc: 0.905781\n",
      "[300]\ttraining's auc: 0.966672\tvalid_1's auc: 0.910248\n",
      "[400]\ttraining's auc: 0.976549\tvalid_1's auc: 0.911233\n",
      "[500]\ttraining's auc: 0.983199\tvalid_1's auc: 0.912279\n",
      "[600]\ttraining's auc: 0.987877\tvalid_1's auc: 0.913311\n",
      "[700]\ttraining's auc: 0.99151\tvalid_1's auc: 0.913104\n",
      "Early stopping, best iteration is:\n",
      "[601]\ttraining's auc: 0.987921\tvalid_1's auc: 0.913394\n",
      "Partial score of fold 1 is: 0.9133936429345475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.927439\tvalid_1's auc: 0.894242\n",
      "[200]\ttraining's auc: 0.952616\tvalid_1's auc: 0.90612\n",
      "[300]\ttraining's auc: 0.966675\tvalid_1's auc: 0.909903\n",
      "[400]\ttraining's auc: 0.976122\tvalid_1's auc: 0.909846\n",
      "[500]\ttraining's auc: 0.982798\tvalid_1's auc: 0.910001\n",
      "Early stopping, best iteration is:\n",
      "[433]\ttraining's auc: 0.978549\tvalid_1's auc: 0.910386\n",
      "Partial score of fold 2 is: 0.9103859268450358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.926695\tvalid_1's auc: 0.896574\n",
      "[200]\ttraining's auc: 0.953339\tvalid_1's auc: 0.906923\n",
      "[300]\ttraining's auc: 0.967698\tvalid_1's auc: 0.911063\n",
      "[400]\ttraining's auc: 0.977019\tvalid_1's auc: 0.911402\n",
      "[500]\ttraining's auc: 0.983599\tvalid_1's auc: 0.912204\n",
      "[600]\ttraining's auc: 0.988306\tvalid_1's auc: 0.912465\n",
      "Early stopping, best iteration is:\n",
      "[527]\ttraining's auc: 0.985035\tvalid_1's auc: 0.912822\n",
      "Partial score of fold 3 is: 0.912821624658012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.927723\tvalid_1's auc: 0.894382\n",
      "[200]\ttraining's auc: 0.953844\tvalid_1's auc: 0.905101\n",
      "[300]\ttraining's auc: 0.967302\tvalid_1's auc: 0.908667\n",
      "[400]\ttraining's auc: 0.976764\tvalid_1's auc: 0.909618\n",
      "[500]\ttraining's auc: 0.983418\tvalid_1's auc: 0.910419\n",
      "[600]\ttraining's auc: 0.988062\tvalid_1's auc: 0.909602\n",
      "Early stopping, best iteration is:\n",
      "[502]\ttraining's auc: 0.983509\tvalid_1's auc: 0.910506\n",
      "Partial score of fold 4 is: 0.9105055927393466\n",
      "AUC score is:  0.91238622058242\n",
      "################### 11 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.892722\tvalid_1's auc: 0.861546\n",
      "[200]\ttraining's auc: 0.92116\tvalid_1's auc: 0.878391\n",
      "[300]\ttraining's auc: 0.938661\tvalid_1's auc: 0.884764\n",
      "[400]\ttraining's auc: 0.951675\tvalid_1's auc: 0.886\n",
      "[500]\ttraining's auc: 0.96134\tvalid_1's auc: 0.887813\n",
      "[600]\ttraining's auc: 0.968664\tvalid_1's auc: 0.888013\n",
      "[700]\ttraining's auc: 0.974965\tvalid_1's auc: 0.888476\n",
      "[800]\ttraining's auc: 0.979981\tvalid_1's auc: 0.88894\n",
      "[900]\ttraining's auc: 0.984232\tvalid_1's auc: 0.88892\n",
      "Early stopping, best iteration is:\n",
      "[821]\ttraining's auc: 0.981016\tvalid_1's auc: 0.889179\n",
      "Partial score of fold 0 is: 0.8891794834987478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.892102\tvalid_1's auc: 0.867161\n",
      "[200]\ttraining's auc: 0.920728\tvalid_1's auc: 0.882321\n",
      "[300]\ttraining's auc: 0.938284\tvalid_1's auc: 0.888273\n",
      "[400]\ttraining's auc: 0.951098\tvalid_1's auc: 0.890204\n",
      "[500]\ttraining's auc: 0.960915\tvalid_1's auc: 0.891921\n",
      "[600]\ttraining's auc: 0.968908\tvalid_1's auc: 0.89251\n",
      "Early stopping, best iteration is:\n",
      "[556]\ttraining's auc: 0.965665\tvalid_1's auc: 0.892671\n",
      "Partial score of fold 1 is: 0.8926713446396665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.89174\tvalid_1's auc: 0.868844\n",
      "[200]\ttraining's auc: 0.921081\tvalid_1's auc: 0.883354\n",
      "[300]\ttraining's auc: 0.939589\tvalid_1's auc: 0.888251\n",
      "[400]\ttraining's auc: 0.952823\tvalid_1's auc: 0.888858\n",
      "[500]\ttraining's auc: 0.962432\tvalid_1's auc: 0.890084\n",
      "[600]\ttraining's auc: 0.969833\tvalid_1's auc: 0.890607\n",
      "[700]\ttraining's auc: 0.975832\tvalid_1's auc: 0.89037\n",
      "Early stopping, best iteration is:\n",
      "[606]\ttraining's auc: 0.970243\tvalid_1's auc: 0.890656\n",
      "Partial score of fold 2 is: 0.8906563556359453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.89293\tvalid_1's auc: 0.859613\n",
      "[200]\ttraining's auc: 0.922735\tvalid_1's auc: 0.878207\n",
      "[300]\ttraining's auc: 0.940872\tvalid_1's auc: 0.882904\n",
      "[400]\ttraining's auc: 0.953368\tvalid_1's auc: 0.885335\n",
      "[500]\ttraining's auc: 0.963037\tvalid_1's auc: 0.886377\n",
      "[600]\ttraining's auc: 0.970458\tvalid_1's auc: 0.886974\n",
      "[700]\ttraining's auc: 0.976343\tvalid_1's auc: 0.887028\n",
      "[800]\ttraining's auc: 0.981165\tvalid_1's auc: 0.887366\n",
      "[900]\ttraining's auc: 0.985151\tvalid_1's auc: 0.887649\n",
      "Early stopping, best iteration is:\n",
      "[880]\ttraining's auc: 0.984357\tvalid_1's auc: 0.887865\n",
      "Partial score of fold 3 is: 0.8878648769097397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.891847\tvalid_1's auc: 0.863203\n",
      "[200]\ttraining's auc: 0.92092\tvalid_1's auc: 0.881557\n",
      "[300]\ttraining's auc: 0.938496\tvalid_1's auc: 0.886627\n",
      "[400]\ttraining's auc: 0.951527\tvalid_1's auc: 0.887953\n",
      "[500]\ttraining's auc: 0.96122\tvalid_1's auc: 0.889705\n",
      "[600]\ttraining's auc: 0.968959\tvalid_1's auc: 0.890598\n",
      "[700]\ttraining's auc: 0.975262\tvalid_1's auc: 0.891309\n",
      "[800]\ttraining's auc: 0.980112\tvalid_1's auc: 0.891853\n",
      "[900]\ttraining's auc: 0.984341\tvalid_1's auc: 0.891646\n",
      "Early stopping, best iteration is:\n",
      "[812]\ttraining's auc: 0.980625\tvalid_1's auc: 0.891974\n",
      "Partial score of fold 4 is: 0.8919737864735117\n",
      "AUC score is:  0.8902325875931045\n",
      "################### 12 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.926776\tvalid_1's auc: 0.861157\n",
      "[200]\ttraining's auc: 0.960163\tvalid_1's auc: 0.871191\n",
      "[300]\ttraining's auc: 0.978577\tvalid_1's auc: 0.872874\n",
      "[400]\ttraining's auc: 0.987995\tvalid_1's auc: 0.873346\n",
      "Early stopping, best iteration is:\n",
      "[351]\ttraining's auc: 0.984099\tvalid_1's auc: 0.874074\n",
      "Partial score of fold 0 is: 0.8740738865124101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.92766\tvalid_1's auc: 0.861345\n",
      "[200]\ttraining's auc: 0.961384\tvalid_1's auc: 0.870877\n",
      "[300]\ttraining's auc: 0.977922\tvalid_1's auc: 0.872606\n",
      "[400]\ttraining's auc: 0.987264\tvalid_1's auc: 0.87176\n",
      "Early stopping, best iteration is:\n",
      "[319]\ttraining's auc: 0.980094\tvalid_1's auc: 0.873252\n",
      "Partial score of fold 1 is: 0.873251853726614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.927908\tvalid_1's auc: 0.865679\n",
      "[200]\ttraining's auc: 0.962704\tvalid_1's auc: 0.874339\n",
      "[300]\ttraining's auc: 0.978872\tvalid_1's auc: 0.877985\n",
      "[400]\ttraining's auc: 0.988244\tvalid_1's auc: 0.878433\n",
      "Early stopping, best iteration is:\n",
      "[354]\ttraining's auc: 0.984633\tvalid_1's auc: 0.879167\n",
      "Partial score of fold 2 is: 0.8791668411508597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.927193\tvalid_1's auc: 0.861163\n",
      "[200]\ttraining's auc: 0.962313\tvalid_1's auc: 0.873238\n",
      "[300]\ttraining's auc: 0.978663\tvalid_1's auc: 0.874597\n",
      "[400]\ttraining's auc: 0.987659\tvalid_1's auc: 0.875332\n",
      "[500]\ttraining's auc: 0.993291\tvalid_1's auc: 0.875084\n",
      "Early stopping, best iteration is:\n",
      "[478]\ttraining's auc: 0.992311\tvalid_1's auc: 0.875976\n",
      "Partial score of fold 3 is: 0.8759759434461256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.926445\tvalid_1's auc: 0.867906\n",
      "[200]\ttraining's auc: 0.960349\tvalid_1's auc: 0.877895\n",
      "[300]\ttraining's auc: 0.97805\tvalid_1's auc: 0.879363\n",
      "[400]\ttraining's auc: 0.987458\tvalid_1's auc: 0.879918\n",
      "[500]\ttraining's auc: 0.992932\tvalid_1's auc: 0.879823\n",
      "[600]\ttraining's auc: 0.995945\tvalid_1's auc: 0.87976\n",
      "Early stopping, best iteration is:\n",
      "[525]\ttraining's auc: 0.993834\tvalid_1's auc: 0.880799\n",
      "Partial score of fold 4 is: 0.880798815469165\n",
      "AUC score is:  0.8765328184220949\n",
      "################### 13 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.967544\tvalid_1's auc: 0.942344\n",
      "[200]\ttraining's auc: 0.98439\tvalid_1's auc: 0.947535\n",
      "[300]\ttraining's auc: 0.99171\tvalid_1's auc: 0.950732\n",
      "[400]\ttraining's auc: 0.995475\tvalid_1's auc: 0.95175\n",
      "Early stopping, best iteration is:\n",
      "[359]\ttraining's auc: 0.99418\tvalid_1's auc: 0.952025\n",
      "Partial score of fold 0 is: 0.9520254603197594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.969031\tvalid_1's auc: 0.945254\n",
      "[200]\ttraining's auc: 0.985094\tvalid_1's auc: 0.950758\n",
      "[300]\ttraining's auc: 0.992139\tvalid_1's auc: 0.951677\n",
      "[400]\ttraining's auc: 0.99552\tvalid_1's auc: 0.951457\n",
      "Early stopping, best iteration is:\n",
      "[370]\ttraining's auc: 0.994668\tvalid_1's auc: 0.951938\n",
      "Partial score of fold 1 is: 0.9519375456908147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.968666\tvalid_1's auc: 0.938447\n",
      "[200]\ttraining's auc: 0.984838\tvalid_1's auc: 0.944198\n",
      "[300]\ttraining's auc: 0.991641\tvalid_1's auc: 0.945354\n",
      "Early stopping, best iteration is:\n",
      "[291]\ttraining's auc: 0.991218\tvalid_1's auc: 0.945428\n",
      "Partial score of fold 2 is: 0.9454280511744414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.96648\tvalid_1's auc: 0.935458\n",
      "[200]\ttraining's auc: 0.983731\tvalid_1's auc: 0.943851\n",
      "[300]\ttraining's auc: 0.991491\tvalid_1's auc: 0.945353\n",
      "[400]\ttraining's auc: 0.995398\tvalid_1's auc: 0.945892\n",
      "Early stopping, best iteration is:\n",
      "[345]\ttraining's auc: 0.993602\tvalid_1's auc: 0.946244\n",
      "Partial score of fold 3 is: 0.9462436667686034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.970521\tvalid_1's auc: 0.912895\n",
      "[200]\ttraining's auc: 0.985466\tvalid_1's auc: 0.920643\n",
      "[300]\ttraining's auc: 0.992133\tvalid_1's auc: 0.92348\n",
      "[400]\ttraining's auc: 0.995579\tvalid_1's auc: 0.926247\n",
      "[500]\ttraining's auc: 0.997681\tvalid_1's auc: 0.927421\n",
      "[600]\ttraining's auc: 0.998872\tvalid_1's auc: 0.92846\n",
      "[700]\ttraining's auc: 0.99948\tvalid_1's auc: 0.928595\n",
      "[800]\ttraining's auc: 0.999784\tvalid_1's auc: 0.929487\n",
      "[900]\ttraining's auc: 0.999922\tvalid_1's auc: 0.9292\n",
      "Early stopping, best iteration is:\n",
      "[837]\ttraining's auc: 0.999852\tvalid_1's auc: 0.929872\n",
      "Partial score of fold 4 is: 0.9298715171555684\n",
      "AUC score is:  0.9420280468776825\n"
     ]
    }
   ],
   "source": [
    "print(\"################### 1 ###################\")\n",
    "lgb_model_130123 = Lgb_Model(train_after, test_after, features, '130123')\n",
    "print(\"################### 2 ###################\")\n",
    "lgb_model_130125 = Lgb_Model(train_after, test_after, features, '130125')\n",
    "print(\"################### 3 ###################\")\n",
    "lgb_model_130129 = Lgb_Model(train_after, test_after, features, '130129')\n",
    "print(\"################### 4 ###################\")\n",
    "lgb_model_130131 = Lgb_Model(train_after, test_after, features, '130131')\n",
    "print(\"################### 5 ###################\")\n",
    "lgb_model_140307 = Lgb_Model(train_after, test_after, features, '140307')\n",
    "print(\"################### 6 ###################\")\n",
    "lgb_model_140313 = Lgb_Model(train_after, test_after, features, '140313')\n",
    "print(\"################### 7 ###################\")\n",
    "lgb_model_140316 = Lgb_Model(train_after, test_after, features, '140316')\n",
    "print(\"################### 8 ###################\")\n",
    "lgb_model_140317 = Lgb_Model(train_after, test_after, features, '140317')\n",
    "print(\"################### 9 ###################\")\n",
    "lgb_model_140321 = Lgb_Model(train_after, test_after, features, '140321')\n",
    "print(\"################### 10 ###################\")\n",
    "lgb_model_140501 = Lgb_Model(train_after, test_after, features, '140501')\n",
    "print(\"################### 11 ###################\")\n",
    "lgb_model_140505 = Lgb_Model(train_after, test_after, features, '140505')\n",
    "print(\"################### 12 ###################\")\n",
    "lgb_model_140641 = Lgb_Model(train_after, test_after, features, '140641')\n",
    "print(\"################### 13 ###################\")\n",
    "lgb_model_140691 = Lgb_Model(train_after, test_after, features, '140691')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"130123_AUC: {lgb_model_130123.score}\")\n",
    "# print(f\"130125_AUC: {lgb_model_130125.score}\")\n",
    "# print(f\"130129_AUC: {lgb_model_130129.score}\")\n",
    "# print(f\"130131_AUC: {lgb_model_130131.score}\")\n",
    "# print(f\"140307_AUC: {lgb_model_140307.score}\")\n",
    "# print(f\"140313_AUC: {lgb_model_140313.score}\")\n",
    "# print(f\"140316_AUC: {lgb_model_140316.score}\")\n",
    "# print(f\"140317_AUC: {lgb_model_140317.score}\")\n",
    "# print(f\"140321_AUC: {lgb_model_140321.score}\")\n",
    "# print(f\"140501_AUC: {lgb_model_140501.score}\")\n",
    "# print(f\"140505_AUC: {lgb_model_140505.score}\")\n",
    "# print(f\"140641_AUC: {lgb_model_140641.score}\")\n",
    "# print(f\"140691_AUC: {lgb_model_140691.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred = pd.DataFrame({\n",
    "   \"130123\" :  lgb_model_130123.oof_pred,\n",
    "   \"130125\" :  lgb_model_130125.oof_pred,\n",
    "   \"130129\" :  lgb_model_130129.oof_pred,\n",
    "   \"130131\" :  lgb_model_130131.oof_pred,\n",
    "   \"140307\" :  lgb_model_140307.oof_pred,\n",
    "   \"140313\" :  lgb_model_140313.oof_pred,\n",
    "   \"140316\" :  lgb_model_140316.oof_pred,\n",
    "   \"140317\" :  lgb_model_140317.oof_pred,\n",
    "   \"140321\" :  lgb_model_140321.oof_pred,\n",
    "   \"140501\" :  lgb_model_140501.oof_pred,\n",
    "   \"140505\" :  lgb_model_140505.oof_pred,\n",
    "   \"140641\" :  lgb_model_140641.oof_pred,\n",
    "   \"140691\" :  lgb_model_140691.oof_pred,\n",
    " })\n",
    "\n",
    "oof_pred.to_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_oof_pred.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8aba8388568f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "config = config_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE = \"case1\"\n",
    "path_w = f\"{CASE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'case1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130123_AUC: 0.7516361499218029\n",
      "130125_AUC: 0.8143444472271061\n",
      "130129_AUC: 0.7408489989012229\n",
      "130131_AUC: 0.7555031839744257\n",
      "140307_AUC: 0.8367614554684554\n",
      "140313_AUC: 0.8280569385165586\n",
      "140316_AUC: 0.8148541893826009\n",
      "140317_AUC: 0.8418859569958494\n",
      "140321_AUC: 0.8757060088536831\n",
      "140501_AUC: 0.91238622058242\n",
      "140505_AUC: 0.8902325875931045\n",
      "140641_AUC: 0.8765328184220949\n",
      "140691_AUC: 0.9420280468776825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_w = f'/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/model/{CASE}_lgb_score.txt'\n",
    "\n",
    "s1 = f\"130123_AUC: {lgb_model_130123.score}\"\n",
    "s2 = f\"130125_AUC: {lgb_model_130125.score}\"\n",
    "s3 = f\"130129_AUC: {lgb_model_130129.score}\"\n",
    "s4 = f\"130131_AUC: {lgb_model_130131.score}\"\n",
    "s5 = f\"140307_AUC: {lgb_model_140307.score}\"\n",
    "s6 = f\"140313_AUC: {lgb_model_140313.score}\"\n",
    "s7 = f\"140316_AUC: {lgb_model_140316.score}\"\n",
    "s8 = f\"140317_AUC: {lgb_model_140317.score}\"\n",
    "s9 = f\"140321_AUC: {lgb_model_140321.score}\"\n",
    "s10 = f\"140501_AUC: {lgb_model_140501.score}\"\n",
    "s11 = f\"140505_AUC: {lgb_model_140505.score}\"\n",
    "s12 = f\"140641_AUC: {lgb_model_140641.score}\"\n",
    "s13 = f\"140691_AUC: {lgb_model_140691.score}\"\n",
    "\n",
    "with open(path_w, mode='w') as f:\n",
    "    f.write(s1+\"\\n\")\n",
    "    f.write(s2+\"\\n\")\n",
    "    f.write(s3+\"\\n\")\n",
    "    f.write(s4+\"\\n\")\n",
    "    f.write(s5+\"\\n\")\n",
    "    f.write(s6+\"\\n\")\n",
    "    f.write(s7+\"\\n\")\n",
    "    f.write(s8+\"\\n\")\n",
    "    f.write(s9+\"\\n\")\n",
    "    f.write(s10+\"\\n\")\n",
    "    f.write(s11+\"\\n\")\n",
    "    f.write(s12+\"\\n\")\n",
    "    f.write(s13+\"\\n\")\n",
    "\n",
    "with open(path_w) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"130123\"] = lgb_model_130123.y_pred\n",
    "sample_submission[\"130125\"] = lgb_model_130125.y_pred\n",
    "sample_submission[\"130129\"] = lgb_model_130129.y_pred\n",
    "sample_submission[\"130131\"] = lgb_model_130131.y_pred\n",
    "sample_submission[\"140307\"] = lgb_model_140307.y_pred\n",
    "sample_submission[\"140313\"] = lgb_model_140313.y_pred\n",
    "sample_submission[\"140316\"] = lgb_model_140316.y_pred\n",
    "sample_submission[\"140317\"] = lgb_model_140317.y_pred\n",
    "sample_submission[\"140321\"] = lgb_model_140321.y_pred\n",
    "sample_submission[\"140501\"] = lgb_model_140501.y_pred\n",
    "sample_submission[\"140505\"] = lgb_model_140505.y_pred\n",
    "sample_submission[\"140641\"] = lgb_model_140641.y_pred\n",
    "sample_submission[\"140691\"] = lgb_model_140691.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>130123</th>\n",
       "      <th>130125</th>\n",
       "      <th>130129</th>\n",
       "      <th>130131</th>\n",
       "      <th>140307</th>\n",
       "      <th>140313</th>\n",
       "      <th>140316</th>\n",
       "      <th>140317</th>\n",
       "      <th>140321</th>\n",
       "      <th>140501</th>\n",
       "      <th>140505</th>\n",
       "      <th>140641</th>\n",
       "      <th>140691</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040232</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>0.067452</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.018505</td>\n",
       "      <td>0.030068</td>\n",
       "      <td>0.025426</td>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.076402</td>\n",
       "      <td>0.013645</td>\n",
       "      <td>0.045617</td>\n",
       "      <td>0.084973</td>\n",
       "      <td>0.040603</td>\n",
       "      <td>0.011788</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.254897</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.037649</td>\n",
       "      <td>0.003275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.241824</td>\n",
       "      <td>0.030543</td>\n",
       "      <td>0.132799</td>\n",
       "      <td>0.228855</td>\n",
       "      <td>0.078287</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.031096</td>\n",
       "      <td>0.022021</td>\n",
       "      <td>0.004850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.109612</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.294879</td>\n",
       "      <td>0.040440</td>\n",
       "      <td>0.044881</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.030360</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.006798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.148123</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>0.177052</td>\n",
       "      <td>0.119795</td>\n",
       "      <td>0.041511</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.061648</td>\n",
       "      <td>0.048735</td>\n",
       "      <td>0.036030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     130123    130125    130129    130131    140307    140313    140316  \\\n",
       "0  0.040232  0.006860  0.025696  0.067452  0.032766  0.009906  0.004553   \n",
       "1  0.076402  0.013645  0.045617  0.084973  0.040603  0.011788  0.010483   \n",
       "2  0.241824  0.030543  0.132799  0.228855  0.078287  0.054810  0.008997   \n",
       "3  0.109612  0.009601  0.064301  0.294879  0.040440  0.044881  0.010482   \n",
       "4  0.148123  0.009579  0.065051  0.177052  0.119795  0.041511  0.011092   \n",
       "\n",
       "     140317    140321    140501    140505    140641    140691  \n",
       "0  0.008393  0.009025  0.018505  0.030068  0.025426  0.007609  \n",
       "1  0.005316  0.005111  0.254897  0.153382  0.037649  0.003275  \n",
       "2  0.007156  0.008335  0.028201  0.031096  0.022021  0.004850  \n",
       "3  0.011213  0.015895  0.030360  0.030806  0.018678  0.006798  \n",
       "4  0.006895  0.010336  0.011060  0.061648  0.048735  0.036030  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96505, 13)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAME_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(f\"{SAME_PATH}\" + \"/Users/td017/kaggle-pipeline/submission/sub_\" + f\"{CASE}\" + \".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Xgb_Model(Base_Model):\n",
    "    \n",
    "#     def train_model(self, train_set, val_set):\n",
    "#         verbosity = 100 if self.verbose else 0\n",
    "#         return xgb.train(self.params, train_set, \n",
    "#                          num_boost_round=5000, evals=[(train_set, 'train'), (val_set, 'val')], \n",
    "#                          verbose_eval=verbosity, early_stopping_rounds=100)\n",
    "        \n",
    "#     def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "#         train_set = xgb.DMatrix(x_train, y_train)\n",
    "#         val_set = xgb.DMatrix(x_val, y_val)\n",
    "#         return train_set, val_set\n",
    "    \n",
    "#     def convert_x(self, x):\n",
    "#         return xgb.DMatrix(x)\n",
    "        \n",
    "#     def get_params(self):\n",
    "#         params = {'objective': 'binary:logistic',\n",
    "#          'learning_rate': 0.01,\n",
    "#         'eval_metric': 'logloss',\n",
    "#         #'num_class': 9,\n",
    "#         'max_depth': 12,\n",
    "#         'eta': 0.1,\n",
    "#         'min_child_weight': 32,\n",
    "#         'subsample': 0.9,\n",
    "#         'colsample_bytree': 0.8,\n",
    "#         'silent': 1,\n",
    "#         'random_state': 71,\n",
    "#         #'num_round': 10000,\n",
    "#         'early_stopping_rounds': 10}\n",
    "        \n",
    "#         #{'colsample_bytree': 0.8,                 \n",
    "#             #'learning_rate': 0.01,\n",
    "#             #'max_depth': 7,#10\n",
    "# #             'subsample': 1,\n",
    "# #             'objective':'reg:squarederror',\n",
    "# #             #'eval_metric':'rmse',\n",
    "# #             'min_child_weight':32,#16,\n",
    "# #             'gamma':0.25,\n",
    "# #             'n_estimators':5000}\n",
    "\n",
    "#         return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"################### 1 ###################\")\n",
    "# xgb_model_130123 = Xgb_Model(train_after, test_after, features, '130123')\n",
    "# print(\"################### 2 ###################\")\n",
    "# xgb_model_130125 = Xgb_Model(train_after, test_after, features, '130125')\n",
    "# print(\"################### 3 ###################\")\n",
    "# xgb_model_130129 = Xgb_Model(train_after, test_after, features, '130129')\n",
    "# print(\"################### 4 ###################\")\n",
    "# xgb_model_130131 = Xgb_Model(train_after, test_after, features, '130131')\n",
    "# print(\"################### 5 ###################\")\n",
    "# xgb_model_140307 = Xgb_Model(train_after, test_after, features, '140307')\n",
    "# print(\"################### 6 ###################\")\n",
    "# xgb_model_140313 = Xgb_Model(train_after, test_after, features, '140313')\n",
    "# print(\"################### 7 ###################\")\n",
    "# xgb_model_140316 = Xgb_Model(train_after, test_after, features, '140316')\n",
    "# print(\"################### 8 ###################\")\n",
    "# xgb_model_140317 = Xgb_Model(train_after, test_after, features, '140317')\n",
    "# print(\"################### 9 ###################\")\n",
    "# xgb_model_140321 = Xgb_Model(train_after, test_after, features, '140321')\n",
    "# print(\"################### 10 ###################\")\n",
    "# xgb_model_140501 = Xgb_Model(train_after, test_after, features, '140501')\n",
    "# print(\"################### 11 ###################\")\n",
    "# xgb_model_140505 = Xgb_Model(train_after, test_after, features, '140505')\n",
    "# print(\"################### 12 ###################\")\n",
    "# xgb_model_140641 = Xgb_Model(train_after, test_after, features, '140641')\n",
    "# print(\"################### 13 ###################\")\n",
    "# xgb_model_140691 = Xgb_Model(train_after, test_after, features, '140691')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"130123_AUC: {xgb_model_130123.score}\")\n",
    "# print(f\"130125_AUC: {xgb_model_130125.score}\")\n",
    "# print(f\"130129_AUC: {xgb_model_130129.score}\")\n",
    "# print(f\"130131_AUC: {xgb_model_130131.score}\")\n",
    "# print(f\"140307_AUC: {xgb_model_140307.score}\")\n",
    "# print(f\"140313_AUC: {xgb_model_140313.score}\")\n",
    "# print(f\"140316_AUC: {xgb_model_140316.score}\")\n",
    "# print(f\"140317_AUC: {xgb_model_140317.score}\")\n",
    "# print(f\"140321_AUC: {xgb_model_140321.score}\")\n",
    "# print(f\"140501_AUC: {xgb_model_140501.score}\")\n",
    "# print(f\"140505_AUC: {xgb_model_140505.score}\")\n",
    "# print(f\"140641_AUC: {xgb_model_140641.score}\")\n",
    "# print(f\"140691_AUC: {xgb_model_140691.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_w = f'/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir0/code/Users/td017/kaggle-pipeline/model/{CASE}_xgb_score.txt'\n",
    "\n",
    "# s1 = f\"130123_AUC: {xgb_model_130123.score}\"\n",
    "# s2 = f\"130125_AUC: {xgb_model_130125.score}\"\n",
    "# s3 = f\"130129_AUC: {xgb_model_130129.score}\"\n",
    "# s4 = f\"130131_AUC: {xgb_model_130131.score}\"\n",
    "# s5 = f\"140307_AUC: {xgb_model_140307.score}\"\n",
    "# s6 = f\"140313_AUC: {xgb_model_140313.score}\"\n",
    "# s7 = f\"140316_AUC: {xgb_model_140316.score}\"\n",
    "# s8 = f\"140317_AUC: {xgb_model_140317.score}\"\n",
    "# s9 = f\"140321_AUC: {xgb_model_140321.score}\"\n",
    "# s10 = f\"140501_AUC: {xgb_model_140501.score}\"\n",
    "# s11 = f\"140505_AUC: {xgb_model_140505.score}\"\n",
    "# s12 = f\"140641_AUC: {xgb_model_140641.score}\"\n",
    "# s13 = f\"140691_AUC: {xgb_model_140691.score}\"\n",
    "\n",
    "# with open(path_w, mode='w') as f:\n",
    "#     f.write(s1+\"\\n\")\n",
    "#     f.write(s2+\"\\n\")\n",
    "#     f.write(s3+\"\\n\")\n",
    "#     f.write(s4+\"\\n\")\n",
    "#     f.write(s5+\"\\n\")\n",
    "#     f.write(s6+\"\\n\")\n",
    "#     f.write(s7+\"\\n\")\n",
    "#     f.write(s8+\"\\n\")\n",
    "#     f.write(s9+\"\\n\")\n",
    "#     f.write(s10+\"\\n\")\n",
    "#     f.write(s11+\"\\n\")\n",
    "#     f.write(s12+\"\\n\")\n",
    "#     f.write(s13+\"\\n\")\n",
    "\n",
    "# with open(path_w) as f:\n",
    "#     print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stackingのような何か"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_stack = pd.read_pickle(f\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/tmp/{CASE}_oof_pred.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>130123</th>\n",
       "      <th>130125</th>\n",
       "      <th>130129</th>\n",
       "      <th>130131</th>\n",
       "      <th>140307</th>\n",
       "      <th>140313</th>\n",
       "      <th>140316</th>\n",
       "      <th>140317</th>\n",
       "      <th>140321</th>\n",
       "      <th>140501</th>\n",
       "      <th>140505</th>\n",
       "      <th>140641</th>\n",
       "      <th>140691</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040674</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.062851</td>\n",
       "      <td>0.042257</td>\n",
       "      <td>0.025029</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.028405</td>\n",
       "      <td>0.017969</td>\n",
       "      <td>0.007592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085360</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>0.170129</td>\n",
       "      <td>0.029627</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>0.015314</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.023297</td>\n",
       "      <td>0.022189</td>\n",
       "      <td>0.005705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126139</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.127633</td>\n",
       "      <td>0.063036</td>\n",
       "      <td>0.012105</td>\n",
       "      <td>0.027708</td>\n",
       "      <td>0.006339</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.022045</td>\n",
       "      <td>0.049357</td>\n",
       "      <td>0.001946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121464</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.275874</td>\n",
       "      <td>0.093228</td>\n",
       "      <td>0.138492</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.097828</td>\n",
       "      <td>0.006208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.157340</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.071833</td>\n",
       "      <td>0.200286</td>\n",
       "      <td>0.175853</td>\n",
       "      <td>0.016206</td>\n",
       "      <td>0.009456</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.015241</td>\n",
       "      <td>0.034619</td>\n",
       "      <td>0.079691</td>\n",
       "      <td>0.021228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.057167</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.077358</td>\n",
       "      <td>0.195276</td>\n",
       "      <td>0.024284</td>\n",
       "      <td>0.017331</td>\n",
       "      <td>0.008168</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.216117</td>\n",
       "      <td>0.078690</td>\n",
       "      <td>0.043991</td>\n",
       "      <td>0.005783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.122690</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.063108</td>\n",
       "      <td>0.262616</td>\n",
       "      <td>0.036614</td>\n",
       "      <td>0.061340</td>\n",
       "      <td>0.008512</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.195561</td>\n",
       "      <td>0.034565</td>\n",
       "      <td>0.016314</td>\n",
       "      <td>0.005006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.109098</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>0.087265</td>\n",
       "      <td>0.161923</td>\n",
       "      <td>0.336647</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.009443</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.006566</td>\n",
       "      <td>0.024890</td>\n",
       "      <td>0.052241</td>\n",
       "      <td>0.011596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.046405</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>0.044197</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.063945</td>\n",
       "      <td>0.061068</td>\n",
       "      <td>0.070388</td>\n",
       "      <td>0.024996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.365136</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>0.081919</td>\n",
       "      <td>0.094931</td>\n",
       "      <td>0.019410</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.008816</td>\n",
       "      <td>0.030763</td>\n",
       "      <td>0.070182</td>\n",
       "      <td>0.005038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.079568</td>\n",
       "      <td>0.010245</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>0.180175</td>\n",
       "      <td>0.081485</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.028116</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>0.011217</td>\n",
       "      <td>0.071563</td>\n",
       "      <td>0.004105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>0.154132</td>\n",
       "      <td>0.239312</td>\n",
       "      <td>0.140326</td>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.041497</td>\n",
       "      <td>0.047794</td>\n",
       "      <td>0.003467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.061159</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>0.090856</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>0.035859</td>\n",
       "      <td>0.010134</td>\n",
       "      <td>0.008642</td>\n",
       "      <td>0.014663</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.017288</td>\n",
       "      <td>0.027841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.044905</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.081006</td>\n",
       "      <td>0.089117</td>\n",
       "      <td>0.035541</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>0.489203</td>\n",
       "      <td>0.075807</td>\n",
       "      <td>0.133046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.041956</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>0.152122</td>\n",
       "      <td>0.161042</td>\n",
       "      <td>0.030062</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.809229</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.005571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.026372</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.029664</td>\n",
       "      <td>0.037896</td>\n",
       "      <td>0.016809</td>\n",
       "      <td>0.014251</td>\n",
       "      <td>0.047587</td>\n",
       "      <td>0.010461</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.040108</td>\n",
       "      <td>0.188952</td>\n",
       "      <td>0.019304</td>\n",
       "      <td>0.020615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.094259</td>\n",
       "      <td>0.094129</td>\n",
       "      <td>0.046128</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.150735</td>\n",
       "      <td>0.085195</td>\n",
       "      <td>0.029019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.096181</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.085306</td>\n",
       "      <td>0.060159</td>\n",
       "      <td>0.040952</td>\n",
       "      <td>0.049391</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.127838</td>\n",
       "      <td>0.061707</td>\n",
       "      <td>0.133956</td>\n",
       "      <td>0.002775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.054043</td>\n",
       "      <td>0.020492</td>\n",
       "      <td>0.032625</td>\n",
       "      <td>0.052402</td>\n",
       "      <td>0.023557</td>\n",
       "      <td>0.263156</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.397490</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.001227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.039348</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.062444</td>\n",
       "      <td>0.041439</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.795809</td>\n",
       "      <td>0.037015</td>\n",
       "      <td>0.093604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.197946</td>\n",
       "      <td>0.019126</td>\n",
       "      <td>0.070284</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.096562</td>\n",
       "      <td>0.076668</td>\n",
       "      <td>0.025669</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.062480</td>\n",
       "      <td>0.112766</td>\n",
       "      <td>0.082681</td>\n",
       "      <td>0.065014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.063994</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.038653</td>\n",
       "      <td>0.061898</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.055832</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.003134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.041434</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.038662</td>\n",
       "      <td>0.054431</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>0.003739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.041471</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.017254</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.041028</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.043102</td>\n",
       "      <td>0.025420</td>\n",
       "      <td>0.019314</td>\n",
       "      <td>0.012347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.113406</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.044823</td>\n",
       "      <td>0.111065</td>\n",
       "      <td>0.070741</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.052875</td>\n",
       "      <td>0.061712</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>0.005159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.054736</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.015811</td>\n",
       "      <td>0.046607</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.045782</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>0.017139</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.055232</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.072642</td>\n",
       "      <td>0.228658</td>\n",
       "      <td>0.050238</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.004326</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.106131</td>\n",
       "      <td>0.068459</td>\n",
       "      <td>0.039691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.056206</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.020788</td>\n",
       "      <td>0.036447</td>\n",
       "      <td>0.078111</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.317557</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>0.060651</td>\n",
       "      <td>0.076743</td>\n",
       "      <td>0.003289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.260726</td>\n",
       "      <td>0.009056</td>\n",
       "      <td>0.122793</td>\n",
       "      <td>0.313873</td>\n",
       "      <td>0.552543</td>\n",
       "      <td>0.019892</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.025298</td>\n",
       "      <td>0.080024</td>\n",
       "      <td>0.021360</td>\n",
       "      <td>0.001155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.102571</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.047837</td>\n",
       "      <td>0.114320</td>\n",
       "      <td>0.012909</td>\n",
       "      <td>0.018308</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>0.005795</td>\n",
       "      <td>0.070586</td>\n",
       "      <td>0.116605</td>\n",
       "      <td>0.110112</td>\n",
       "      <td>0.041476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.023222</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.119785</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.018019</td>\n",
       "      <td>0.364247</td>\n",
       "      <td>0.189599</td>\n",
       "      <td>0.003772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.026429</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.059208</td>\n",
       "      <td>0.043634</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.009759</td>\n",
       "      <td>0.016282</td>\n",
       "      <td>0.056762</td>\n",
       "      <td>0.010654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.027181</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.030387</td>\n",
       "      <td>0.046613</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.012976</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.077024</td>\n",
       "      <td>0.005233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.072247</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.024490</td>\n",
       "      <td>0.118829</td>\n",
       "      <td>0.036453</td>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.046279</td>\n",
       "      <td>0.024379</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.049517</td>\n",
       "      <td>0.020649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.055254</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.066181</td>\n",
       "      <td>0.137164</td>\n",
       "      <td>0.044227</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.044801</td>\n",
       "      <td>0.040590</td>\n",
       "      <td>0.123403</td>\n",
       "      <td>0.006302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.031331</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.015508</td>\n",
       "      <td>0.067096</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.009784</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>0.026653</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>0.013452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.190444</td>\n",
       "      <td>0.007190</td>\n",
       "      <td>0.088450</td>\n",
       "      <td>0.209372</td>\n",
       "      <td>0.030015</td>\n",
       "      <td>0.013437</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>0.075707</td>\n",
       "      <td>0.040142</td>\n",
       "      <td>0.006768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.040774</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.026747</td>\n",
       "      <td>0.043536</td>\n",
       "      <td>0.053947</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>0.246843</td>\n",
       "      <td>0.047128</td>\n",
       "      <td>0.004159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.111156</td>\n",
       "      <td>0.006908</td>\n",
       "      <td>0.183072</td>\n",
       "      <td>0.363087</td>\n",
       "      <td>0.051420</td>\n",
       "      <td>0.022599</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.009444</td>\n",
       "      <td>0.037673</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.091918</td>\n",
       "      <td>0.013110</td>\n",
       "      <td>0.031401</td>\n",
       "      <td>0.095199</td>\n",
       "      <td>0.163837</td>\n",
       "      <td>0.336636</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.017308</td>\n",
       "      <td>0.025577</td>\n",
       "      <td>0.028841</td>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.015166</td>\n",
       "      <td>0.045015</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.015153</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.566003</td>\n",
       "      <td>0.222384</td>\n",
       "      <td>0.198386</td>\n",
       "      <td>0.004211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.187936</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.086250</td>\n",
       "      <td>0.123461</td>\n",
       "      <td>0.086219</td>\n",
       "      <td>0.056654</td>\n",
       "      <td>0.013818</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.089852</td>\n",
       "      <td>0.495824</td>\n",
       "      <td>0.785635</td>\n",
       "      <td>0.019734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.068163</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.088175</td>\n",
       "      <td>0.022753</td>\n",
       "      <td>0.023855</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.011653</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.197490</td>\n",
       "      <td>0.041567</td>\n",
       "      <td>0.004652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.032174</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.135915</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>0.022837</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.002790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.029859</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.016356</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.089975</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.042298</td>\n",
       "      <td>0.109962</td>\n",
       "      <td>0.025848</td>\n",
       "      <td>0.033754</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>0.036523</td>\n",
       "      <td>0.040902</td>\n",
       "      <td>0.013881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.024052</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.036286</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>0.016090</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.068015</td>\n",
       "      <td>0.005844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.322679</td>\n",
       "      <td>0.042881</td>\n",
       "      <td>0.280757</td>\n",
       "      <td>0.239979</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>0.086332</td>\n",
       "      <td>0.035925</td>\n",
       "      <td>0.034911</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.037881</td>\n",
       "      <td>0.055862</td>\n",
       "      <td>0.019190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.065344</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.027197</td>\n",
       "      <td>0.024965</td>\n",
       "      <td>0.192749</td>\n",
       "      <td>0.053048</td>\n",
       "      <td>0.031554</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.019688</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.005389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.107778</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.101960</td>\n",
       "      <td>0.075626</td>\n",
       "      <td>0.029267</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>0.050490</td>\n",
       "      <td>0.006883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128326</th>\n",
       "      <td>0.267781</td>\n",
       "      <td>0.017042</td>\n",
       "      <td>0.072724</td>\n",
       "      <td>0.076038</td>\n",
       "      <td>0.024175</td>\n",
       "      <td>0.042266</td>\n",
       "      <td>0.023507</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.045632</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.003951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128327</th>\n",
       "      <td>0.075486</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.018180</td>\n",
       "      <td>0.073437</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>0.013194</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.014193</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.006201</td>\n",
       "      <td>0.136952</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.004264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128328</th>\n",
       "      <td>0.028529</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.063680</td>\n",
       "      <td>0.040284</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128329</th>\n",
       "      <td>0.153220</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>0.068846</td>\n",
       "      <td>0.052140</td>\n",
       "      <td>0.015265</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.000554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128330</th>\n",
       "      <td>0.107490</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.019094</td>\n",
       "      <td>0.084472</td>\n",
       "      <td>0.072056</td>\n",
       "      <td>0.032801</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.040455</td>\n",
       "      <td>0.042849</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.003148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128331</th>\n",
       "      <td>0.078056</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>0.088439</td>\n",
       "      <td>0.041684</td>\n",
       "      <td>0.031097</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.151646</td>\n",
       "      <td>0.006971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128332</th>\n",
       "      <td>0.193809</td>\n",
       "      <td>0.027033</td>\n",
       "      <td>0.088583</td>\n",
       "      <td>0.269442</td>\n",
       "      <td>0.041934</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.007588</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.012095</td>\n",
       "      <td>0.125095</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128333</th>\n",
       "      <td>0.835790</td>\n",
       "      <td>0.016157</td>\n",
       "      <td>0.240370</td>\n",
       "      <td>0.498923</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>0.047217</td>\n",
       "      <td>0.015681</td>\n",
       "      <td>0.078516</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>0.019846</td>\n",
       "      <td>0.018238</td>\n",
       "      <td>0.212806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128334</th>\n",
       "      <td>0.074030</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.031598</td>\n",
       "      <td>0.107120</td>\n",
       "      <td>0.011913</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.017181</td>\n",
       "      <td>0.013798</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128335</th>\n",
       "      <td>0.107261</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>0.123970</td>\n",
       "      <td>0.027995</td>\n",
       "      <td>0.047113</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>0.001470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128336</th>\n",
       "      <td>0.192988</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.024538</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.004286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128337</th>\n",
       "      <td>0.621995</td>\n",
       "      <td>0.046994</td>\n",
       "      <td>0.090381</td>\n",
       "      <td>0.226377</td>\n",
       "      <td>0.232607</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.017930</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.047028</td>\n",
       "      <td>0.068479</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128338</th>\n",
       "      <td>0.054893</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.041183</td>\n",
       "      <td>0.070429</td>\n",
       "      <td>0.781999</td>\n",
       "      <td>0.059884</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.005687</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.096438</td>\n",
       "      <td>0.005313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128339</th>\n",
       "      <td>0.115606</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.035498</td>\n",
       "      <td>0.092977</td>\n",
       "      <td>0.066719</td>\n",
       "      <td>0.031171</td>\n",
       "      <td>0.007437</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.270150</td>\n",
       "      <td>0.026005</td>\n",
       "      <td>0.007594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128340</th>\n",
       "      <td>0.092148</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.119503</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.008356</td>\n",
       "      <td>0.005703</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.002057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128341</th>\n",
       "      <td>0.222970</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.026831</td>\n",
       "      <td>0.522513</td>\n",
       "      <td>0.090257</td>\n",
       "      <td>0.014196</td>\n",
       "      <td>0.027426</td>\n",
       "      <td>0.016989</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.037794</td>\n",
       "      <td>0.039281</td>\n",
       "      <td>0.008842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128342</th>\n",
       "      <td>0.127603</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.087260</td>\n",
       "      <td>0.054336</td>\n",
       "      <td>0.053410</td>\n",
       "      <td>0.038022</td>\n",
       "      <td>0.026066</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.014193</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.005856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128343</th>\n",
       "      <td>0.062447</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>0.054428</td>\n",
       "      <td>0.022047</td>\n",
       "      <td>0.144439</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.027417</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128344</th>\n",
       "      <td>0.091427</td>\n",
       "      <td>0.010324</td>\n",
       "      <td>0.040573</td>\n",
       "      <td>0.069516</td>\n",
       "      <td>0.055728</td>\n",
       "      <td>0.019834</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.024625</td>\n",
       "      <td>0.025670</td>\n",
       "      <td>0.014949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128345</th>\n",
       "      <td>0.129026</td>\n",
       "      <td>0.015356</td>\n",
       "      <td>0.032906</td>\n",
       "      <td>0.097279</td>\n",
       "      <td>0.156396</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>0.029611</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>0.038897</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.005747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128346</th>\n",
       "      <td>0.172413</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>0.054520</td>\n",
       "      <td>0.258535</td>\n",
       "      <td>0.020083</td>\n",
       "      <td>0.037071</td>\n",
       "      <td>0.029325</td>\n",
       "      <td>0.017998</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.026685</td>\n",
       "      <td>0.053445</td>\n",
       "      <td>0.005168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128347</th>\n",
       "      <td>0.087219</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.083837</td>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.024680</td>\n",
       "      <td>0.021607</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.011406</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128348</th>\n",
       "      <td>0.095306</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>0.040032</td>\n",
       "      <td>0.094283</td>\n",
       "      <td>0.022671</td>\n",
       "      <td>0.017450</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.024263</td>\n",
       "      <td>0.066381</td>\n",
       "      <td>0.016690</td>\n",
       "      <td>0.007380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128349</th>\n",
       "      <td>0.040847</td>\n",
       "      <td>0.009691</td>\n",
       "      <td>0.022291</td>\n",
       "      <td>0.029264</td>\n",
       "      <td>0.047977</td>\n",
       "      <td>0.012241</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.023977</td>\n",
       "      <td>0.025026</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.003102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128350</th>\n",
       "      <td>0.057297</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>0.093683</td>\n",
       "      <td>0.061593</td>\n",
       "      <td>0.012086</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.001805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128351</th>\n",
       "      <td>0.106541</td>\n",
       "      <td>0.014771</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.187949</td>\n",
       "      <td>0.034316</td>\n",
       "      <td>0.026337</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.041527</td>\n",
       "      <td>0.141172</td>\n",
       "      <td>0.016927</td>\n",
       "      <td>0.044290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128352</th>\n",
       "      <td>0.079477</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.046532</td>\n",
       "      <td>0.070907</td>\n",
       "      <td>0.025855</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>0.031119</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.011941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128353</th>\n",
       "      <td>0.139489</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.178122</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.055050</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.009926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128354</th>\n",
       "      <td>0.119519</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>0.059871</td>\n",
       "      <td>0.206195</td>\n",
       "      <td>0.043901</td>\n",
       "      <td>0.070081</td>\n",
       "      <td>0.172358</td>\n",
       "      <td>0.073191</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.265854</td>\n",
       "      <td>0.168033</td>\n",
       "      <td>0.027313</td>\n",
       "      <td>0.010541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128355</th>\n",
       "      <td>0.061592</td>\n",
       "      <td>0.007806</td>\n",
       "      <td>0.011195</td>\n",
       "      <td>0.066337</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.016806</td>\n",
       "      <td>0.009131</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.034677</td>\n",
       "      <td>0.019522</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>0.032989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128356</th>\n",
       "      <td>0.170436</td>\n",
       "      <td>0.502818</td>\n",
       "      <td>0.081219</td>\n",
       "      <td>0.085122</td>\n",
       "      <td>0.039271</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.092996</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.027091</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128357</th>\n",
       "      <td>0.063930</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>0.051476</td>\n",
       "      <td>0.011925</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.022240</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.016679</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.004273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128358</th>\n",
       "      <td>0.399266</td>\n",
       "      <td>0.021684</td>\n",
       "      <td>0.106649</td>\n",
       "      <td>0.527720</td>\n",
       "      <td>0.021666</td>\n",
       "      <td>0.085019</td>\n",
       "      <td>0.203758</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>0.097779</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>0.008880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128359</th>\n",
       "      <td>0.064149</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>0.017747</td>\n",
       "      <td>0.046414</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.132508</td>\n",
       "      <td>0.005498</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.005977</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.006052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128360</th>\n",
       "      <td>0.211671</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.398195</td>\n",
       "      <td>0.120440</td>\n",
       "      <td>0.292873</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.011412</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.006855</td>\n",
       "      <td>0.047359</td>\n",
       "      <td>0.207585</td>\n",
       "      <td>0.004438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128361</th>\n",
       "      <td>0.080531</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>0.008143</td>\n",
       "      <td>0.115370</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.018526</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>0.003425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128362</th>\n",
       "      <td>0.104335</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.063325</td>\n",
       "      <td>0.077384</td>\n",
       "      <td>0.067053</td>\n",
       "      <td>0.036992</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>0.055862</td>\n",
       "      <td>0.014646</td>\n",
       "      <td>0.005711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128363</th>\n",
       "      <td>0.052033</td>\n",
       "      <td>0.009047</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.010308</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.074087</td>\n",
       "      <td>0.058170</td>\n",
       "      <td>0.039460</td>\n",
       "      <td>0.003872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128364</th>\n",
       "      <td>0.051925</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.027733</td>\n",
       "      <td>0.150894</td>\n",
       "      <td>0.018294</td>\n",
       "      <td>0.021442</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.236244</td>\n",
       "      <td>0.016795</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128365</th>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.309209</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.001869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128366</th>\n",
       "      <td>0.127554</td>\n",
       "      <td>0.022465</td>\n",
       "      <td>0.035363</td>\n",
       "      <td>0.165323</td>\n",
       "      <td>0.042124</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>0.043668</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128367</th>\n",
       "      <td>0.269526</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.071512</td>\n",
       "      <td>0.267386</td>\n",
       "      <td>0.017118</td>\n",
       "      <td>0.053507</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.001891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128368</th>\n",
       "      <td>0.182727</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.075764</td>\n",
       "      <td>0.283841</td>\n",
       "      <td>0.103625</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.022997</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128369</th>\n",
       "      <td>0.063185</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.034815</td>\n",
       "      <td>0.078383</td>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.023001</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.001414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128370</th>\n",
       "      <td>0.073455</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.156536</td>\n",
       "      <td>0.094165</td>\n",
       "      <td>0.114267</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.006210</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>0.043201</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.005507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128371</th>\n",
       "      <td>0.099073</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.113751</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>0.017179</td>\n",
       "      <td>0.014450</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.007296</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128372</th>\n",
       "      <td>0.036741</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>0.040602</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>0.010083</td>\n",
       "      <td>0.012278</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.036399</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>0.009212</td>\n",
       "      <td>0.003041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128373</th>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>0.069937</td>\n",
       "      <td>0.054073</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.087104</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128374</th>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.023168</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.007751</td>\n",
       "      <td>0.033291</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>0.005410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128375</th>\n",
       "      <td>0.036759</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.012942</td>\n",
       "      <td>0.067150</td>\n",
       "      <td>0.015591</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.001803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128376 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          130123    130125    130129    130131    140307    140313    140316  \\\n",
       "0       0.040674  0.005444  0.062851  0.042257  0.025029  0.008335  0.001427   \n",
       "1       0.085360  0.008760  0.065909  0.170129  0.029627  0.016038  0.015314   \n",
       "2       0.126139  0.003443  0.127633  0.063036  0.012105  0.027708  0.006339   \n",
       "3       0.121464  0.009970  0.275874  0.093228  0.138492  0.014284  0.002960   \n",
       "4       0.157340  0.009805  0.071833  0.200286  0.175853  0.016206  0.009456   \n",
       "5       0.057167  0.014837  0.077358  0.195276  0.024284  0.017331  0.008168   \n",
       "6       0.122690  0.004273  0.063108  0.262616  0.036614  0.061340  0.008512   \n",
       "7       0.109098  0.005392  0.087265  0.161923  0.336647  0.009870  0.009443   \n",
       "8       0.046405  0.005422  0.041465  0.044197  0.042708  0.027321  0.002224   \n",
       "9       0.365136  0.005485  0.081919  0.094931  0.019410  0.006162  0.013284   \n",
       "10      0.079568  0.010245  0.059002  0.180175  0.081485  0.008925  0.028116   \n",
       "11      0.308208  0.017372  0.154132  0.239312  0.140326  0.024198  0.008849   \n",
       "12      0.061159  0.003380  0.064141  0.090856  0.109145  0.013229  0.035859   \n",
       "13      0.044905  0.003285  0.081006  0.089117  0.035541  0.011062  0.004148   \n",
       "14      0.041956  0.010140  0.152122  0.161042  0.030062  0.028765  0.014119   \n",
       "15      0.026372  0.003431  0.029664  0.037896  0.016809  0.014251  0.047587   \n",
       "16      0.069002  0.004157  0.094259  0.094129  0.046128  0.014256  0.009693   \n",
       "17      0.096181  0.003675  0.085306  0.060159  0.040952  0.049391  0.002218   \n",
       "18      0.054043  0.020492  0.032625  0.052402  0.023557  0.263156  0.010464   \n",
       "19      0.039348  0.003307  0.062444  0.041439  0.004779  0.002003  0.002108   \n",
       "20      0.197946  0.019126  0.070284  0.128800  0.096562  0.076668  0.025669   \n",
       "21      0.063994  0.008610  0.038653  0.061898  0.013788  0.007122  0.005710   \n",
       "22      0.041434  0.003578  0.009218  0.077206  0.024898  0.012581  0.002153   \n",
       "23      0.041471  0.004064  0.017254  0.021281  0.041028  0.017986  0.002931   \n",
       "24      0.113406  0.003836  0.044823  0.111065  0.070741  0.006740  0.003625   \n",
       "25      0.054736  0.003790  0.015811  0.046607  0.015090  0.008916  0.004818   \n",
       "26      0.055232  0.002823  0.072642  0.228658  0.050238  0.019539  0.006968   \n",
       "27      0.056206  0.003565  0.020788  0.036447  0.078111  0.023618  0.012167   \n",
       "28      0.260726  0.009056  0.122793  0.313873  0.552543  0.019892  0.003417   \n",
       "29      0.102571  0.004898  0.047837  0.114320  0.012909  0.018308  0.008207   \n",
       "30      0.023222  0.004544  0.039000  0.048020  0.119785  0.002223  0.001755   \n",
       "31      0.026429  0.004613  0.005597  0.045539  0.059208  0.043634  0.016709   \n",
       "32      0.027181  0.003599  0.030387  0.046613  0.016479  0.012976  0.003494   \n",
       "33      0.072247  0.006286  0.024490  0.118829  0.036453  0.014373  0.006193   \n",
       "34      0.055254  0.006300  0.066181  0.137164  0.044227  0.042687  0.002354   \n",
       "35      0.031331  0.003603  0.015508  0.067096  0.005386  0.009784  0.002830   \n",
       "36      0.190444  0.007190  0.088450  0.209372  0.030015  0.013437  0.004741   \n",
       "37      0.040774  0.004676  0.026747  0.043536  0.053947  0.016765  0.002736   \n",
       "38      0.111156  0.006908  0.183072  0.363087  0.051420  0.022599  0.010555   \n",
       "39      0.091918  0.013110  0.031401  0.095199  0.163837  0.336636  0.032787   \n",
       "40      0.049100  0.005001  0.015166  0.045015  0.007653  0.015153  0.005952   \n",
       "41      0.187936  0.007760  0.086250  0.123461  0.086219  0.056654  0.013818   \n",
       "42      0.068163  0.004371  0.023785  0.088175  0.022753  0.023855  0.012961   \n",
       "43      0.032174  0.004642  0.135915  0.019587  0.004183  0.022837  0.013867   \n",
       "44      0.029859  0.002172  0.014954  0.006684  0.005441  0.005224  0.001685   \n",
       "45      0.089975  0.002588  0.042298  0.109962  0.025848  0.033754  0.013493   \n",
       "46      0.024052  0.003332  0.021901  0.036286  0.013556  0.016090  0.007078   \n",
       "47      0.322679  0.042881  0.280757  0.239979  0.075940  0.086332  0.035925   \n",
       "48      0.065344  0.003210  0.032969  0.027197  0.024965  0.192749  0.053048   \n",
       "49      0.107778  0.004555  0.101960  0.075626  0.029267  0.015061  0.002178   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "128326  0.267781  0.017042  0.072724  0.076038  0.024175  0.042266  0.023507   \n",
       "128327  0.075486  0.005508  0.018180  0.073437  0.019931  0.013194  0.004751   \n",
       "128328  0.028529  0.004189  0.015064  0.093197  0.004934  0.010700  0.002127   \n",
       "128329  0.153220  0.014013  0.038361  0.068846  0.052140  0.015265  0.013450   \n",
       "128330  0.107490  0.004546  0.019094  0.084472  0.072056  0.032801  0.001379   \n",
       "128331  0.078056  0.017978  0.025086  0.088439  0.041684  0.031097  0.011817   \n",
       "128332  0.193809  0.027033  0.088583  0.269442  0.041934  0.010330  0.009410   \n",
       "128333  0.835790  0.016157  0.240370  0.498923  0.049205  0.047217  0.015681   \n",
       "128334  0.074030  0.005758  0.031598  0.107120  0.011913  0.011441  0.007435   \n",
       "128335  0.107261  0.005191  0.016013  0.123970  0.027995  0.047113  0.008165   \n",
       "128336  0.192988  0.007796  0.089624  0.162400  0.024538  0.008571  0.007199   \n",
       "128337  0.621995  0.046994  0.090381  0.226377  0.232607  0.016923  0.017930   \n",
       "128338  0.054893  0.005205  0.041183  0.070429  0.781999  0.059884  0.012834   \n",
       "128339  0.115606  0.025886  0.035498  0.092977  0.066719  0.031171  0.007437   \n",
       "128340  0.092148  0.008666  0.114426  0.119503  0.013326  0.008356  0.005703   \n",
       "128341  0.222970  0.005532  0.026831  0.522513  0.090257  0.014196  0.027426   \n",
       "128342  0.127603  0.009098  0.087260  0.054336  0.053410  0.038022  0.026066   \n",
       "128343  0.062447  0.003191  0.015536  0.054428  0.022047  0.144439  0.048020   \n",
       "128344  0.091427  0.010324  0.040573  0.069516  0.055728  0.019834  0.015418   \n",
       "128345  0.129026  0.015356  0.032906  0.097279  0.156396  0.010615  0.007425   \n",
       "128346  0.172413  0.010792  0.054520  0.258535  0.020083  0.037071  0.029325   \n",
       "128347  0.087219  0.003763  0.083837  0.073022  0.024680  0.021607  0.028533   \n",
       "128348  0.095306  0.004823  0.040032  0.094283  0.022671  0.017450  0.003911   \n",
       "128349  0.040847  0.009691  0.022291  0.029264  0.047977  0.012241  0.014049   \n",
       "128350  0.057297  0.006852  0.093683  0.061593  0.012086  0.007344  0.003918   \n",
       "128351  0.106541  0.014771  0.059100  0.187949  0.034316  0.026337  0.006781   \n",
       "128352  0.079477  0.007634  0.046532  0.070907  0.025855  0.008287  0.001323   \n",
       "128353  0.139489  0.004087  0.022089  0.178122  0.060754  0.034341  0.001630   \n",
       "128354  0.119519  0.009463  0.059871  0.206195  0.043901  0.070081  0.172358   \n",
       "128355  0.061592  0.007806  0.011195  0.066337  0.012270  0.016806  0.009131   \n",
       "128356  0.170436  0.502818  0.081219  0.085122  0.039271  0.012552  0.002950   \n",
       "128357  0.063930  0.007829  0.027893  0.051476  0.011925  0.011157  0.008399   \n",
       "128358  0.399266  0.021684  0.106649  0.527720  0.021666  0.085019  0.203758   \n",
       "128359  0.064149  0.018536  0.017747  0.046414  0.007247  0.012906  0.132508   \n",
       "128360  0.211671  0.011123  0.398195  0.120440  0.292873  0.027500  0.011412   \n",
       "128361  0.080531  0.007425  0.008143  0.115370  0.032707  0.018526  0.004843   \n",
       "128362  0.104335  0.009343  0.063325  0.077384  0.067053  0.036992  0.003465   \n",
       "128363  0.052033  0.009047  0.037198  0.055075  0.006127  0.010308  0.006062   \n",
       "128364  0.051925  0.003843  0.027733  0.150894  0.018294  0.021442  0.004371   \n",
       "128365  0.043076  0.006920  0.004660  0.028369  0.014906  0.006632  0.003697   \n",
       "128366  0.127554  0.022465  0.035363  0.165323  0.042124  0.016609  0.011228   \n",
       "128367  0.269526  0.003393  0.071512  0.267386  0.017118  0.053507  0.009688   \n",
       "128368  0.182727  0.021578  0.075764  0.283841  0.103625  0.013850  0.006530   \n",
       "128369  0.063185  0.005254  0.034815  0.078383  0.020063  0.023001  0.004359   \n",
       "128370  0.073455  0.003666  0.156536  0.094165  0.114267  0.016447  0.008172   \n",
       "128371  0.099073  0.017699  0.026763  0.113751  0.049408  0.017179  0.014450   \n",
       "128372  0.036741  0.005896  0.011296  0.040602  0.019387  0.010083  0.012278   \n",
       "128373  0.061404  0.005674  0.013801  0.069937  0.054073  0.013075  0.087104   \n",
       "128374  0.032424  0.008682  0.005722  0.023168  0.012079  0.005588  0.001359   \n",
       "128375  0.036759  0.001487  0.012942  0.067150  0.015591  0.010608  0.004957   \n",
       "\n",
       "          140317    140321    140501    140505    140641    140691  \n",
       "0       0.003847  0.007631  0.003609  0.028405  0.017969  0.007592  \n",
       "1       0.002974  0.000512  0.005411  0.023297  0.022189  0.005705  \n",
       "2       0.025767  0.000768  0.012390  0.022045  0.049357  0.001946  \n",
       "3       0.003669  0.003650  0.031252  0.027514  0.097828  0.006208  \n",
       "4       0.003538  0.003391  0.015241  0.034619  0.079691  0.021228  \n",
       "5       0.008273  0.001174  0.216117  0.078690  0.043991  0.005783  \n",
       "6       0.003397  0.002626  0.195561  0.034565  0.016314  0.005006  \n",
       "7       0.002951  0.003167  0.006566  0.024890  0.052241  0.011596  \n",
       "8       0.011864  0.002540  0.063945  0.061068  0.070388  0.024996  \n",
       "9       0.005044  0.005173  0.008816  0.030763  0.070182  0.005038  \n",
       "10      0.002691  0.002024  0.005015  0.011217  0.071563  0.004105  \n",
       "11      0.003390  0.001274  0.004285  0.041497  0.047794  0.003467  \n",
       "12      0.010134  0.008642  0.014663  0.044722  0.017288  0.027841  \n",
       "13      0.002551  0.003709  0.064506  0.489203  0.075807  0.133046  \n",
       "14      0.002848  0.004580  0.003921  0.809229  0.013230  0.005571  \n",
       "15      0.010461  0.001626  0.040108  0.188952  0.019304  0.020615  \n",
       "16      0.003634  0.009968  0.021593  0.150735  0.085195  0.029019  \n",
       "17      0.002937  0.001397  0.127838  0.061707  0.133956  0.002775  \n",
       "18      0.008865  0.000996  0.006194  0.397490  0.010218  0.001227  \n",
       "19      0.004543  0.003037  0.011164  0.795809  0.037015  0.093604  \n",
       "20      0.005205  0.012895  0.062480  0.112766  0.082681  0.065014  \n",
       "21      0.006428  0.000873  0.004005  0.055832  0.024607  0.003134  \n",
       "22      0.004210  0.002053  0.038662  0.054431  0.028530  0.003739  \n",
       "23      0.002714  0.002590  0.043102  0.025420  0.019314  0.012347  \n",
       "24      0.003393  0.000943  0.052875  0.061712  0.014541  0.005159  \n",
       "25      0.007261  0.001171  0.045782  0.008248  0.017139  0.001226  \n",
       "26      0.006368  0.004326  0.004126  0.106131  0.068459  0.039691  \n",
       "27      0.317557  0.012159  0.009022  0.060651  0.076743  0.003289  \n",
       "28      0.002560  0.000605  0.025298  0.080024  0.021360  0.001155  \n",
       "29      0.006006  0.005795  0.070586  0.116605  0.110112  0.041476  \n",
       "30      0.000796  0.001863  0.018019  0.364247  0.189599  0.003772  \n",
       "31      0.004859  0.002401  0.009759  0.016282  0.056762  0.010654  \n",
       "32      0.004344  0.004949  0.005861  0.011556  0.077024  0.005233  \n",
       "33      0.005033  0.046279  0.024379  0.115936  0.049517  0.020649  \n",
       "34      0.004038  0.001126  0.044801  0.040590  0.123403  0.006302  \n",
       "35      0.026653  0.005664  0.006136  0.004114  0.018717  0.013452  \n",
       "36      0.008570  0.000394  0.012382  0.075707  0.040142  0.006768  \n",
       "37      0.003583  0.000339  0.009949  0.246843  0.047128  0.004159  \n",
       "38      0.007944  0.003587  0.009444  0.037673  0.030273  0.000764  \n",
       "39      0.007101  0.003587  0.017308  0.025577  0.028841  0.001673  \n",
       "40      0.002118  0.000260  0.566003  0.222384  0.198386  0.004211  \n",
       "41      0.012704  0.004444  0.089852  0.495824  0.785635  0.019734  \n",
       "42      0.000860  0.011653  0.003735  0.197490  0.041567  0.004652  \n",
       "43      0.013172  0.001618  0.005433  0.006701  0.076923  0.002790  \n",
       "44      0.016356  0.000887  0.002012  0.004485  0.008138  0.000403  \n",
       "45      0.005747  0.003806  0.018907  0.036523  0.040902  0.013881  \n",
       "46      0.011252  0.002217  0.006870  0.017341  0.068015  0.005844  \n",
       "47      0.034911  0.011817  0.012699  0.037881  0.055862  0.019190  \n",
       "48      0.031554  0.001446  0.007195  0.019688  0.102341  0.005389  \n",
       "49      0.003781  0.003298  0.001757  0.011503  0.050490  0.006883  \n",
       "...          ...       ...       ...       ...       ...       ...  \n",
       "128326  0.009046  0.002370  0.013250  0.045632  0.018517  0.003951  \n",
       "128327  0.014193  0.002542  0.006201  0.136952  0.009480  0.004264  \n",
       "128328  0.008075  0.005335  0.017986  0.063680  0.040284  0.001575  \n",
       "128329  0.003506  0.003949  0.006226  0.017313  0.004176  0.000554  \n",
       "128330  0.003560  0.001151  0.040455  0.042849  0.003271  0.003148  \n",
       "128331  0.005058  0.002957  0.011135  0.034383  0.151646  0.006971  \n",
       "128332  0.009130  0.007588  0.003702  0.012095  0.125095  0.000908  \n",
       "128333  0.078516  0.022315  0.016175  0.019846  0.018238  0.212806  \n",
       "128334  0.008806  0.000668  0.017181  0.013798  0.004122  0.000612  \n",
       "128335  0.002825  0.000300  0.002797  0.014164  0.003478  0.001470  \n",
       "128336  0.003463  0.000390  0.001486  0.018814  0.007110  0.004286  \n",
       "128337  0.009487  0.005650  0.047028  0.068479  0.027127  0.002081  \n",
       "128338  0.004484  0.002461  0.005687  0.022354  0.096438  0.005313  \n",
       "128339  0.007131  0.002607  0.008710  0.270150  0.026005  0.007594  \n",
       "128340  0.001998  0.000668  0.006330  0.022771  0.015206  0.002057  \n",
       "128341  0.016989  0.007868  0.015594  0.037794  0.039281  0.008842  \n",
       "128342  0.003306  0.002917  0.014193  0.015828  0.018519  0.005856  \n",
       "128343  0.004981  0.002572  0.006551  0.027417  0.003957  0.004000  \n",
       "128344  0.006185  0.002227  0.007881  0.024625  0.025670  0.014949  \n",
       "128345  0.029611  0.001103  0.012741  0.038897  0.025051  0.005747  \n",
       "128346  0.017998  0.000396  0.005247  0.026685  0.053445  0.005168  \n",
       "128347  0.011406  0.000687  0.003897  0.009908  0.008006  0.000457  \n",
       "128348  0.011100  0.002410  0.024263  0.066381  0.016690  0.007380  \n",
       "128349  0.006036  0.002118  0.023977  0.025026  0.005836  0.003102  \n",
       "128350  0.002263  0.000383  0.001661  0.005299  0.000727  0.001805  \n",
       "128351  0.008671  0.004505  0.041527  0.141172  0.016927  0.044290  \n",
       "128352  0.007195  0.000902  0.009653  0.031119  0.007259  0.011941  \n",
       "128353  0.009876  0.000446  0.017236  0.055050  0.004422  0.009926  \n",
       "128354  0.073191  0.003032  0.265854  0.168033  0.027313  0.010541  \n",
       "128355  0.004411  0.000523  0.034677  0.019522  0.009188  0.032989  \n",
       "128356  0.092996  0.000944  0.011225  0.027091  0.002925  0.000445  \n",
       "128357  0.022240  0.001347  0.007407  0.016679  0.007031  0.004273  \n",
       "128358  0.003827  0.003326  0.017651  0.097779  0.006926  0.008880  \n",
       "128359  0.005498  0.000885  0.005977  0.009973  0.007605  0.006052  \n",
       "128360  0.007406  0.001955  0.006855  0.047359  0.207585  0.004438  \n",
       "128361  0.006787  0.000561  0.008945  0.009430  0.006129  0.003425  \n",
       "128362  0.010110  0.003145  0.033257  0.055862  0.014646  0.005711  \n",
       "128363  0.003965  0.001248  0.074087  0.058170  0.039460  0.003872  \n",
       "128364  0.001278  0.002522  0.236244  0.016795  0.008806  0.001343  \n",
       "128365  0.002763  0.000255  0.309209  0.009066  0.003656  0.001869  \n",
       "128366  0.004075  0.002459  0.006344  0.043668  0.003579  0.004364  \n",
       "128367  0.011492  0.000978  0.004941  0.016961  0.003932  0.001891  \n",
       "128368  0.005357  0.003418  0.003115  0.022997  0.007747  0.002852  \n",
       "128369  0.030358  0.000634  0.010388  0.028654  0.003953  0.001414  \n",
       "128370  0.006210  0.000265  0.005152  0.043201  0.009588  0.005507  \n",
       "128371  0.005738  0.001125  0.007296  0.006019  0.002697  0.000282  \n",
       "128372  0.009474  0.002726  0.036399  0.015552  0.009212  0.003041  \n",
       "128373  0.001836  0.000716  0.002179  0.003252  0.000837  0.001653  \n",
       "128374  0.004713  0.001427  0.007751  0.033291  0.014821  0.005410  \n",
       "128375  0.001988  0.000334  0.018958  0.012048  0.019002  0.001803  \n",
       "\n",
       "[128376 rows x 13 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_pred_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_stack = oof_pred_stack.rename(columns=lambda s: s+\"_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_id</th>\n",
       "      <th>130123</th>\n",
       "      <th>130125</th>\n",
       "      <th>130129</th>\n",
       "      <th>130131</th>\n",
       "      <th>140307</th>\n",
       "      <th>140313</th>\n",
       "      <th>140316</th>\n",
       "      <th>140317</th>\n",
       "      <th>140321</th>\n",
       "      <th>140501</th>\n",
       "      <th>140505</th>\n",
       "      <th>140641</th>\n",
       "      <th>140691</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>njibeyLPrsnu4HCopjBihW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FqBfZgvrWVNMsCqGmZMdv3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KYE5JJ4y6zJBipkCKobwVg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tC5JqjsVxsKxQ8Ykk9S7fg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pigjc37smwP2E3Z4VtKinB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>YCHTGgFS6shv3GCMB7sB5j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nQsjoHBDtiJvKNUxzUoR4d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LXdStjW5USNHWpcXHd4E5j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9qw6QFmtqjjbPS9pMrwi7S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e6qaQuMJ3xsZJ96QmgCnxN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HYFuxPUAEMCcj6LAb4QmER</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JocGnYfx4qYadKzeaPctLc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HGbyviQqFLKMXRxJtMoTy7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62ubQAVp2p6UqUSA6Udxk2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pcpvb6nFijgNJ7a8qm6Wkh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>uJT7J9ik4ofmhL82yhuPUj</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bSrGfvw9LivGMGDvwgNZdd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>54LjSZcKzsEgGJFZhzStDN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BgXQUPyGMSJXcHDYyTq3rD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JbyVvLTSa9VXvC5ih9i8Sg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>qoPCLGyW9AuNuYey9VrDkk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RpRVwuyV3eG5qo3LudZwNk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xfUANe5qKkNHpDzgBkjtki</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Xa6swkgZirn3xJ7dfxkPxc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>v8ToYS27VqbMAEwjXSs3Eg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D9xHwctWqXg2cWtL5do8WM</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HJw8j6CLqATfmUcfSGCj9m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>qwT85z8GdnRfhE7qxMLcbe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>KBKcEhnC4AKZk5MKbC55xf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>R4CBpqHCVbGufzHwq3Mpxj</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ftz5AVKzxe2bnqb6QxFtHA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SwrNFka4nj9QnGhHQuPtZe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>p5Q4FMMRzdfN5PnbXvLYPG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>52yLTccEAEWmaKzumWxrEZ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>adzeEFtCXno47skKoGELRL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>DGHgXrMFi4uyhGx2C6LES7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>aa7Lick7NBCEJSXRHMGZ8a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>wtPXmSPAFD2Ha4aj5w37ET</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>pFEEnuyh54DfgB2uvnsfGm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mrJF7AkeTvCWcnYJ7mAxni</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RrXj94gsfmN6LgavYimqoe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TsRJJXmvg732DhDt8MVEki</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>XZsFA273csK7yUndLcpWqg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>JE8t9HUBCBuUL2VMGQfb5Q</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ZWcwhniND3ieECyM28rdVB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>wmKXgFyCNevTdErXfxNwBB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>QMiYGb9aS6JtLojt6jBdCe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>sM8Xnf5TG7Hd8VZemqqP5H</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>N2PnHCwkmyrHMA2njexPFD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mgiLGai8RCgujnodCbrDuF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128326</th>\n",
       "      <td>P6d5W5vCLmg3BJ3RUPxgvF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128327</th>\n",
       "      <td>KZVXqUevzF58MnwrsUAm54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128328</th>\n",
       "      <td>CgGnPbozUGmdHjvZGVrbWK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128329</th>\n",
       "      <td>NdBUNcA6BJAB7qncqeiZQD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128330</th>\n",
       "      <td>m9THdhH5nDrAym2WEmZeN6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128331</th>\n",
       "      <td>xEPpK5Fp3p5TpfznUFVLoM</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128332</th>\n",
       "      <td>3XkgPU82pKDq4ytu5RyZY7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128333</th>\n",
       "      <td>iKmzej9arWmuC6pHukcCJS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128334</th>\n",
       "      <td>io83TZYwKeTguzvHQjhYr4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128335</th>\n",
       "      <td>KVkYXdAdtjnL6uBwAfuJZA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128336</th>\n",
       "      <td>9dYFLGUKiVaagfXKjttyUb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128337</th>\n",
       "      <td>EmAbc5unejku7YYbVUkRsn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128338</th>\n",
       "      <td>T8Arwp9ytT9sArwF7cg6EK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128339</th>\n",
       "      <td>NsN2MRztpx37rMA7RyKTPN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128340</th>\n",
       "      <td>ACayZrwYDxVL2JrKsXCPgY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128341</th>\n",
       "      <td>UnyX7S2X6swjoW4o3ddCrb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128342</th>\n",
       "      <td>MFrwU5pNtgv5AaSQeLFFGd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128343</th>\n",
       "      <td>BTJx6unCs8QrbmLuoKuJ8j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128344</th>\n",
       "      <td>zeJYa9K5s4CoNeRMnBLPkf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128345</th>\n",
       "      <td>7FwAaHzrdjmewgMbDJ2oxJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128346</th>\n",
       "      <td>nLm7crXd2j92p5PsgUEvTN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128347</th>\n",
       "      <td>EwSAZfHevhAqVSoz75yfCJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128348</th>\n",
       "      <td>j4FrkAGD6oYjkzuWh7FQ6P</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128349</th>\n",
       "      <td>t3muYok7ZTcNZSMQbeUcMT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128350</th>\n",
       "      <td>T6NdUqAs9ChEcUejG5ruwG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128351</th>\n",
       "      <td>L4kSviAtCyZHX42b7a6Sbb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128352</th>\n",
       "      <td>jP4Qph2kPCvaVhLQx6vr4M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128353</th>\n",
       "      <td>TejKW9EsYSBVTdb2vovWEW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128354</th>\n",
       "      <td>wMUCjtf3qvLc5u5jd9GTX4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128355</th>\n",
       "      <td>HSxTJNAzekBTnYFk5WQVZn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128356</th>\n",
       "      <td>BbpVKPSJHSuw9DUoJn7MUY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128357</th>\n",
       "      <td>45wA8Rf6QFhMwKB74SFuEa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128358</th>\n",
       "      <td>weyoiViCoVRKuAujoCUag6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128359</th>\n",
       "      <td>TW9rxBbrwfLmjAJeMkmKU8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128360</th>\n",
       "      <td>BPoAPggA9uEmyDLQcwEdAK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128361</th>\n",
       "      <td>VrhCzPMS8aeyp6RXWxJhHk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128362</th>\n",
       "      <td>4nh2uQmYmFnedqCEZyXpBc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128363</th>\n",
       "      <td>fbS5FS9c2YeuJ2YYRyqysn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128364</th>\n",
       "      <td>WLExJDxwG93x6kwpRigAwV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128365</th>\n",
       "      <td>hHE7iYk973MPjntF8U89g2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128366</th>\n",
       "      <td>gfB3DfMhtUBn3KPiLMuevR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128367</th>\n",
       "      <td>BAMyNuKrFVB9hfbR7bSwmH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128368</th>\n",
       "      <td>gbFpWzcp2hbG7a4YiR4XGN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128369</th>\n",
       "      <td>g4yifhFA4cW7ztsXFTqfo7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128370</th>\n",
       "      <td>HMtbm8GKkxsDSpzPLQeYkF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128371</th>\n",
       "      <td>aJpMhZgCP3VWxjTkgCTxGK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128372</th>\n",
       "      <td>KLqw7L7yeZdVjL63xpqNkS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128373</th>\n",
       "      <td>6GWrUUS54k6QVMBMT26LzF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128374</th>\n",
       "      <td>LA3aFfzrdi6r2B6y9u2BkF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128375</th>\n",
       "      <td>nN22TUE4fzGWE2i5Y4r6Qe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128376 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   purchase_id  130123  130125  130129  130131  140307  \\\n",
       "0       njibeyLPrsnu4HCopjBihW       0       0       0       0       0   \n",
       "1       FqBfZgvrWVNMsCqGmZMdv3       0       0       1       0       0   \n",
       "2       KYE5JJ4y6zJBipkCKobwVg       0       0       0       0       0   \n",
       "3       tC5JqjsVxsKxQ8Ykk9S7fg       0       0       0       1       0   \n",
       "4       Pigjc37smwP2E3Z4VtKinB       0       0       0       0       0   \n",
       "5       YCHTGgFS6shv3GCMB7sB5j       0       0       0       1       0   \n",
       "6       nQsjoHBDtiJvKNUxzUoR4d       0       0       0       0       0   \n",
       "7       LXdStjW5USNHWpcXHd4E5j       0       0       0       0       0   \n",
       "8       9qw6QFmtqjjbPS9pMrwi7S       0       0       1       0       0   \n",
       "9       e6qaQuMJ3xsZJ96QmgCnxN       0       0       0       0       0   \n",
       "10      HYFuxPUAEMCcj6LAb4QmER       1       0       1       0       0   \n",
       "11      JocGnYfx4qYadKzeaPctLc       0       0       0       1       0   \n",
       "12      HGbyviQqFLKMXRxJtMoTy7       0       0       0       0       0   \n",
       "13      62ubQAVp2p6UqUSA6Udxk2       1       0       0       1       1   \n",
       "14      pcpvb6nFijgNJ7a8qm6Wkh       0       0       0       0       0   \n",
       "15      uJT7J9ik4ofmhL82yhuPUj       0       0       0       0       0   \n",
       "16      bSrGfvw9LivGMGDvwgNZdd       0       0       0       0       0   \n",
       "17      54LjSZcKzsEgGJFZhzStDN       1       0       0       0       1   \n",
       "18      BgXQUPyGMSJXcHDYyTq3rD       0       0       0       0       0   \n",
       "19      JbyVvLTSa9VXvC5ih9i8Sg       0       0       1       0       0   \n",
       "20      qoPCLGyW9AuNuYey9VrDkk       0       0       0       0       0   \n",
       "21      RpRVwuyV3eG5qo3LudZwNk       0       0       0       0       0   \n",
       "22      xfUANe5qKkNHpDzgBkjtki       0       0       0       0       0   \n",
       "23      Xa6swkgZirn3xJ7dfxkPxc       0       0       0       1       0   \n",
       "24      v8ToYS27VqbMAEwjXSs3Eg       0       0       0       0       0   \n",
       "25      D9xHwctWqXg2cWtL5do8WM       1       0       1       0       0   \n",
       "26      HJw8j6CLqATfmUcfSGCj9m       0       0       0       0       0   \n",
       "27      qwT85z8GdnRfhE7qxMLcbe       0       0       0       0       0   \n",
       "28      KBKcEhnC4AKZk5MKbC55xf       0       0       0       0       0   \n",
       "29      R4CBpqHCVbGufzHwq3Mpxj       0       1       0       0       0   \n",
       "30      ftz5AVKzxe2bnqb6QxFtHA       0       0       0       0       0   \n",
       "31      SwrNFka4nj9QnGhHQuPtZe       0       0       0       0       0   \n",
       "32      p5Q4FMMRzdfN5PnbXvLYPG       0       0       0       0       0   \n",
       "33      52yLTccEAEWmaKzumWxrEZ       1       0       0       1       0   \n",
       "34      adzeEFtCXno47skKoGELRL       1       0       0       0       0   \n",
       "35      DGHgXrMFi4uyhGx2C6LES7       0       0       0       0       0   \n",
       "36      aa7Lick7NBCEJSXRHMGZ8a       0       0       0       0       0   \n",
       "37      wtPXmSPAFD2Ha4aj5w37ET       0       0       0       0       0   \n",
       "38      pFEEnuyh54DfgB2uvnsfGm       0       0       0       0       1   \n",
       "39      mrJF7AkeTvCWcnYJ7mAxni       0       0       0       0       0   \n",
       "40      RrXj94gsfmN6LgavYimqoe       0       0       0       0       0   \n",
       "41      TsRJJXmvg732DhDt8MVEki       0       0       0       0       0   \n",
       "42      XZsFA273csK7yUndLcpWqg       0       0       0       0       0   \n",
       "43      JE8t9HUBCBuUL2VMGQfb5Q       0       0       0       0       0   \n",
       "44      ZWcwhniND3ieECyM28rdVB       0       0       0       0       0   \n",
       "45      wmKXgFyCNevTdErXfxNwBB       1       0       0       1       0   \n",
       "46      QMiYGb9aS6JtLojt6jBdCe       0       0       0       0       0   \n",
       "47      sM8Xnf5TG7Hd8VZemqqP5H       0       0       0       0       1   \n",
       "48      N2PnHCwkmyrHMA2njexPFD       0       0       0       0       0   \n",
       "49      mgiLGai8RCgujnodCbrDuF       0       0       0       0       0   \n",
       "...                        ...     ...     ...     ...     ...     ...   \n",
       "128326  P6d5W5vCLmg3BJ3RUPxgvF       0       0       0       0       0   \n",
       "128327  KZVXqUevzF58MnwrsUAm54       0       0       0       0       1   \n",
       "128328  CgGnPbozUGmdHjvZGVrbWK       0       0       0       0       0   \n",
       "128329  NdBUNcA6BJAB7qncqeiZQD       0       0       0       0       0   \n",
       "128330  m9THdhH5nDrAym2WEmZeN6       0       0       0       0       0   \n",
       "128331  xEPpK5Fp3p5TpfznUFVLoM       1       0       0       0       0   \n",
       "128332  3XkgPU82pKDq4ytu5RyZY7       0       0       0       0       0   \n",
       "128333  iKmzej9arWmuC6pHukcCJS       0       0       0       0       0   \n",
       "128334  io83TZYwKeTguzvHQjhYr4       0       0       0       0       0   \n",
       "128335  KVkYXdAdtjnL6uBwAfuJZA       0       0       0       0       1   \n",
       "128336  9dYFLGUKiVaagfXKjttyUb       0       0       1       0       0   \n",
       "128337  EmAbc5unejku7YYbVUkRsn       0       0       0       0       0   \n",
       "128338  T8Arwp9ytT9sArwF7cg6EK       0       0       0       0       0   \n",
       "128339  NsN2MRztpx37rMA7RyKTPN       0       0       0       0       0   \n",
       "128340  ACayZrwYDxVL2JrKsXCPgY       0       0       0       0       0   \n",
       "128341  UnyX7S2X6swjoW4o3ddCrb       0       0       0       1       0   \n",
       "128342  MFrwU5pNtgv5AaSQeLFFGd       0       0       0       0       0   \n",
       "128343  BTJx6unCs8QrbmLuoKuJ8j       0       0       0       0       0   \n",
       "128344  zeJYa9K5s4CoNeRMnBLPkf       0       0       0       0       0   \n",
       "128345  7FwAaHzrdjmewgMbDJ2oxJ       0       0       0       1       0   \n",
       "128346  nLm7crXd2j92p5PsgUEvTN       0       0       0       0       0   \n",
       "128347  EwSAZfHevhAqVSoz75yfCJ       0       0       0       0       0   \n",
       "128348  j4FrkAGD6oYjkzuWh7FQ6P       0       0       0       0       0   \n",
       "128349  t3muYok7ZTcNZSMQbeUcMT       0       0       0       0       0   \n",
       "128350  T6NdUqAs9ChEcUejG5ruwG       0       0       0       0       0   \n",
       "128351  L4kSviAtCyZHX42b7a6Sbb       0       0       0       0       0   \n",
       "128352  jP4Qph2kPCvaVhLQx6vr4M       0       0       0       0       0   \n",
       "128353  TejKW9EsYSBVTdb2vovWEW       0       0       0       0       0   \n",
       "128354  wMUCjtf3qvLc5u5jd9GTX4       0       0       0       0       0   \n",
       "128355  HSxTJNAzekBTnYFk5WQVZn       0       0       0       0       0   \n",
       "128356  BbpVKPSJHSuw9DUoJn7MUY       0       0       0       0       1   \n",
       "128357  45wA8Rf6QFhMwKB74SFuEa       0       0       0       0       0   \n",
       "128358  weyoiViCoVRKuAujoCUag6       1       0       0       0       0   \n",
       "128359  TW9rxBbrwfLmjAJeMkmKU8       0       0       0       0       0   \n",
       "128360  BPoAPggA9uEmyDLQcwEdAK       0       0       0       0       0   \n",
       "128361  VrhCzPMS8aeyp6RXWxJhHk       0       0       0       1       0   \n",
       "128362  4nh2uQmYmFnedqCEZyXpBc       0       0       0       0       0   \n",
       "128363  fbS5FS9c2YeuJ2YYRyqysn       0       0       0       0       0   \n",
       "128364  WLExJDxwG93x6kwpRigAwV       0       0       0       0       0   \n",
       "128365  hHE7iYk973MPjntF8U89g2       0       0       0       0       0   \n",
       "128366  gfB3DfMhtUBn3KPiLMuevR       0       0       0       0       0   \n",
       "128367  BAMyNuKrFVB9hfbR7bSwmH       0       0       0       0       0   \n",
       "128368  gbFpWzcp2hbG7a4YiR4XGN       0       0       0       0       0   \n",
       "128369  g4yifhFA4cW7ztsXFTqfo7       0       0       0       0       0   \n",
       "128370  HMtbm8GKkxsDSpzPLQeYkF       0       0       0       0       0   \n",
       "128371  aJpMhZgCP3VWxjTkgCTxGK       0       0       0       0       0   \n",
       "128372  KLqw7L7yeZdVjL63xpqNkS       0       0       0       0       0   \n",
       "128373  6GWrUUS54k6QVMBMT26LzF       0       0       0       0       0   \n",
       "128374  LA3aFfzrdi6r2B6y9u2BkF       0       0       0       0       0   \n",
       "128375  nN22TUE4fzGWE2i5Y4r6Qe       0       0       0       0       0   \n",
       "\n",
       "        140313  140316  140317  140321  140501  140505  140641  140691  \n",
       "0            0       0       0       0       0       0       0       0  \n",
       "1            0       0       0       0       0       0       0       0  \n",
       "2            0       0       0       0       0       0       0       0  \n",
       "3            0       0       0       0       0       0       0       0  \n",
       "4            0       0       0       0       0       1       0       0  \n",
       "5            0       0       0       0       0       0       0       0  \n",
       "6            0       0       0       0       0       0       0       0  \n",
       "7            0       1       0       0       0       0       0       0  \n",
       "8            0       0       0       0       0       0       0       0  \n",
       "9            0       0       0       0       0       0       0       0  \n",
       "10           0       0       0       0       0       0       0       0  \n",
       "11           0       0       0       0       0       0       1       0  \n",
       "12           0       0       0       0       0       0       0       0  \n",
       "13           0       0       0       0       0       0       0       0  \n",
       "14           0       0       0       0       1       1       0       0  \n",
       "15           0       0       0       0       0       1       0       0  \n",
       "16           0       0       0       0       0       0       0       0  \n",
       "17           0       0       0       0       0       0       0       0  \n",
       "18           1       0       0       0       0       0       0       0  \n",
       "19           0       0       0       0       0       0       0       0  \n",
       "20           0       0       0       0       0       0       0       0  \n",
       "21           0       0       0       0       0       0       0       0  \n",
       "22           1       0       0       0       0       0       0       0  \n",
       "23           0       0       0       0       0       0       0       0  \n",
       "24           1       0       0       0       0       0       0       0  \n",
       "25           0       0       0       0       0       0       0       0  \n",
       "26           0       0       0       0       0       0       0       0  \n",
       "27           1       0       0       0       0       0       0       0  \n",
       "28           0       0       0       0       0       1       0       0  \n",
       "29           0       0       0       0       0       0       0       0  \n",
       "30           0       0       0       0       0       0       0       0  \n",
       "31           0       0       0       0       0       0       0       0  \n",
       "32           0       0       0       0       0       0       0       0  \n",
       "33           0       0       0       0       0       0       0       0  \n",
       "34           0       0       0       0       0       0       1       0  \n",
       "35           0       0       0       0       0       0       0       0  \n",
       "36           0       0       0       0       0       0       0       0  \n",
       "37           0       0       0       0       0       0       0       0  \n",
       "38           0       0       0       0       1       0       0       0  \n",
       "39           0       0       0       0       0       0       0       0  \n",
       "40           0       0       0       0       0       0       0       0  \n",
       "41           1       0       0       0       0       0       0       0  \n",
       "42           0       0       0       0       0       0       0       0  \n",
       "43           0       0       0       0       0       0       0       0  \n",
       "44           0       0       0       0       0       0       0       0  \n",
       "45           0       0       0       0       0       0       0       0  \n",
       "46           0       0       0       0       0       0       0       0  \n",
       "47           0       0       0       0       0       0       0       0  \n",
       "48           0       0       0       0       0       0       0       0  \n",
       "49           0       0       0       0       0       0       0       0  \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "128326       0       0       0       0       0       0       0       0  \n",
       "128327       0       1       0       0       0       0       0       0  \n",
       "128328       0       0       0       0       0       0       0       0  \n",
       "128329       0       0       0       0       0       0       0       0  \n",
       "128330       0       0       0       0       0       0       0       0  \n",
       "128331       0       0       0       0       0       0       0       0  \n",
       "128332       0       0       0       0       0       0       0       0  \n",
       "128333       0       0       0       0       0       0       0       0  \n",
       "128334       0       0       0       0       0       0       0       0  \n",
       "128335       0       0       0       0       0       0       0       0  \n",
       "128336       0       0       0       0       0       0       0       0  \n",
       "128337       0       0       0       0       0       0       0       0  \n",
       "128338       0       0       0       0       0       0       0       0  \n",
       "128339       0       0       0       0       0       0       0       0  \n",
       "128340       0       0       0       0       0       0       0       0  \n",
       "128341       0       0       0       0       0       0       0       0  \n",
       "128342       1       0       0       0       0       0       0       0  \n",
       "128343       0       0       0       0       0       0       0       0  \n",
       "128344       0       0       0       0       0       0       0       0  \n",
       "128345       0       0       0       0       0       0       0       0  \n",
       "128346       0       0       0       0       0       0       0       1  \n",
       "128347       0       0       0       0       0       0       0       0  \n",
       "128348       0       0       0       0       0       0       0       0  \n",
       "128349       1       0       0       0       0       0       0       0  \n",
       "128350       0       0       0       0       0       0       0       0  \n",
       "128351       0       0       0       0       0       0       0       0  \n",
       "128352       0       0       0       0       0       0       0       0  \n",
       "128353       0       0       0       0       0       0       0       0  \n",
       "128354       0       0       0       0       0       0       0       0  \n",
       "128355       0       0       0       0       0       0       0       0  \n",
       "128356       0       0       0       0       0       0       0       0  \n",
       "128357       0       0       0       0       0       1       0       0  \n",
       "128358       0       0       0       0       0       0       0       0  \n",
       "128359       0       0       0       0       0       0       0       0  \n",
       "128360       0       0       0       0       0       0       0       0  \n",
       "128361       0       0       0       0       0       0       0       0  \n",
       "128362       0       0       0       0       0       1       0       0  \n",
       "128363       0       0       0       0       0       0       0       0  \n",
       "128364       0       0       0       0       0       0       0       0  \n",
       "128365       0       0       0       0       0       0       0       0  \n",
       "128366       0       0       0       0       0       0       0       0  \n",
       "128367       0       0       0       0       0       0       0       0  \n",
       "128368       0       0       0       0       0       0       0       0  \n",
       "128369       0       0       0       0       0       0       0       0  \n",
       "128370       0       0       0       0       0       0       0       0  \n",
       "128371       1       0       0       0       0       0       0       0  \n",
       "128372       0       0       0       0       0       0       0       0  \n",
       "128373       0       0       0       0       0       0       0       0  \n",
       "128374       0       0       0       0       0       0       0       0  \n",
       "128375       0       0       0       0       0       0       0       0  \n",
       "\n",
       "[128376 rows x 14 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plus_oof_pred = pd.concat([train,oof_pred_stack],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_id</th>\n",
       "      <th>130123</th>\n",
       "      <th>130125</th>\n",
       "      <th>130129</th>\n",
       "      <th>130131</th>\n",
       "      <th>140307</th>\n",
       "      <th>140313</th>\n",
       "      <th>140316</th>\n",
       "      <th>140317</th>\n",
       "      <th>140321</th>\n",
       "      <th>140501</th>\n",
       "      <th>140505</th>\n",
       "      <th>140641</th>\n",
       "      <th>140691</th>\n",
       "      <th>130123_pred</th>\n",
       "      <th>130125_pred</th>\n",
       "      <th>130129_pred</th>\n",
       "      <th>130131_pred</th>\n",
       "      <th>140307_pred</th>\n",
       "      <th>140313_pred</th>\n",
       "      <th>140316_pred</th>\n",
       "      <th>140317_pred</th>\n",
       "      <th>140321_pred</th>\n",
       "      <th>140501_pred</th>\n",
       "      <th>140505_pred</th>\n",
       "      <th>140641_pred</th>\n",
       "      <th>140691_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>njibeyLPrsnu4HCopjBihW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040674</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.062851</td>\n",
       "      <td>0.042257</td>\n",
       "      <td>0.025029</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.028405</td>\n",
       "      <td>0.017969</td>\n",
       "      <td>0.007592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FqBfZgvrWVNMsCqGmZMdv3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.085360</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>0.170129</td>\n",
       "      <td>0.029627</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>0.015314</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.023297</td>\n",
       "      <td>0.022189</td>\n",
       "      <td>0.005705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KYE5JJ4y6zJBipkCKobwVg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.126139</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.127633</td>\n",
       "      <td>0.063036</td>\n",
       "      <td>0.012105</td>\n",
       "      <td>0.027708</td>\n",
       "      <td>0.006339</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.022045</td>\n",
       "      <td>0.049357</td>\n",
       "      <td>0.001946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tC5JqjsVxsKxQ8Ykk9S7fg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121464</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.275874</td>\n",
       "      <td>0.093228</td>\n",
       "      <td>0.138492</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.097828</td>\n",
       "      <td>0.006208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pigjc37smwP2E3Z4VtKinB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157340</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.071833</td>\n",
       "      <td>0.200286</td>\n",
       "      <td>0.175853</td>\n",
       "      <td>0.016206</td>\n",
       "      <td>0.009456</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.015241</td>\n",
       "      <td>0.034619</td>\n",
       "      <td>0.079691</td>\n",
       "      <td>0.021228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>YCHTGgFS6shv3GCMB7sB5j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057167</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.077358</td>\n",
       "      <td>0.195276</td>\n",
       "      <td>0.024284</td>\n",
       "      <td>0.017331</td>\n",
       "      <td>0.008168</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.216117</td>\n",
       "      <td>0.078690</td>\n",
       "      <td>0.043991</td>\n",
       "      <td>0.005783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nQsjoHBDtiJvKNUxzUoR4d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.122690</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.063108</td>\n",
       "      <td>0.262616</td>\n",
       "      <td>0.036614</td>\n",
       "      <td>0.061340</td>\n",
       "      <td>0.008512</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.195561</td>\n",
       "      <td>0.034565</td>\n",
       "      <td>0.016314</td>\n",
       "      <td>0.005006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LXdStjW5USNHWpcXHd4E5j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109098</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>0.087265</td>\n",
       "      <td>0.161923</td>\n",
       "      <td>0.336647</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.009443</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.006566</td>\n",
       "      <td>0.024890</td>\n",
       "      <td>0.052241</td>\n",
       "      <td>0.011596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9qw6QFmtqjjbPS9pMrwi7S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046405</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>0.044197</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.063945</td>\n",
       "      <td>0.061068</td>\n",
       "      <td>0.070388</td>\n",
       "      <td>0.024996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e6qaQuMJ3xsZJ96QmgCnxN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.365136</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>0.081919</td>\n",
       "      <td>0.094931</td>\n",
       "      <td>0.019410</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.008816</td>\n",
       "      <td>0.030763</td>\n",
       "      <td>0.070182</td>\n",
       "      <td>0.005038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HYFuxPUAEMCcj6LAb4QmER</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079568</td>\n",
       "      <td>0.010245</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>0.180175</td>\n",
       "      <td>0.081485</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.028116</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>0.011217</td>\n",
       "      <td>0.071563</td>\n",
       "      <td>0.004105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JocGnYfx4qYadKzeaPctLc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>0.154132</td>\n",
       "      <td>0.239312</td>\n",
       "      <td>0.140326</td>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.041497</td>\n",
       "      <td>0.047794</td>\n",
       "      <td>0.003467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HGbyviQqFLKMXRxJtMoTy7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061159</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>0.090856</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>0.035859</td>\n",
       "      <td>0.010134</td>\n",
       "      <td>0.008642</td>\n",
       "      <td>0.014663</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.017288</td>\n",
       "      <td>0.027841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62ubQAVp2p6UqUSA6Udxk2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044905</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.081006</td>\n",
       "      <td>0.089117</td>\n",
       "      <td>0.035541</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>0.489203</td>\n",
       "      <td>0.075807</td>\n",
       "      <td>0.133046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pcpvb6nFijgNJ7a8qm6Wkh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041956</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>0.152122</td>\n",
       "      <td>0.161042</td>\n",
       "      <td>0.030062</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.809229</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.005571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>uJT7J9ik4ofmhL82yhuPUj</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026372</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.029664</td>\n",
       "      <td>0.037896</td>\n",
       "      <td>0.016809</td>\n",
       "      <td>0.014251</td>\n",
       "      <td>0.047587</td>\n",
       "      <td>0.010461</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.040108</td>\n",
       "      <td>0.188952</td>\n",
       "      <td>0.019304</td>\n",
       "      <td>0.020615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bSrGfvw9LivGMGDvwgNZdd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.094259</td>\n",
       "      <td>0.094129</td>\n",
       "      <td>0.046128</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.150735</td>\n",
       "      <td>0.085195</td>\n",
       "      <td>0.029019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>54LjSZcKzsEgGJFZhzStDN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.096181</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.085306</td>\n",
       "      <td>0.060159</td>\n",
       "      <td>0.040952</td>\n",
       "      <td>0.049391</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.127838</td>\n",
       "      <td>0.061707</td>\n",
       "      <td>0.133956</td>\n",
       "      <td>0.002775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BgXQUPyGMSJXcHDYyTq3rD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054043</td>\n",
       "      <td>0.020492</td>\n",
       "      <td>0.032625</td>\n",
       "      <td>0.052402</td>\n",
       "      <td>0.023557</td>\n",
       "      <td>0.263156</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.397490</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.001227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JbyVvLTSa9VXvC5ih9i8Sg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039348</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.062444</td>\n",
       "      <td>0.041439</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.795809</td>\n",
       "      <td>0.037015</td>\n",
       "      <td>0.093604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>qoPCLGyW9AuNuYey9VrDkk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.197946</td>\n",
       "      <td>0.019126</td>\n",
       "      <td>0.070284</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.096562</td>\n",
       "      <td>0.076668</td>\n",
       "      <td>0.025669</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.062480</td>\n",
       "      <td>0.112766</td>\n",
       "      <td>0.082681</td>\n",
       "      <td>0.065014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RpRVwuyV3eG5qo3LudZwNk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063994</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.038653</td>\n",
       "      <td>0.061898</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.055832</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.003134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xfUANe5qKkNHpDzgBkjtki</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041434</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.038662</td>\n",
       "      <td>0.054431</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>0.003739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Xa6swkgZirn3xJ7dfxkPxc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041471</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.017254</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.041028</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.043102</td>\n",
       "      <td>0.025420</td>\n",
       "      <td>0.019314</td>\n",
       "      <td>0.012347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>v8ToYS27VqbMAEwjXSs3Eg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113406</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.044823</td>\n",
       "      <td>0.111065</td>\n",
       "      <td>0.070741</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.052875</td>\n",
       "      <td>0.061712</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>0.005159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D9xHwctWqXg2cWtL5do8WM</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054736</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.015811</td>\n",
       "      <td>0.046607</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.045782</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>0.017139</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HJw8j6CLqATfmUcfSGCj9m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055232</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.072642</td>\n",
       "      <td>0.228658</td>\n",
       "      <td>0.050238</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.004326</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.106131</td>\n",
       "      <td>0.068459</td>\n",
       "      <td>0.039691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>qwT85z8GdnRfhE7qxMLcbe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056206</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.020788</td>\n",
       "      <td>0.036447</td>\n",
       "      <td>0.078111</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.317557</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>0.060651</td>\n",
       "      <td>0.076743</td>\n",
       "      <td>0.003289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>KBKcEhnC4AKZk5MKbC55xf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260726</td>\n",
       "      <td>0.009056</td>\n",
       "      <td>0.122793</td>\n",
       "      <td>0.313873</td>\n",
       "      <td>0.552543</td>\n",
       "      <td>0.019892</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.025298</td>\n",
       "      <td>0.080024</td>\n",
       "      <td>0.021360</td>\n",
       "      <td>0.001155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>R4CBpqHCVbGufzHwq3Mpxj</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102571</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.047837</td>\n",
       "      <td>0.114320</td>\n",
       "      <td>0.012909</td>\n",
       "      <td>0.018308</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>0.005795</td>\n",
       "      <td>0.070586</td>\n",
       "      <td>0.116605</td>\n",
       "      <td>0.110112</td>\n",
       "      <td>0.041476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ftz5AVKzxe2bnqb6QxFtHA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023222</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.119785</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.018019</td>\n",
       "      <td>0.364247</td>\n",
       "      <td>0.189599</td>\n",
       "      <td>0.003772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SwrNFka4nj9QnGhHQuPtZe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026429</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.059208</td>\n",
       "      <td>0.043634</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.009759</td>\n",
       "      <td>0.016282</td>\n",
       "      <td>0.056762</td>\n",
       "      <td>0.010654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>p5Q4FMMRzdfN5PnbXvLYPG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027181</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.030387</td>\n",
       "      <td>0.046613</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.012976</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.077024</td>\n",
       "      <td>0.005233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>52yLTccEAEWmaKzumWxrEZ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072247</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.024490</td>\n",
       "      <td>0.118829</td>\n",
       "      <td>0.036453</td>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.046279</td>\n",
       "      <td>0.024379</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.049517</td>\n",
       "      <td>0.020649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>adzeEFtCXno47skKoGELRL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055254</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.066181</td>\n",
       "      <td>0.137164</td>\n",
       "      <td>0.044227</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.044801</td>\n",
       "      <td>0.040590</td>\n",
       "      <td>0.123403</td>\n",
       "      <td>0.006302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>DGHgXrMFi4uyhGx2C6LES7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031331</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.015508</td>\n",
       "      <td>0.067096</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.009784</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>0.026653</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>0.013452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>aa7Lick7NBCEJSXRHMGZ8a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.190444</td>\n",
       "      <td>0.007190</td>\n",
       "      <td>0.088450</td>\n",
       "      <td>0.209372</td>\n",
       "      <td>0.030015</td>\n",
       "      <td>0.013437</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>0.075707</td>\n",
       "      <td>0.040142</td>\n",
       "      <td>0.006768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>wtPXmSPAFD2Ha4aj5w37ET</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040774</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.026747</td>\n",
       "      <td>0.043536</td>\n",
       "      <td>0.053947</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>0.246843</td>\n",
       "      <td>0.047128</td>\n",
       "      <td>0.004159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>pFEEnuyh54DfgB2uvnsfGm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111156</td>\n",
       "      <td>0.006908</td>\n",
       "      <td>0.183072</td>\n",
       "      <td>0.363087</td>\n",
       "      <td>0.051420</td>\n",
       "      <td>0.022599</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.009444</td>\n",
       "      <td>0.037673</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mrJF7AkeTvCWcnYJ7mAxni</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091918</td>\n",
       "      <td>0.013110</td>\n",
       "      <td>0.031401</td>\n",
       "      <td>0.095199</td>\n",
       "      <td>0.163837</td>\n",
       "      <td>0.336636</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.017308</td>\n",
       "      <td>0.025577</td>\n",
       "      <td>0.028841</td>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RrXj94gsfmN6LgavYimqoe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.015166</td>\n",
       "      <td>0.045015</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.015153</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.566003</td>\n",
       "      <td>0.222384</td>\n",
       "      <td>0.198386</td>\n",
       "      <td>0.004211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TsRJJXmvg732DhDt8MVEki</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.187936</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.086250</td>\n",
       "      <td>0.123461</td>\n",
       "      <td>0.086219</td>\n",
       "      <td>0.056654</td>\n",
       "      <td>0.013818</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.089852</td>\n",
       "      <td>0.495824</td>\n",
       "      <td>0.785635</td>\n",
       "      <td>0.019734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>XZsFA273csK7yUndLcpWqg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.068163</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.088175</td>\n",
       "      <td>0.022753</td>\n",
       "      <td>0.023855</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.011653</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.197490</td>\n",
       "      <td>0.041567</td>\n",
       "      <td>0.004652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>JE8t9HUBCBuUL2VMGQfb5Q</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032174</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.135915</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>0.022837</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.002790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ZWcwhniND3ieECyM28rdVB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.016356</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>wmKXgFyCNevTdErXfxNwBB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089975</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.042298</td>\n",
       "      <td>0.109962</td>\n",
       "      <td>0.025848</td>\n",
       "      <td>0.033754</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>0.036523</td>\n",
       "      <td>0.040902</td>\n",
       "      <td>0.013881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>QMiYGb9aS6JtLojt6jBdCe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024052</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.036286</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>0.016090</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.068015</td>\n",
       "      <td>0.005844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>sM8Xnf5TG7Hd8VZemqqP5H</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.322679</td>\n",
       "      <td>0.042881</td>\n",
       "      <td>0.280757</td>\n",
       "      <td>0.239979</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>0.086332</td>\n",
       "      <td>0.035925</td>\n",
       "      <td>0.034911</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.037881</td>\n",
       "      <td>0.055862</td>\n",
       "      <td>0.019190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>N2PnHCwkmyrHMA2njexPFD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065344</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.027197</td>\n",
       "      <td>0.024965</td>\n",
       "      <td>0.192749</td>\n",
       "      <td>0.053048</td>\n",
       "      <td>0.031554</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.019688</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.005389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mgiLGai8RCgujnodCbrDuF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107778</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.101960</td>\n",
       "      <td>0.075626</td>\n",
       "      <td>0.029267</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>0.050490</td>\n",
       "      <td>0.006883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128326</th>\n",
       "      <td>P6d5W5vCLmg3BJ3RUPxgvF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.267781</td>\n",
       "      <td>0.017042</td>\n",
       "      <td>0.072724</td>\n",
       "      <td>0.076038</td>\n",
       "      <td>0.024175</td>\n",
       "      <td>0.042266</td>\n",
       "      <td>0.023507</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.045632</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.003951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128327</th>\n",
       "      <td>KZVXqUevzF58MnwrsUAm54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075486</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.018180</td>\n",
       "      <td>0.073437</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>0.013194</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.014193</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.006201</td>\n",
       "      <td>0.136952</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.004264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128328</th>\n",
       "      <td>CgGnPbozUGmdHjvZGVrbWK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028529</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.063680</td>\n",
       "      <td>0.040284</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128329</th>\n",
       "      <td>NdBUNcA6BJAB7qncqeiZQD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153220</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>0.068846</td>\n",
       "      <td>0.052140</td>\n",
       "      <td>0.015265</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.000554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128330</th>\n",
       "      <td>m9THdhH5nDrAym2WEmZeN6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107490</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.019094</td>\n",
       "      <td>0.084472</td>\n",
       "      <td>0.072056</td>\n",
       "      <td>0.032801</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.040455</td>\n",
       "      <td>0.042849</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.003148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128331</th>\n",
       "      <td>xEPpK5Fp3p5TpfznUFVLoM</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078056</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>0.088439</td>\n",
       "      <td>0.041684</td>\n",
       "      <td>0.031097</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.151646</td>\n",
       "      <td>0.006971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128332</th>\n",
       "      <td>3XkgPU82pKDq4ytu5RyZY7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193809</td>\n",
       "      <td>0.027033</td>\n",
       "      <td>0.088583</td>\n",
       "      <td>0.269442</td>\n",
       "      <td>0.041934</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.007588</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.012095</td>\n",
       "      <td>0.125095</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128333</th>\n",
       "      <td>iKmzej9arWmuC6pHukcCJS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835790</td>\n",
       "      <td>0.016157</td>\n",
       "      <td>0.240370</td>\n",
       "      <td>0.498923</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>0.047217</td>\n",
       "      <td>0.015681</td>\n",
       "      <td>0.078516</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>0.019846</td>\n",
       "      <td>0.018238</td>\n",
       "      <td>0.212806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128334</th>\n",
       "      <td>io83TZYwKeTguzvHQjhYr4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074030</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.031598</td>\n",
       "      <td>0.107120</td>\n",
       "      <td>0.011913</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.017181</td>\n",
       "      <td>0.013798</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128335</th>\n",
       "      <td>KVkYXdAdtjnL6uBwAfuJZA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107261</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>0.123970</td>\n",
       "      <td>0.027995</td>\n",
       "      <td>0.047113</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>0.001470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128336</th>\n",
       "      <td>9dYFLGUKiVaagfXKjttyUb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.192988</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.024538</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.004286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128337</th>\n",
       "      <td>EmAbc5unejku7YYbVUkRsn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.621995</td>\n",
       "      <td>0.046994</td>\n",
       "      <td>0.090381</td>\n",
       "      <td>0.226377</td>\n",
       "      <td>0.232607</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.017930</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.047028</td>\n",
       "      <td>0.068479</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128338</th>\n",
       "      <td>T8Arwp9ytT9sArwF7cg6EK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054893</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.041183</td>\n",
       "      <td>0.070429</td>\n",
       "      <td>0.781999</td>\n",
       "      <td>0.059884</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.005687</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.096438</td>\n",
       "      <td>0.005313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128339</th>\n",
       "      <td>NsN2MRztpx37rMA7RyKTPN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.115606</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.035498</td>\n",
       "      <td>0.092977</td>\n",
       "      <td>0.066719</td>\n",
       "      <td>0.031171</td>\n",
       "      <td>0.007437</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.270150</td>\n",
       "      <td>0.026005</td>\n",
       "      <td>0.007594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128340</th>\n",
       "      <td>ACayZrwYDxVL2JrKsXCPgY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092148</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.119503</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.008356</td>\n",
       "      <td>0.005703</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.002057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128341</th>\n",
       "      <td>UnyX7S2X6swjoW4o3ddCrb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222970</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.026831</td>\n",
       "      <td>0.522513</td>\n",
       "      <td>0.090257</td>\n",
       "      <td>0.014196</td>\n",
       "      <td>0.027426</td>\n",
       "      <td>0.016989</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.037794</td>\n",
       "      <td>0.039281</td>\n",
       "      <td>0.008842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128342</th>\n",
       "      <td>MFrwU5pNtgv5AaSQeLFFGd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127603</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.087260</td>\n",
       "      <td>0.054336</td>\n",
       "      <td>0.053410</td>\n",
       "      <td>0.038022</td>\n",
       "      <td>0.026066</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.014193</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.005856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128343</th>\n",
       "      <td>BTJx6unCs8QrbmLuoKuJ8j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062447</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>0.054428</td>\n",
       "      <td>0.022047</td>\n",
       "      <td>0.144439</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.027417</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128344</th>\n",
       "      <td>zeJYa9K5s4CoNeRMnBLPkf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091427</td>\n",
       "      <td>0.010324</td>\n",
       "      <td>0.040573</td>\n",
       "      <td>0.069516</td>\n",
       "      <td>0.055728</td>\n",
       "      <td>0.019834</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.024625</td>\n",
       "      <td>0.025670</td>\n",
       "      <td>0.014949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128345</th>\n",
       "      <td>7FwAaHzrdjmewgMbDJ2oxJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129026</td>\n",
       "      <td>0.015356</td>\n",
       "      <td>0.032906</td>\n",
       "      <td>0.097279</td>\n",
       "      <td>0.156396</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>0.029611</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>0.038897</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.005747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128346</th>\n",
       "      <td>nLm7crXd2j92p5PsgUEvTN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.172413</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>0.054520</td>\n",
       "      <td>0.258535</td>\n",
       "      <td>0.020083</td>\n",
       "      <td>0.037071</td>\n",
       "      <td>0.029325</td>\n",
       "      <td>0.017998</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.026685</td>\n",
       "      <td>0.053445</td>\n",
       "      <td>0.005168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128347</th>\n",
       "      <td>EwSAZfHevhAqVSoz75yfCJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087219</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.083837</td>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.024680</td>\n",
       "      <td>0.021607</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.011406</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128348</th>\n",
       "      <td>j4FrkAGD6oYjkzuWh7FQ6P</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095306</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>0.040032</td>\n",
       "      <td>0.094283</td>\n",
       "      <td>0.022671</td>\n",
       "      <td>0.017450</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.024263</td>\n",
       "      <td>0.066381</td>\n",
       "      <td>0.016690</td>\n",
       "      <td>0.007380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128349</th>\n",
       "      <td>t3muYok7ZTcNZSMQbeUcMT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040847</td>\n",
       "      <td>0.009691</td>\n",
       "      <td>0.022291</td>\n",
       "      <td>0.029264</td>\n",
       "      <td>0.047977</td>\n",
       "      <td>0.012241</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.023977</td>\n",
       "      <td>0.025026</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.003102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128350</th>\n",
       "      <td>T6NdUqAs9ChEcUejG5ruwG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057297</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>0.093683</td>\n",
       "      <td>0.061593</td>\n",
       "      <td>0.012086</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.001805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128351</th>\n",
       "      <td>L4kSviAtCyZHX42b7a6Sbb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.106541</td>\n",
       "      <td>0.014771</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.187949</td>\n",
       "      <td>0.034316</td>\n",
       "      <td>0.026337</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.041527</td>\n",
       "      <td>0.141172</td>\n",
       "      <td>0.016927</td>\n",
       "      <td>0.044290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128352</th>\n",
       "      <td>jP4Qph2kPCvaVhLQx6vr4M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079477</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.046532</td>\n",
       "      <td>0.070907</td>\n",
       "      <td>0.025855</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>0.031119</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.011941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128353</th>\n",
       "      <td>TejKW9EsYSBVTdb2vovWEW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139489</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.178122</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.055050</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.009926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128354</th>\n",
       "      <td>wMUCjtf3qvLc5u5jd9GTX4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119519</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>0.059871</td>\n",
       "      <td>0.206195</td>\n",
       "      <td>0.043901</td>\n",
       "      <td>0.070081</td>\n",
       "      <td>0.172358</td>\n",
       "      <td>0.073191</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.265854</td>\n",
       "      <td>0.168033</td>\n",
       "      <td>0.027313</td>\n",
       "      <td>0.010541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128355</th>\n",
       "      <td>HSxTJNAzekBTnYFk5WQVZn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061592</td>\n",
       "      <td>0.007806</td>\n",
       "      <td>0.011195</td>\n",
       "      <td>0.066337</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.016806</td>\n",
       "      <td>0.009131</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.034677</td>\n",
       "      <td>0.019522</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>0.032989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128356</th>\n",
       "      <td>BbpVKPSJHSuw9DUoJn7MUY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170436</td>\n",
       "      <td>0.502818</td>\n",
       "      <td>0.081219</td>\n",
       "      <td>0.085122</td>\n",
       "      <td>0.039271</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.092996</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.027091</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128357</th>\n",
       "      <td>45wA8Rf6QFhMwKB74SFuEa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063930</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>0.051476</td>\n",
       "      <td>0.011925</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.022240</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.016679</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.004273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128358</th>\n",
       "      <td>weyoiViCoVRKuAujoCUag6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.399266</td>\n",
       "      <td>0.021684</td>\n",
       "      <td>0.106649</td>\n",
       "      <td>0.527720</td>\n",
       "      <td>0.021666</td>\n",
       "      <td>0.085019</td>\n",
       "      <td>0.203758</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>0.097779</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>0.008880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128359</th>\n",
       "      <td>TW9rxBbrwfLmjAJeMkmKU8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064149</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>0.017747</td>\n",
       "      <td>0.046414</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.132508</td>\n",
       "      <td>0.005498</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.005977</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.006052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128360</th>\n",
       "      <td>BPoAPggA9uEmyDLQcwEdAK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.211671</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.398195</td>\n",
       "      <td>0.120440</td>\n",
       "      <td>0.292873</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.011412</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.006855</td>\n",
       "      <td>0.047359</td>\n",
       "      <td>0.207585</td>\n",
       "      <td>0.004438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128361</th>\n",
       "      <td>VrhCzPMS8aeyp6RXWxJhHk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080531</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>0.008143</td>\n",
       "      <td>0.115370</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.018526</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>0.003425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128362</th>\n",
       "      <td>4nh2uQmYmFnedqCEZyXpBc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.104335</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.063325</td>\n",
       "      <td>0.077384</td>\n",
       "      <td>0.067053</td>\n",
       "      <td>0.036992</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>0.055862</td>\n",
       "      <td>0.014646</td>\n",
       "      <td>0.005711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128363</th>\n",
       "      <td>fbS5FS9c2YeuJ2YYRyqysn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052033</td>\n",
       "      <td>0.009047</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.010308</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.074087</td>\n",
       "      <td>0.058170</td>\n",
       "      <td>0.039460</td>\n",
       "      <td>0.003872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128364</th>\n",
       "      <td>WLExJDxwG93x6kwpRigAwV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051925</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.027733</td>\n",
       "      <td>0.150894</td>\n",
       "      <td>0.018294</td>\n",
       "      <td>0.021442</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.236244</td>\n",
       "      <td>0.016795</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128365</th>\n",
       "      <td>hHE7iYk973MPjntF8U89g2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.309209</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.001869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128366</th>\n",
       "      <td>gfB3DfMhtUBn3KPiLMuevR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127554</td>\n",
       "      <td>0.022465</td>\n",
       "      <td>0.035363</td>\n",
       "      <td>0.165323</td>\n",
       "      <td>0.042124</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>0.043668</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128367</th>\n",
       "      <td>BAMyNuKrFVB9hfbR7bSwmH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.269526</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.071512</td>\n",
       "      <td>0.267386</td>\n",
       "      <td>0.017118</td>\n",
       "      <td>0.053507</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.001891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128368</th>\n",
       "      <td>gbFpWzcp2hbG7a4YiR4XGN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.182727</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.075764</td>\n",
       "      <td>0.283841</td>\n",
       "      <td>0.103625</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.022997</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128369</th>\n",
       "      <td>g4yifhFA4cW7ztsXFTqfo7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063185</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.034815</td>\n",
       "      <td>0.078383</td>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.023001</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.001414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128370</th>\n",
       "      <td>HMtbm8GKkxsDSpzPLQeYkF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.073455</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.156536</td>\n",
       "      <td>0.094165</td>\n",
       "      <td>0.114267</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.006210</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>0.043201</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.005507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128371</th>\n",
       "      <td>aJpMhZgCP3VWxjTkgCTxGK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.099073</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.113751</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>0.017179</td>\n",
       "      <td>0.014450</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.007296</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128372</th>\n",
       "      <td>KLqw7L7yeZdVjL63xpqNkS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036741</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>0.040602</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>0.010083</td>\n",
       "      <td>0.012278</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.036399</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>0.009212</td>\n",
       "      <td>0.003041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128373</th>\n",
       "      <td>6GWrUUS54k6QVMBMT26LzF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>0.069937</td>\n",
       "      <td>0.054073</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.087104</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128374</th>\n",
       "      <td>LA3aFfzrdi6r2B6y9u2BkF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.023168</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.007751</td>\n",
       "      <td>0.033291</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>0.005410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128375</th>\n",
       "      <td>nN22TUE4fzGWE2i5Y4r6Qe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036759</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.012942</td>\n",
       "      <td>0.067150</td>\n",
       "      <td>0.015591</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.001803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128376 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   purchase_id  130123  130125  130129  130131  140307  \\\n",
       "0       njibeyLPrsnu4HCopjBihW       0       0       0       0       0   \n",
       "1       FqBfZgvrWVNMsCqGmZMdv3       0       0       1       0       0   \n",
       "2       KYE5JJ4y6zJBipkCKobwVg       0       0       0       0       0   \n",
       "3       tC5JqjsVxsKxQ8Ykk9S7fg       0       0       0       1       0   \n",
       "4       Pigjc37smwP2E3Z4VtKinB       0       0       0       0       0   \n",
       "5       YCHTGgFS6shv3GCMB7sB5j       0       0       0       1       0   \n",
       "6       nQsjoHBDtiJvKNUxzUoR4d       0       0       0       0       0   \n",
       "7       LXdStjW5USNHWpcXHd4E5j       0       0       0       0       0   \n",
       "8       9qw6QFmtqjjbPS9pMrwi7S       0       0       1       0       0   \n",
       "9       e6qaQuMJ3xsZJ96QmgCnxN       0       0       0       0       0   \n",
       "10      HYFuxPUAEMCcj6LAb4QmER       1       0       1       0       0   \n",
       "11      JocGnYfx4qYadKzeaPctLc       0       0       0       1       0   \n",
       "12      HGbyviQqFLKMXRxJtMoTy7       0       0       0       0       0   \n",
       "13      62ubQAVp2p6UqUSA6Udxk2       1       0       0       1       1   \n",
       "14      pcpvb6nFijgNJ7a8qm6Wkh       0       0       0       0       0   \n",
       "15      uJT7J9ik4ofmhL82yhuPUj       0       0       0       0       0   \n",
       "16      bSrGfvw9LivGMGDvwgNZdd       0       0       0       0       0   \n",
       "17      54LjSZcKzsEgGJFZhzStDN       1       0       0       0       1   \n",
       "18      BgXQUPyGMSJXcHDYyTq3rD       0       0       0       0       0   \n",
       "19      JbyVvLTSa9VXvC5ih9i8Sg       0       0       1       0       0   \n",
       "20      qoPCLGyW9AuNuYey9VrDkk       0       0       0       0       0   \n",
       "21      RpRVwuyV3eG5qo3LudZwNk       0       0       0       0       0   \n",
       "22      xfUANe5qKkNHpDzgBkjtki       0       0       0       0       0   \n",
       "23      Xa6swkgZirn3xJ7dfxkPxc       0       0       0       1       0   \n",
       "24      v8ToYS27VqbMAEwjXSs3Eg       0       0       0       0       0   \n",
       "25      D9xHwctWqXg2cWtL5do8WM       1       0       1       0       0   \n",
       "26      HJw8j6CLqATfmUcfSGCj9m       0       0       0       0       0   \n",
       "27      qwT85z8GdnRfhE7qxMLcbe       0       0       0       0       0   \n",
       "28      KBKcEhnC4AKZk5MKbC55xf       0       0       0       0       0   \n",
       "29      R4CBpqHCVbGufzHwq3Mpxj       0       1       0       0       0   \n",
       "30      ftz5AVKzxe2bnqb6QxFtHA       0       0       0       0       0   \n",
       "31      SwrNFka4nj9QnGhHQuPtZe       0       0       0       0       0   \n",
       "32      p5Q4FMMRzdfN5PnbXvLYPG       0       0       0       0       0   \n",
       "33      52yLTccEAEWmaKzumWxrEZ       1       0       0       1       0   \n",
       "34      adzeEFtCXno47skKoGELRL       1       0       0       0       0   \n",
       "35      DGHgXrMFi4uyhGx2C6LES7       0       0       0       0       0   \n",
       "36      aa7Lick7NBCEJSXRHMGZ8a       0       0       0       0       0   \n",
       "37      wtPXmSPAFD2Ha4aj5w37ET       0       0       0       0       0   \n",
       "38      pFEEnuyh54DfgB2uvnsfGm       0       0       0       0       1   \n",
       "39      mrJF7AkeTvCWcnYJ7mAxni       0       0       0       0       0   \n",
       "40      RrXj94gsfmN6LgavYimqoe       0       0       0       0       0   \n",
       "41      TsRJJXmvg732DhDt8MVEki       0       0       0       0       0   \n",
       "42      XZsFA273csK7yUndLcpWqg       0       0       0       0       0   \n",
       "43      JE8t9HUBCBuUL2VMGQfb5Q       0       0       0       0       0   \n",
       "44      ZWcwhniND3ieECyM28rdVB       0       0       0       0       0   \n",
       "45      wmKXgFyCNevTdErXfxNwBB       1       0       0       1       0   \n",
       "46      QMiYGb9aS6JtLojt6jBdCe       0       0       0       0       0   \n",
       "47      sM8Xnf5TG7Hd8VZemqqP5H       0       0       0       0       1   \n",
       "48      N2PnHCwkmyrHMA2njexPFD       0       0       0       0       0   \n",
       "49      mgiLGai8RCgujnodCbrDuF       0       0       0       0       0   \n",
       "...                        ...     ...     ...     ...     ...     ...   \n",
       "128326  P6d5W5vCLmg3BJ3RUPxgvF       0       0       0       0       0   \n",
       "128327  KZVXqUevzF58MnwrsUAm54       0       0       0       0       1   \n",
       "128328  CgGnPbozUGmdHjvZGVrbWK       0       0       0       0       0   \n",
       "128329  NdBUNcA6BJAB7qncqeiZQD       0       0       0       0       0   \n",
       "128330  m9THdhH5nDrAym2WEmZeN6       0       0       0       0       0   \n",
       "128331  xEPpK5Fp3p5TpfznUFVLoM       1       0       0       0       0   \n",
       "128332  3XkgPU82pKDq4ytu5RyZY7       0       0       0       0       0   \n",
       "128333  iKmzej9arWmuC6pHukcCJS       0       0       0       0       0   \n",
       "128334  io83TZYwKeTguzvHQjhYr4       0       0       0       0       0   \n",
       "128335  KVkYXdAdtjnL6uBwAfuJZA       0       0       0       0       1   \n",
       "128336  9dYFLGUKiVaagfXKjttyUb       0       0       1       0       0   \n",
       "128337  EmAbc5unejku7YYbVUkRsn       0       0       0       0       0   \n",
       "128338  T8Arwp9ytT9sArwF7cg6EK       0       0       0       0       0   \n",
       "128339  NsN2MRztpx37rMA7RyKTPN       0       0       0       0       0   \n",
       "128340  ACayZrwYDxVL2JrKsXCPgY       0       0       0       0       0   \n",
       "128341  UnyX7S2X6swjoW4o3ddCrb       0       0       0       1       0   \n",
       "128342  MFrwU5pNtgv5AaSQeLFFGd       0       0       0       0       0   \n",
       "128343  BTJx6unCs8QrbmLuoKuJ8j       0       0       0       0       0   \n",
       "128344  zeJYa9K5s4CoNeRMnBLPkf       0       0       0       0       0   \n",
       "128345  7FwAaHzrdjmewgMbDJ2oxJ       0       0       0       1       0   \n",
       "128346  nLm7crXd2j92p5PsgUEvTN       0       0       0       0       0   \n",
       "128347  EwSAZfHevhAqVSoz75yfCJ       0       0       0       0       0   \n",
       "128348  j4FrkAGD6oYjkzuWh7FQ6P       0       0       0       0       0   \n",
       "128349  t3muYok7ZTcNZSMQbeUcMT       0       0       0       0       0   \n",
       "128350  T6NdUqAs9ChEcUejG5ruwG       0       0       0       0       0   \n",
       "128351  L4kSviAtCyZHX42b7a6Sbb       0       0       0       0       0   \n",
       "128352  jP4Qph2kPCvaVhLQx6vr4M       0       0       0       0       0   \n",
       "128353  TejKW9EsYSBVTdb2vovWEW       0       0       0       0       0   \n",
       "128354  wMUCjtf3qvLc5u5jd9GTX4       0       0       0       0       0   \n",
       "128355  HSxTJNAzekBTnYFk5WQVZn       0       0       0       0       0   \n",
       "128356  BbpVKPSJHSuw9DUoJn7MUY       0       0       0       0       1   \n",
       "128357  45wA8Rf6QFhMwKB74SFuEa       0       0       0       0       0   \n",
       "128358  weyoiViCoVRKuAujoCUag6       1       0       0       0       0   \n",
       "128359  TW9rxBbrwfLmjAJeMkmKU8       0       0       0       0       0   \n",
       "128360  BPoAPggA9uEmyDLQcwEdAK       0       0       0       0       0   \n",
       "128361  VrhCzPMS8aeyp6RXWxJhHk       0       0       0       1       0   \n",
       "128362  4nh2uQmYmFnedqCEZyXpBc       0       0       0       0       0   \n",
       "128363  fbS5FS9c2YeuJ2YYRyqysn       0       0       0       0       0   \n",
       "128364  WLExJDxwG93x6kwpRigAwV       0       0       0       0       0   \n",
       "128365  hHE7iYk973MPjntF8U89g2       0       0       0       0       0   \n",
       "128366  gfB3DfMhtUBn3KPiLMuevR       0       0       0       0       0   \n",
       "128367  BAMyNuKrFVB9hfbR7bSwmH       0       0       0       0       0   \n",
       "128368  gbFpWzcp2hbG7a4YiR4XGN       0       0       0       0       0   \n",
       "128369  g4yifhFA4cW7ztsXFTqfo7       0       0       0       0       0   \n",
       "128370  HMtbm8GKkxsDSpzPLQeYkF       0       0       0       0       0   \n",
       "128371  aJpMhZgCP3VWxjTkgCTxGK       0       0       0       0       0   \n",
       "128372  KLqw7L7yeZdVjL63xpqNkS       0       0       0       0       0   \n",
       "128373  6GWrUUS54k6QVMBMT26LzF       0       0       0       0       0   \n",
       "128374  LA3aFfzrdi6r2B6y9u2BkF       0       0       0       0       0   \n",
       "128375  nN22TUE4fzGWE2i5Y4r6Qe       0       0       0       0       0   \n",
       "\n",
       "        140313  140316  140317  140321  140501  140505  140641  140691  \\\n",
       "0            0       0       0       0       0       0       0       0   \n",
       "1            0       0       0       0       0       0       0       0   \n",
       "2            0       0       0       0       0       0       0       0   \n",
       "3            0       0       0       0       0       0       0       0   \n",
       "4            0       0       0       0       0       1       0       0   \n",
       "5            0       0       0       0       0       0       0       0   \n",
       "6            0       0       0       0       0       0       0       0   \n",
       "7            0       1       0       0       0       0       0       0   \n",
       "8            0       0       0       0       0       0       0       0   \n",
       "9            0       0       0       0       0       0       0       0   \n",
       "10           0       0       0       0       0       0       0       0   \n",
       "11           0       0       0       0       0       0       1       0   \n",
       "12           0       0       0       0       0       0       0       0   \n",
       "13           0       0       0       0       0       0       0       0   \n",
       "14           0       0       0       0       1       1       0       0   \n",
       "15           0       0       0       0       0       1       0       0   \n",
       "16           0       0       0       0       0       0       0       0   \n",
       "17           0       0       0       0       0       0       0       0   \n",
       "18           1       0       0       0       0       0       0       0   \n",
       "19           0       0       0       0       0       0       0       0   \n",
       "20           0       0       0       0       0       0       0       0   \n",
       "21           0       0       0       0       0       0       0       0   \n",
       "22           1       0       0       0       0       0       0       0   \n",
       "23           0       0       0       0       0       0       0       0   \n",
       "24           1       0       0       0       0       0       0       0   \n",
       "25           0       0       0       0       0       0       0       0   \n",
       "26           0       0       0       0       0       0       0       0   \n",
       "27           1       0       0       0       0       0       0       0   \n",
       "28           0       0       0       0       0       1       0       0   \n",
       "29           0       0       0       0       0       0       0       0   \n",
       "30           0       0       0       0       0       0       0       0   \n",
       "31           0       0       0       0       0       0       0       0   \n",
       "32           0       0       0       0       0       0       0       0   \n",
       "33           0       0       0       0       0       0       0       0   \n",
       "34           0       0       0       0       0       0       1       0   \n",
       "35           0       0       0       0       0       0       0       0   \n",
       "36           0       0       0       0       0       0       0       0   \n",
       "37           0       0       0       0       0       0       0       0   \n",
       "38           0       0       0       0       1       0       0       0   \n",
       "39           0       0       0       0       0       0       0       0   \n",
       "40           0       0       0       0       0       0       0       0   \n",
       "41           1       0       0       0       0       0       0       0   \n",
       "42           0       0       0       0       0       0       0       0   \n",
       "43           0       0       0       0       0       0       0       0   \n",
       "44           0       0       0       0       0       0       0       0   \n",
       "45           0       0       0       0       0       0       0       0   \n",
       "46           0       0       0       0       0       0       0       0   \n",
       "47           0       0       0       0       0       0       0       0   \n",
       "48           0       0       0       0       0       0       0       0   \n",
       "49           0       0       0       0       0       0       0       0   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "128326       0       0       0       0       0       0       0       0   \n",
       "128327       0       1       0       0       0       0       0       0   \n",
       "128328       0       0       0       0       0       0       0       0   \n",
       "128329       0       0       0       0       0       0       0       0   \n",
       "128330       0       0       0       0       0       0       0       0   \n",
       "128331       0       0       0       0       0       0       0       0   \n",
       "128332       0       0       0       0       0       0       0       0   \n",
       "128333       0       0       0       0       0       0       0       0   \n",
       "128334       0       0       0       0       0       0       0       0   \n",
       "128335       0       0       0       0       0       0       0       0   \n",
       "128336       0       0       0       0       0       0       0       0   \n",
       "128337       0       0       0       0       0       0       0       0   \n",
       "128338       0       0       0       0       0       0       0       0   \n",
       "128339       0       0       0       0       0       0       0       0   \n",
       "128340       0       0       0       0       0       0       0       0   \n",
       "128341       0       0       0       0       0       0       0       0   \n",
       "128342       1       0       0       0       0       0       0       0   \n",
       "128343       0       0       0       0       0       0       0       0   \n",
       "128344       0       0       0       0       0       0       0       0   \n",
       "128345       0       0       0       0       0       0       0       0   \n",
       "128346       0       0       0       0       0       0       0       1   \n",
       "128347       0       0       0       0       0       0       0       0   \n",
       "128348       0       0       0       0       0       0       0       0   \n",
       "128349       1       0       0       0       0       0       0       0   \n",
       "128350       0       0       0       0       0       0       0       0   \n",
       "128351       0       0       0       0       0       0       0       0   \n",
       "128352       0       0       0       0       0       0       0       0   \n",
       "128353       0       0       0       0       0       0       0       0   \n",
       "128354       0       0       0       0       0       0       0       0   \n",
       "128355       0       0       0       0       0       0       0       0   \n",
       "128356       0       0       0       0       0       0       0       0   \n",
       "128357       0       0       0       0       0       1       0       0   \n",
       "128358       0       0       0       0       0       0       0       0   \n",
       "128359       0       0       0       0       0       0       0       0   \n",
       "128360       0       0       0       0       0       0       0       0   \n",
       "128361       0       0       0       0       0       0       0       0   \n",
       "128362       0       0       0       0       0       1       0       0   \n",
       "128363       0       0       0       0       0       0       0       0   \n",
       "128364       0       0       0       0       0       0       0       0   \n",
       "128365       0       0       0       0       0       0       0       0   \n",
       "128366       0       0       0       0       0       0       0       0   \n",
       "128367       0       0       0       0       0       0       0       0   \n",
       "128368       0       0       0       0       0       0       0       0   \n",
       "128369       0       0       0       0       0       0       0       0   \n",
       "128370       0       0       0       0       0       0       0       0   \n",
       "128371       1       0       0       0       0       0       0       0   \n",
       "128372       0       0       0       0       0       0       0       0   \n",
       "128373       0       0       0       0       0       0       0       0   \n",
       "128374       0       0       0       0       0       0       0       0   \n",
       "128375       0       0       0       0       0       0       0       0   \n",
       "\n",
       "        130123_pred  130125_pred  130129_pred  130131_pred  140307_pred  \\\n",
       "0          0.040674     0.005444     0.062851     0.042257     0.025029   \n",
       "1          0.085360     0.008760     0.065909     0.170129     0.029627   \n",
       "2          0.126139     0.003443     0.127633     0.063036     0.012105   \n",
       "3          0.121464     0.009970     0.275874     0.093228     0.138492   \n",
       "4          0.157340     0.009805     0.071833     0.200286     0.175853   \n",
       "5          0.057167     0.014837     0.077358     0.195276     0.024284   \n",
       "6          0.122690     0.004273     0.063108     0.262616     0.036614   \n",
       "7          0.109098     0.005392     0.087265     0.161923     0.336647   \n",
       "8          0.046405     0.005422     0.041465     0.044197     0.042708   \n",
       "9          0.365136     0.005485     0.081919     0.094931     0.019410   \n",
       "10         0.079568     0.010245     0.059002     0.180175     0.081485   \n",
       "11         0.308208     0.017372     0.154132     0.239312     0.140326   \n",
       "12         0.061159     0.003380     0.064141     0.090856     0.109145   \n",
       "13         0.044905     0.003285     0.081006     0.089117     0.035541   \n",
       "14         0.041956     0.010140     0.152122     0.161042     0.030062   \n",
       "15         0.026372     0.003431     0.029664     0.037896     0.016809   \n",
       "16         0.069002     0.004157     0.094259     0.094129     0.046128   \n",
       "17         0.096181     0.003675     0.085306     0.060159     0.040952   \n",
       "18         0.054043     0.020492     0.032625     0.052402     0.023557   \n",
       "19         0.039348     0.003307     0.062444     0.041439     0.004779   \n",
       "20         0.197946     0.019126     0.070284     0.128800     0.096562   \n",
       "21         0.063994     0.008610     0.038653     0.061898     0.013788   \n",
       "22         0.041434     0.003578     0.009218     0.077206     0.024898   \n",
       "23         0.041471     0.004064     0.017254     0.021281     0.041028   \n",
       "24         0.113406     0.003836     0.044823     0.111065     0.070741   \n",
       "25         0.054736     0.003790     0.015811     0.046607     0.015090   \n",
       "26         0.055232     0.002823     0.072642     0.228658     0.050238   \n",
       "27         0.056206     0.003565     0.020788     0.036447     0.078111   \n",
       "28         0.260726     0.009056     0.122793     0.313873     0.552543   \n",
       "29         0.102571     0.004898     0.047837     0.114320     0.012909   \n",
       "30         0.023222     0.004544     0.039000     0.048020     0.119785   \n",
       "31         0.026429     0.004613     0.005597     0.045539     0.059208   \n",
       "32         0.027181     0.003599     0.030387     0.046613     0.016479   \n",
       "33         0.072247     0.006286     0.024490     0.118829     0.036453   \n",
       "34         0.055254     0.006300     0.066181     0.137164     0.044227   \n",
       "35         0.031331     0.003603     0.015508     0.067096     0.005386   \n",
       "36         0.190444     0.007190     0.088450     0.209372     0.030015   \n",
       "37         0.040774     0.004676     0.026747     0.043536     0.053947   \n",
       "38         0.111156     0.006908     0.183072     0.363087     0.051420   \n",
       "39         0.091918     0.013110     0.031401     0.095199     0.163837   \n",
       "40         0.049100     0.005001     0.015166     0.045015     0.007653   \n",
       "41         0.187936     0.007760     0.086250     0.123461     0.086219   \n",
       "42         0.068163     0.004371     0.023785     0.088175     0.022753   \n",
       "43         0.032174     0.004642     0.135915     0.019587     0.004183   \n",
       "44         0.029859     0.002172     0.014954     0.006684     0.005441   \n",
       "45         0.089975     0.002588     0.042298     0.109962     0.025848   \n",
       "46         0.024052     0.003332     0.021901     0.036286     0.013556   \n",
       "47         0.322679     0.042881     0.280757     0.239979     0.075940   \n",
       "48         0.065344     0.003210     0.032969     0.027197     0.024965   \n",
       "49         0.107778     0.004555     0.101960     0.075626     0.029267   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "128326     0.267781     0.017042     0.072724     0.076038     0.024175   \n",
       "128327     0.075486     0.005508     0.018180     0.073437     0.019931   \n",
       "128328     0.028529     0.004189     0.015064     0.093197     0.004934   \n",
       "128329     0.153220     0.014013     0.038361     0.068846     0.052140   \n",
       "128330     0.107490     0.004546     0.019094     0.084472     0.072056   \n",
       "128331     0.078056     0.017978     0.025086     0.088439     0.041684   \n",
       "128332     0.193809     0.027033     0.088583     0.269442     0.041934   \n",
       "128333     0.835790     0.016157     0.240370     0.498923     0.049205   \n",
       "128334     0.074030     0.005758     0.031598     0.107120     0.011913   \n",
       "128335     0.107261     0.005191     0.016013     0.123970     0.027995   \n",
       "128336     0.192988     0.007796     0.089624     0.162400     0.024538   \n",
       "128337     0.621995     0.046994     0.090381     0.226377     0.232607   \n",
       "128338     0.054893     0.005205     0.041183     0.070429     0.781999   \n",
       "128339     0.115606     0.025886     0.035498     0.092977     0.066719   \n",
       "128340     0.092148     0.008666     0.114426     0.119503     0.013326   \n",
       "128341     0.222970     0.005532     0.026831     0.522513     0.090257   \n",
       "128342     0.127603     0.009098     0.087260     0.054336     0.053410   \n",
       "128343     0.062447     0.003191     0.015536     0.054428     0.022047   \n",
       "128344     0.091427     0.010324     0.040573     0.069516     0.055728   \n",
       "128345     0.129026     0.015356     0.032906     0.097279     0.156396   \n",
       "128346     0.172413     0.010792     0.054520     0.258535     0.020083   \n",
       "128347     0.087219     0.003763     0.083837     0.073022     0.024680   \n",
       "128348     0.095306     0.004823     0.040032     0.094283     0.022671   \n",
       "128349     0.040847     0.009691     0.022291     0.029264     0.047977   \n",
       "128350     0.057297     0.006852     0.093683     0.061593     0.012086   \n",
       "128351     0.106541     0.014771     0.059100     0.187949     0.034316   \n",
       "128352     0.079477     0.007634     0.046532     0.070907     0.025855   \n",
       "128353     0.139489     0.004087     0.022089     0.178122     0.060754   \n",
       "128354     0.119519     0.009463     0.059871     0.206195     0.043901   \n",
       "128355     0.061592     0.007806     0.011195     0.066337     0.012270   \n",
       "128356     0.170436     0.502818     0.081219     0.085122     0.039271   \n",
       "128357     0.063930     0.007829     0.027893     0.051476     0.011925   \n",
       "128358     0.399266     0.021684     0.106649     0.527720     0.021666   \n",
       "128359     0.064149     0.018536     0.017747     0.046414     0.007247   \n",
       "128360     0.211671     0.011123     0.398195     0.120440     0.292873   \n",
       "128361     0.080531     0.007425     0.008143     0.115370     0.032707   \n",
       "128362     0.104335     0.009343     0.063325     0.077384     0.067053   \n",
       "128363     0.052033     0.009047     0.037198     0.055075     0.006127   \n",
       "128364     0.051925     0.003843     0.027733     0.150894     0.018294   \n",
       "128365     0.043076     0.006920     0.004660     0.028369     0.014906   \n",
       "128366     0.127554     0.022465     0.035363     0.165323     0.042124   \n",
       "128367     0.269526     0.003393     0.071512     0.267386     0.017118   \n",
       "128368     0.182727     0.021578     0.075764     0.283841     0.103625   \n",
       "128369     0.063185     0.005254     0.034815     0.078383     0.020063   \n",
       "128370     0.073455     0.003666     0.156536     0.094165     0.114267   \n",
       "128371     0.099073     0.017699     0.026763     0.113751     0.049408   \n",
       "128372     0.036741     0.005896     0.011296     0.040602     0.019387   \n",
       "128373     0.061404     0.005674     0.013801     0.069937     0.054073   \n",
       "128374     0.032424     0.008682     0.005722     0.023168     0.012079   \n",
       "128375     0.036759     0.001487     0.012942     0.067150     0.015591   \n",
       "\n",
       "        140313_pred  140316_pred  140317_pred  140321_pred  140501_pred  \\\n",
       "0          0.008335     0.001427     0.003847     0.007631     0.003609   \n",
       "1          0.016038     0.015314     0.002974     0.000512     0.005411   \n",
       "2          0.027708     0.006339     0.025767     0.000768     0.012390   \n",
       "3          0.014284     0.002960     0.003669     0.003650     0.031252   \n",
       "4          0.016206     0.009456     0.003538     0.003391     0.015241   \n",
       "5          0.017331     0.008168     0.008273     0.001174     0.216117   \n",
       "6          0.061340     0.008512     0.003397     0.002626     0.195561   \n",
       "7          0.009870     0.009443     0.002951     0.003167     0.006566   \n",
       "8          0.027321     0.002224     0.011864     0.002540     0.063945   \n",
       "9          0.006162     0.013284     0.005044     0.005173     0.008816   \n",
       "10         0.008925     0.028116     0.002691     0.002024     0.005015   \n",
       "11         0.024198     0.008849     0.003390     0.001274     0.004285   \n",
       "12         0.013229     0.035859     0.010134     0.008642     0.014663   \n",
       "13         0.011062     0.004148     0.002551     0.003709     0.064506   \n",
       "14         0.028765     0.014119     0.002848     0.004580     0.003921   \n",
       "15         0.014251     0.047587     0.010461     0.001626     0.040108   \n",
       "16         0.014256     0.009693     0.003634     0.009968     0.021593   \n",
       "17         0.049391     0.002218     0.002937     0.001397     0.127838   \n",
       "18         0.263156     0.010464     0.008865     0.000996     0.006194   \n",
       "19         0.002003     0.002108     0.004543     0.003037     0.011164   \n",
       "20         0.076668     0.025669     0.005205     0.012895     0.062480   \n",
       "21         0.007122     0.005710     0.006428     0.000873     0.004005   \n",
       "22         0.012581     0.002153     0.004210     0.002053     0.038662   \n",
       "23         0.017986     0.002931     0.002714     0.002590     0.043102   \n",
       "24         0.006740     0.003625     0.003393     0.000943     0.052875   \n",
       "25         0.008916     0.004818     0.007261     0.001171     0.045782   \n",
       "26         0.019539     0.006968     0.006368     0.004326     0.004126   \n",
       "27         0.023618     0.012167     0.317557     0.012159     0.009022   \n",
       "28         0.019892     0.003417     0.002560     0.000605     0.025298   \n",
       "29         0.018308     0.008207     0.006006     0.005795     0.070586   \n",
       "30         0.002223     0.001755     0.000796     0.001863     0.018019   \n",
       "31         0.043634     0.016709     0.004859     0.002401     0.009759   \n",
       "32         0.012976     0.003494     0.004344     0.004949     0.005861   \n",
       "33         0.014373     0.006193     0.005033     0.046279     0.024379   \n",
       "34         0.042687     0.002354     0.004038     0.001126     0.044801   \n",
       "35         0.009784     0.002830     0.026653     0.005664     0.006136   \n",
       "36         0.013437     0.004741     0.008570     0.000394     0.012382   \n",
       "37         0.016765     0.002736     0.003583     0.000339     0.009949   \n",
       "38         0.022599     0.010555     0.007944     0.003587     0.009444   \n",
       "39         0.336636     0.032787     0.007101     0.003587     0.017308   \n",
       "40         0.015153     0.005952     0.002118     0.000260     0.566003   \n",
       "41         0.056654     0.013818     0.012704     0.004444     0.089852   \n",
       "42         0.023855     0.012961     0.000860     0.011653     0.003735   \n",
       "43         0.022837     0.013867     0.013172     0.001618     0.005433   \n",
       "44         0.005224     0.001685     0.016356     0.000887     0.002012   \n",
       "45         0.033754     0.013493     0.005747     0.003806     0.018907   \n",
       "46         0.016090     0.007078     0.011252     0.002217     0.006870   \n",
       "47         0.086332     0.035925     0.034911     0.011817     0.012699   \n",
       "48         0.192749     0.053048     0.031554     0.001446     0.007195   \n",
       "49         0.015061     0.002178     0.003781     0.003298     0.001757   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "128326     0.042266     0.023507     0.009046     0.002370     0.013250   \n",
       "128327     0.013194     0.004751     0.014193     0.002542     0.006201   \n",
       "128328     0.010700     0.002127     0.008075     0.005335     0.017986   \n",
       "128329     0.015265     0.013450     0.003506     0.003949     0.006226   \n",
       "128330     0.032801     0.001379     0.003560     0.001151     0.040455   \n",
       "128331     0.031097     0.011817     0.005058     0.002957     0.011135   \n",
       "128332     0.010330     0.009410     0.009130     0.007588     0.003702   \n",
       "128333     0.047217     0.015681     0.078516     0.022315     0.016175   \n",
       "128334     0.011441     0.007435     0.008806     0.000668     0.017181   \n",
       "128335     0.047113     0.008165     0.002825     0.000300     0.002797   \n",
       "128336     0.008571     0.007199     0.003463     0.000390     0.001486   \n",
       "128337     0.016923     0.017930     0.009487     0.005650     0.047028   \n",
       "128338     0.059884     0.012834     0.004484     0.002461     0.005687   \n",
       "128339     0.031171     0.007437     0.007131     0.002607     0.008710   \n",
       "128340     0.008356     0.005703     0.001998     0.000668     0.006330   \n",
       "128341     0.014196     0.027426     0.016989     0.007868     0.015594   \n",
       "128342     0.038022     0.026066     0.003306     0.002917     0.014193   \n",
       "128343     0.144439     0.048020     0.004981     0.002572     0.006551   \n",
       "128344     0.019834     0.015418     0.006185     0.002227     0.007881   \n",
       "128345     0.010615     0.007425     0.029611     0.001103     0.012741   \n",
       "128346     0.037071     0.029325     0.017998     0.000396     0.005247   \n",
       "128347     0.021607     0.028533     0.011406     0.000687     0.003897   \n",
       "128348     0.017450     0.003911     0.011100     0.002410     0.024263   \n",
       "128349     0.012241     0.014049     0.006036     0.002118     0.023977   \n",
       "128350     0.007344     0.003918     0.002263     0.000383     0.001661   \n",
       "128351     0.026337     0.006781     0.008671     0.004505     0.041527   \n",
       "128352     0.008287     0.001323     0.007195     0.000902     0.009653   \n",
       "128353     0.034341     0.001630     0.009876     0.000446     0.017236   \n",
       "128354     0.070081     0.172358     0.073191     0.003032     0.265854   \n",
       "128355     0.016806     0.009131     0.004411     0.000523     0.034677   \n",
       "128356     0.012552     0.002950     0.092996     0.000944     0.011225   \n",
       "128357     0.011157     0.008399     0.022240     0.001347     0.007407   \n",
       "128358     0.085019     0.203758     0.003827     0.003326     0.017651   \n",
       "128359     0.012906     0.132508     0.005498     0.000885     0.005977   \n",
       "128360     0.027500     0.011412     0.007406     0.001955     0.006855   \n",
       "128361     0.018526     0.004843     0.006787     0.000561     0.008945   \n",
       "128362     0.036992     0.003465     0.010110     0.003145     0.033257   \n",
       "128363     0.010308     0.006062     0.003965     0.001248     0.074087   \n",
       "128364     0.021442     0.004371     0.001278     0.002522     0.236244   \n",
       "128365     0.006632     0.003697     0.002763     0.000255     0.309209   \n",
       "128366     0.016609     0.011228     0.004075     0.002459     0.006344   \n",
       "128367     0.053507     0.009688     0.011492     0.000978     0.004941   \n",
       "128368     0.013850     0.006530     0.005357     0.003418     0.003115   \n",
       "128369     0.023001     0.004359     0.030358     0.000634     0.010388   \n",
       "128370     0.016447     0.008172     0.006210     0.000265     0.005152   \n",
       "128371     0.017179     0.014450     0.005738     0.001125     0.007296   \n",
       "128372     0.010083     0.012278     0.009474     0.002726     0.036399   \n",
       "128373     0.013075     0.087104     0.001836     0.000716     0.002179   \n",
       "128374     0.005588     0.001359     0.004713     0.001427     0.007751   \n",
       "128375     0.010608     0.004957     0.001988     0.000334     0.018958   \n",
       "\n",
       "        140505_pred  140641_pred  140691_pred  \n",
       "0          0.028405     0.017969     0.007592  \n",
       "1          0.023297     0.022189     0.005705  \n",
       "2          0.022045     0.049357     0.001946  \n",
       "3          0.027514     0.097828     0.006208  \n",
       "4          0.034619     0.079691     0.021228  \n",
       "5          0.078690     0.043991     0.005783  \n",
       "6          0.034565     0.016314     0.005006  \n",
       "7          0.024890     0.052241     0.011596  \n",
       "8          0.061068     0.070388     0.024996  \n",
       "9          0.030763     0.070182     0.005038  \n",
       "10         0.011217     0.071563     0.004105  \n",
       "11         0.041497     0.047794     0.003467  \n",
       "12         0.044722     0.017288     0.027841  \n",
       "13         0.489203     0.075807     0.133046  \n",
       "14         0.809229     0.013230     0.005571  \n",
       "15         0.188952     0.019304     0.020615  \n",
       "16         0.150735     0.085195     0.029019  \n",
       "17         0.061707     0.133956     0.002775  \n",
       "18         0.397490     0.010218     0.001227  \n",
       "19         0.795809     0.037015     0.093604  \n",
       "20         0.112766     0.082681     0.065014  \n",
       "21         0.055832     0.024607     0.003134  \n",
       "22         0.054431     0.028530     0.003739  \n",
       "23         0.025420     0.019314     0.012347  \n",
       "24         0.061712     0.014541     0.005159  \n",
       "25         0.008248     0.017139     0.001226  \n",
       "26         0.106131     0.068459     0.039691  \n",
       "27         0.060651     0.076743     0.003289  \n",
       "28         0.080024     0.021360     0.001155  \n",
       "29         0.116605     0.110112     0.041476  \n",
       "30         0.364247     0.189599     0.003772  \n",
       "31         0.016282     0.056762     0.010654  \n",
       "32         0.011556     0.077024     0.005233  \n",
       "33         0.115936     0.049517     0.020649  \n",
       "34         0.040590     0.123403     0.006302  \n",
       "35         0.004114     0.018717     0.013452  \n",
       "36         0.075707     0.040142     0.006768  \n",
       "37         0.246843     0.047128     0.004159  \n",
       "38         0.037673     0.030273     0.000764  \n",
       "39         0.025577     0.028841     0.001673  \n",
       "40         0.222384     0.198386     0.004211  \n",
       "41         0.495824     0.785635     0.019734  \n",
       "42         0.197490     0.041567     0.004652  \n",
       "43         0.006701     0.076923     0.002790  \n",
       "44         0.004485     0.008138     0.000403  \n",
       "45         0.036523     0.040902     0.013881  \n",
       "46         0.017341     0.068015     0.005844  \n",
       "47         0.037881     0.055862     0.019190  \n",
       "48         0.019688     0.102341     0.005389  \n",
       "49         0.011503     0.050490     0.006883  \n",
       "...             ...          ...          ...  \n",
       "128326     0.045632     0.018517     0.003951  \n",
       "128327     0.136952     0.009480     0.004264  \n",
       "128328     0.063680     0.040284     0.001575  \n",
       "128329     0.017313     0.004176     0.000554  \n",
       "128330     0.042849     0.003271     0.003148  \n",
       "128331     0.034383     0.151646     0.006971  \n",
       "128332     0.012095     0.125095     0.000908  \n",
       "128333     0.019846     0.018238     0.212806  \n",
       "128334     0.013798     0.004122     0.000612  \n",
       "128335     0.014164     0.003478     0.001470  \n",
       "128336     0.018814     0.007110     0.004286  \n",
       "128337     0.068479     0.027127     0.002081  \n",
       "128338     0.022354     0.096438     0.005313  \n",
       "128339     0.270150     0.026005     0.007594  \n",
       "128340     0.022771     0.015206     0.002057  \n",
       "128341     0.037794     0.039281     0.008842  \n",
       "128342     0.015828     0.018519     0.005856  \n",
       "128343     0.027417     0.003957     0.004000  \n",
       "128344     0.024625     0.025670     0.014949  \n",
       "128345     0.038897     0.025051     0.005747  \n",
       "128346     0.026685     0.053445     0.005168  \n",
       "128347     0.009908     0.008006     0.000457  \n",
       "128348     0.066381     0.016690     0.007380  \n",
       "128349     0.025026     0.005836     0.003102  \n",
       "128350     0.005299     0.000727     0.001805  \n",
       "128351     0.141172     0.016927     0.044290  \n",
       "128352     0.031119     0.007259     0.011941  \n",
       "128353     0.055050     0.004422     0.009926  \n",
       "128354     0.168033     0.027313     0.010541  \n",
       "128355     0.019522     0.009188     0.032989  \n",
       "128356     0.027091     0.002925     0.000445  \n",
       "128357     0.016679     0.007031     0.004273  \n",
       "128358     0.097779     0.006926     0.008880  \n",
       "128359     0.009973     0.007605     0.006052  \n",
       "128360     0.047359     0.207585     0.004438  \n",
       "128361     0.009430     0.006129     0.003425  \n",
       "128362     0.055862     0.014646     0.005711  \n",
       "128363     0.058170     0.039460     0.003872  \n",
       "128364     0.016795     0.008806     0.001343  \n",
       "128365     0.009066     0.003656     0.001869  \n",
       "128366     0.043668     0.003579     0.004364  \n",
       "128367     0.016961     0.003932     0.001891  \n",
       "128368     0.022997     0.007747     0.002852  \n",
       "128369     0.028654     0.003953     0.001414  \n",
       "128370     0.043201     0.009588     0.005507  \n",
       "128371     0.006019     0.002697     0.000282  \n",
       "128372     0.015552     0.009212     0.003041  \n",
       "128373     0.003252     0.000837     0.001653  \n",
       "128374     0.033291     0.014821     0.005410  \n",
       "128375     0.012048     0.019002     0.001803  \n",
       "\n",
       "[128376 rows x 27 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_plus_oof_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3rcdjjRyw9qSh6NcZMKSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y56uzwqQzynHYZ4bDfLPp5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xDMdFERmC7CD9yFvyvKJnh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzZENdjz7SvUQkGZV45afF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zFWkhHbLYJ9Fh5kUvCrx4g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Z9668Qr6T63NGv2vshFZ22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iibUNHJdKdwir3YoN23fYB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mRUYb9nmdychPJVDTmTuEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Svsd4RkZcFTcnnhHTrtto4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2EvrkYaUPmeQxnk6RiVQb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5tZuLssiSvTz6parm5st3Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HxC7Jz66uEfzDp3FmmJfkY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UhScGN7ZDDutGeT5T4EsVo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FY4jDHwCJNoBnGHVUeGgy2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PmaB926DFLPNkGKjQJVieJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>iXoHx4Afpa6KwFVzeRoTwP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>xMwVHN4T8LCDYoeN2by55A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>qo5QysbFHDfJReD5VrC3NQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HovNFU3nwArY28rUvBd3SJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TyUa69smDm5AVp2yWnzyUJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8xwyY6xyqtyLmowe475MW6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M6ouG87EQRkw9hFJoSMdRV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>XjegnSPJLErpCkVUiqJxSU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>XKy5emoTL2TNFvDDtBzKjj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Z2Eksv2hZkn2n9KANkBYtX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>w6S4dFJWKLNazv5rMjY5f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MZagg4orrhSw4jhazzqtLQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hGA9WhRvr8SUqSLE3pq27c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>iZu7zUExZtxuPawmKVxvB6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gfPaZCYs39PzLEBWqAvBtf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>nEGeLzfBNL2pzCSrJncSjA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ZJDrCt4Xz6EUEoSB5a9iJB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Nnj7tfbSJra8yPBFu6TDi5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>KhXWitDjjAFkop36jz9sAd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>J2hWRtYBCat69QF6pn54hR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>wwBo844JEkiwK2MDBKJvnD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>V4yVh9axGqpxioM33KL5vV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>KXSXNxMbAZDJYb8PuePY2h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>WyeD4MgTQr9Jw3BLN8Kpfc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>985Cz7UZBvH8MmuC4VEx8S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>raeKhHCqEs5t4pRouomkcT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>FHYjLx7xtjFWHbhqBAMyqD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>V8CTXLWUFUKZv5PSGXBKqJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6BCFWRhSTezTuvZHa2CmDV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>96yYKks6EdyM4T7qDPFXEX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2QpRAe5oKeX4ChkNpv2MvB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>qdfoF36hUUeg9DZX9UbAeA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ew9UHoRPy6B76qgk4uKbe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RjBBBvToqCw8VVHMspweWY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>msddw6HzRYiWtrLXvzrrA7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96455</th>\n",
       "      <td>ygxW6s2AC3mScTZXoiZKic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96456</th>\n",
       "      <td>jCoT9mfdjf5X3VKeqc3erG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96457</th>\n",
       "      <td>ugqMGK2FtSSzhy4J539M7j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96458</th>\n",
       "      <td>6fWbyQ4cCfbkg4APanPk4V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96459</th>\n",
       "      <td>yaSityeYfqPHpi82sM7hFB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96460</th>\n",
       "      <td>ejVQUZ2V2ytdcWZXR4BWBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96461</th>\n",
       "      <td>Kz5rJMqd6CMRDiLwkmMQXd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96462</th>\n",
       "      <td>YaZ43yPa5q8efHgTFMEfRc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96463</th>\n",
       "      <td>4VfXynqT2jT5qD6kgNHJxH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96464</th>\n",
       "      <td>yLkE3GMnrRiXDSCdXThcZk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96465</th>\n",
       "      <td>R4yoM2YHpbZYk9MpwHKsYg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96466</th>\n",
       "      <td>9j5PLZQaiojdkX3Wk3UTfk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96467</th>\n",
       "      <td>B8m9eqJBN9MPwYvuyjzVPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96468</th>\n",
       "      <td>DQDkJQBaiUaKMeRx6w6qCX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96469</th>\n",
       "      <td>ThZdTheWpWZMUhRQJytRB7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96470</th>\n",
       "      <td>jkBHJPbcZbxVgVcS2wexJA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96471</th>\n",
       "      <td>J8cc5bwf3NqE2mMbwNRHCN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96472</th>\n",
       "      <td>bh2YcrMY5JoNA3UhefUD9U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96473</th>\n",
       "      <td>XDhQ3MobNJRjwij5rrBEuJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96474</th>\n",
       "      <td>iKRY2NiD9oD4WLUoD3vbq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96475</th>\n",
       "      <td>wcDZSBvuX2Y3G5KEkacBGU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96476</th>\n",
       "      <td>UDQqnguFReLfJNsKbJG94Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96477</th>\n",
       "      <td>AoAAKaxQFVdWtyCFrNNZS8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96478</th>\n",
       "      <td>vr4EKfTUrNRGNhyAvNTPJb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96479</th>\n",
       "      <td>XjyPKEeKFFmPmytCM35Nxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96480</th>\n",
       "      <td>SeyEuYcCra5dHdk7oZjU6F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96481</th>\n",
       "      <td>WHu2NhBNu3Pdcr7C8n8VN9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96482</th>\n",
       "      <td>TP8evPxviYBDkd78yVyzS6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96483</th>\n",
       "      <td>VsE8YBTWC5LZN2WcrVdTWo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96484</th>\n",
       "      <td>XKwDTAwNLG4CRBgbkrTaWH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96485</th>\n",
       "      <td>qb47GGmKMWUSgMcAaqLiyF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96486</th>\n",
       "      <td>rDAsqctcYNNd2H6KpDsLha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96487</th>\n",
       "      <td>ch9dwK78Pudohwf2NxDnFk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96488</th>\n",
       "      <td>PDEVpknCprxxZpc959D6yB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96489</th>\n",
       "      <td>qRGYb3tBXgoUE6p6BgVrdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96490</th>\n",
       "      <td>KQ4BNJnvahptZu9hS8VQJ7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96491</th>\n",
       "      <td>BBshmju8bQqtXjPcKSFyQE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96492</th>\n",
       "      <td>FHEoJBL6Na93eUHow6mMx5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96493</th>\n",
       "      <td>RUFbfqJFFVHmt6b7ajizCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96494</th>\n",
       "      <td>ZQfwhQaURx5vJBpdLJgaz8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96495</th>\n",
       "      <td>M6srBckskjzzu6TfJM9rAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96496</th>\n",
       "      <td>29aVtMnjVNJ77iLGBe3jq6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96497</th>\n",
       "      <td>7sWVKTdKQh8itKEKdV82m8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96498</th>\n",
       "      <td>z48CbjWJGHmEvUXo22GL4H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96499</th>\n",
       "      <td>8TszNXCs79zaK5X3f9Psvm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96500</th>\n",
       "      <td>PMDTVvzExc6nMHnS8y5UV8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96501</th>\n",
       "      <td>djcxSQDVYtDZrFYJLteAwS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96502</th>\n",
       "      <td>XbGdy3cKdHS2i2Shr2Tcze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96503</th>\n",
       "      <td>ZtMf5Wd6itWKuoL5hu4vjh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96504</th>\n",
       "      <td>9MRdDyT4E8H9ZznRkdMXVC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96505 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  purchase_id\n",
       "0      C3rcdjjRyw9qSh6NcZMKSX\n",
       "1      Y56uzwqQzynHYZ4bDfLPp5\n",
       "2      xDMdFERmC7CD9yFvyvKJnh\n",
       "3      zzZENdjz7SvUQkGZV45afF\n",
       "4      zFWkhHbLYJ9Fh5kUvCrx4g\n",
       "5      Z9668Qr6T63NGv2vshFZ22\n",
       "6      iibUNHJdKdwir3YoN23fYB\n",
       "7      mRUYb9nmdychPJVDTmTuEM\n",
       "8      Svsd4RkZcFTcnnhHTrtto4\n",
       "9      2EvrkYaUPmeQxnk6RiVQb7\n",
       "10     5tZuLssiSvTz6parm5st3Z\n",
       "11     HxC7Jz66uEfzDp3FmmJfkY\n",
       "12     UhScGN7ZDDutGeT5T4EsVo\n",
       "13     FY4jDHwCJNoBnGHVUeGgy2\n",
       "14     PmaB926DFLPNkGKjQJVieJ\n",
       "15     iXoHx4Afpa6KwFVzeRoTwP\n",
       "16     xMwVHN4T8LCDYoeN2by55A\n",
       "17     qo5QysbFHDfJReD5VrC3NQ\n",
       "18     HovNFU3nwArY28rUvBd3SJ\n",
       "19     TyUa69smDm5AVp2yWnzyUJ\n",
       "20     8xwyY6xyqtyLmowe475MW6\n",
       "21     M6ouG87EQRkw9hFJoSMdRV\n",
       "22     XjegnSPJLErpCkVUiqJxSU\n",
       "23     XKy5emoTL2TNFvDDtBzKjj\n",
       "24     Z2Eksv2hZkn2n9KANkBYtX\n",
       "25     w6S4dFJWKLNazv5rMjY5f9\n",
       "26     MZagg4orrhSw4jhazzqtLQ\n",
       "27     hGA9WhRvr8SUqSLE3pq27c\n",
       "28     iZu7zUExZtxuPawmKVxvB6\n",
       "29     gfPaZCYs39PzLEBWqAvBtf\n",
       "30     nEGeLzfBNL2pzCSrJncSjA\n",
       "31     ZJDrCt4Xz6EUEoSB5a9iJB\n",
       "32     Nnj7tfbSJra8yPBFu6TDi5\n",
       "33     KhXWitDjjAFkop36jz9sAd\n",
       "34     J2hWRtYBCat69QF6pn54hR\n",
       "35     wwBo844JEkiwK2MDBKJvnD\n",
       "36     V4yVh9axGqpxioM33KL5vV\n",
       "37     KXSXNxMbAZDJYb8PuePY2h\n",
       "38     WyeD4MgTQr9Jw3BLN8Kpfc\n",
       "39     985Cz7UZBvH8MmuC4VEx8S\n",
       "40     raeKhHCqEs5t4pRouomkcT\n",
       "41     FHYjLx7xtjFWHbhqBAMyqD\n",
       "42     V8CTXLWUFUKZv5PSGXBKqJ\n",
       "43     6BCFWRhSTezTuvZHa2CmDV\n",
       "44     96yYKks6EdyM4T7qDPFXEX\n",
       "45     2QpRAe5oKeX4ChkNpv2MvB\n",
       "46     qdfoF36hUUeg9DZX9UbAeA\n",
       "47     ew9UHoRPy6B76qgk4uKbe7\n",
       "48     RjBBBvToqCw8VVHMspweWY\n",
       "49     msddw6HzRYiWtrLXvzrrA7\n",
       "...                       ...\n",
       "96455  ygxW6s2AC3mScTZXoiZKic\n",
       "96456  jCoT9mfdjf5X3VKeqc3erG\n",
       "96457  ugqMGK2FtSSzhy4J539M7j\n",
       "96458  6fWbyQ4cCfbkg4APanPk4V\n",
       "96459  yaSityeYfqPHpi82sM7hFB\n",
       "96460  ejVQUZ2V2ytdcWZXR4BWBD\n",
       "96461  Kz5rJMqd6CMRDiLwkmMQXd\n",
       "96462  YaZ43yPa5q8efHgTFMEfRc\n",
       "96463  4VfXynqT2jT5qD6kgNHJxH\n",
       "96464  yLkE3GMnrRiXDSCdXThcZk\n",
       "96465  R4yoM2YHpbZYk9MpwHKsYg\n",
       "96466  9j5PLZQaiojdkX3Wk3UTfk\n",
       "96467  B8m9eqJBN9MPwYvuyjzVPY\n",
       "96468  DQDkJQBaiUaKMeRx6w6qCX\n",
       "96469  ThZdTheWpWZMUhRQJytRB7\n",
       "96470  jkBHJPbcZbxVgVcS2wexJA\n",
       "96471  J8cc5bwf3NqE2mMbwNRHCN\n",
       "96472  bh2YcrMY5JoNA3UhefUD9U\n",
       "96473  XDhQ3MobNJRjwij5rrBEuJ\n",
       "96474  iKRY2NiD9oD4WLUoD3vbq2\n",
       "96475  wcDZSBvuX2Y3G5KEkacBGU\n",
       "96476  UDQqnguFReLfJNsKbJG94Z\n",
       "96477  AoAAKaxQFVdWtyCFrNNZS8\n",
       "96478  vr4EKfTUrNRGNhyAvNTPJb\n",
       "96479  XjyPKEeKFFmPmytCM35Nxi\n",
       "96480  SeyEuYcCra5dHdk7oZjU6F\n",
       "96481  WHu2NhBNu3Pdcr7C8n8VN9\n",
       "96482  TP8evPxviYBDkd78yVyzS6\n",
       "96483  VsE8YBTWC5LZN2WcrVdTWo\n",
       "96484  XKwDTAwNLG4CRBgbkrTaWH\n",
       "96485  qb47GGmKMWUSgMcAaqLiyF\n",
       "96486  rDAsqctcYNNd2H6KpDsLha\n",
       "96487  ch9dwK78Pudohwf2NxDnFk\n",
       "96488  PDEVpknCprxxZpc959D6yB\n",
       "96489  qRGYb3tBXgoUE6p6BgVrdc\n",
       "96490  KQ4BNJnvahptZu9hS8VQJ7\n",
       "96491  BBshmju8bQqtXjPcKSFyQE\n",
       "96492  FHEoJBL6Na93eUHow6mMx5\n",
       "96493  RUFbfqJFFVHmt6b7ajizCT\n",
       "96494  ZQfwhQaURx5vJBpdLJgaz8\n",
       "96495  M6srBckskjzzu6TfJM9rAX\n",
       "96496  29aVtMnjVNJ77iLGBe3jq6\n",
       "96497  7sWVKTdKQh8itKEKdV82m8\n",
       "96498  z48CbjWJGHmEvUXo22GL4H\n",
       "96499  8TszNXCs79zaK5X3f9Psvm\n",
       "96500  PMDTVvzExc6nMHnS8y5UV8\n",
       "96501  djcxSQDVYtDZrFYJLteAwS\n",
       "96502  XbGdy3cKdHS2i2Shr2Tcze\n",
       "96503  ZtMf5Wd6itWKuoL5hu4vjh\n",
       "96504  9MRdDyT4E8H9ZznRkdMXVC\n",
       "\n",
       "[96505 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.read_csv(f\"{SAME_PATH}\"+f\"Users/td017/kaggle-pipeline/submission/sub_{CASE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>130123</th>\n",
       "      <th>130125</th>\n",
       "      <th>130129</th>\n",
       "      <th>130131</th>\n",
       "      <th>140307</th>\n",
       "      <th>140313</th>\n",
       "      <th>140316</th>\n",
       "      <th>140317</th>\n",
       "      <th>140321</th>\n",
       "      <th>140501</th>\n",
       "      <th>140505</th>\n",
       "      <th>140641</th>\n",
       "      <th>140691</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040232</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>0.067452</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.018505</td>\n",
       "      <td>0.030068</td>\n",
       "      <td>0.025426</td>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.076402</td>\n",
       "      <td>0.013645</td>\n",
       "      <td>0.045617</td>\n",
       "      <td>0.084973</td>\n",
       "      <td>0.040603</td>\n",
       "      <td>0.011788</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.254897</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.037649</td>\n",
       "      <td>0.003275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.241824</td>\n",
       "      <td>0.030543</td>\n",
       "      <td>0.132799</td>\n",
       "      <td>0.228855</td>\n",
       "      <td>0.078287</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.031096</td>\n",
       "      <td>0.022021</td>\n",
       "      <td>0.004850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.109612</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.294879</td>\n",
       "      <td>0.040440</td>\n",
       "      <td>0.044881</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.030360</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.006798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.148123</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>0.177052</td>\n",
       "      <td>0.119795</td>\n",
       "      <td>0.041511</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.061648</td>\n",
       "      <td>0.048735</td>\n",
       "      <td>0.036030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.110663</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.080869</td>\n",
       "      <td>0.116072</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.029676</td>\n",
       "      <td>0.095509</td>\n",
       "      <td>0.040977</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>0.005558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.157866</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.074565</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.192951</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>0.054553</td>\n",
       "      <td>0.019914</td>\n",
       "      <td>0.006475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.137579</td>\n",
       "      <td>0.035408</td>\n",
       "      <td>0.085052</td>\n",
       "      <td>0.300336</td>\n",
       "      <td>0.137301</td>\n",
       "      <td>0.021337</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.007803</td>\n",
       "      <td>0.020294</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.060856</td>\n",
       "      <td>0.035409</td>\n",
       "      <td>0.009408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.109786</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.089285</td>\n",
       "      <td>0.179042</td>\n",
       "      <td>0.183370</td>\n",
       "      <td>0.035798</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.021637</td>\n",
       "      <td>0.172807</td>\n",
       "      <td>0.124942</td>\n",
       "      <td>0.056038</td>\n",
       "      <td>0.004552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.107374</td>\n",
       "      <td>0.025556</td>\n",
       "      <td>0.051660</td>\n",
       "      <td>0.219676</td>\n",
       "      <td>0.158542</td>\n",
       "      <td>0.139714</td>\n",
       "      <td>0.013758</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.003825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.101286</td>\n",
       "      <td>0.039677</td>\n",
       "      <td>0.022023</td>\n",
       "      <td>0.160185</td>\n",
       "      <td>0.036040</td>\n",
       "      <td>0.033209</td>\n",
       "      <td>0.010502</td>\n",
       "      <td>0.006677</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.088070</td>\n",
       "      <td>0.097055</td>\n",
       "      <td>0.030676</td>\n",
       "      <td>0.014967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.109366</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>0.073806</td>\n",
       "      <td>0.163040</td>\n",
       "      <td>0.032267</td>\n",
       "      <td>0.039163</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.067530</td>\n",
       "      <td>0.077171</td>\n",
       "      <td>0.013175</td>\n",
       "      <td>0.014167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.183691</td>\n",
       "      <td>0.012937</td>\n",
       "      <td>0.086621</td>\n",
       "      <td>0.170772</td>\n",
       "      <td>0.070020</td>\n",
       "      <td>0.095257</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.015920</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.013599</td>\n",
       "      <td>0.015663</td>\n",
       "      <td>0.008305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.088039</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>0.064675</td>\n",
       "      <td>0.064894</td>\n",
       "      <td>0.074808</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.026477</td>\n",
       "      <td>0.126996</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.003256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.183376</td>\n",
       "      <td>0.023416</td>\n",
       "      <td>0.084722</td>\n",
       "      <td>0.225365</td>\n",
       "      <td>0.057707</td>\n",
       "      <td>0.040954</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.021510</td>\n",
       "      <td>0.080025</td>\n",
       "      <td>0.058221</td>\n",
       "      <td>0.020870</td>\n",
       "      <td>0.027794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.171143</td>\n",
       "      <td>0.071667</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>0.046897</td>\n",
       "      <td>0.031246</td>\n",
       "      <td>0.059392</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.007547</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.001415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.100105</td>\n",
       "      <td>0.032906</td>\n",
       "      <td>0.038482</td>\n",
       "      <td>0.147695</td>\n",
       "      <td>0.079915</td>\n",
       "      <td>0.057597</td>\n",
       "      <td>0.013058</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.016884</td>\n",
       "      <td>0.030483</td>\n",
       "      <td>0.692458</td>\n",
       "      <td>0.016825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.136375</td>\n",
       "      <td>0.031044</td>\n",
       "      <td>0.076937</td>\n",
       "      <td>0.179066</td>\n",
       "      <td>0.161210</td>\n",
       "      <td>0.047519</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>0.014234</td>\n",
       "      <td>0.030943</td>\n",
       "      <td>0.116676</td>\n",
       "      <td>0.026162</td>\n",
       "      <td>0.020893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.064761</td>\n",
       "      <td>0.034348</td>\n",
       "      <td>0.021603</td>\n",
       "      <td>0.047098</td>\n",
       "      <td>0.031167</td>\n",
       "      <td>0.017307</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.023566</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.004829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.122729</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.078212</td>\n",
       "      <td>0.184213</td>\n",
       "      <td>0.009748</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>0.014068</td>\n",
       "      <td>0.011820</td>\n",
       "      <td>0.005137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.106310</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.047447</td>\n",
       "      <td>0.115014</td>\n",
       "      <td>0.080254</td>\n",
       "      <td>0.052485</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>0.007745</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.023467</td>\n",
       "      <td>0.058822</td>\n",
       "      <td>0.057344</td>\n",
       "      <td>0.018216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.163807</td>\n",
       "      <td>0.026799</td>\n",
       "      <td>0.142801</td>\n",
       "      <td>0.219892</td>\n",
       "      <td>0.158371</td>\n",
       "      <td>0.033492</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>0.015696</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>0.129354</td>\n",
       "      <td>0.050691</td>\n",
       "      <td>0.008672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.079120</td>\n",
       "      <td>0.026704</td>\n",
       "      <td>0.055886</td>\n",
       "      <td>0.050065</td>\n",
       "      <td>0.036738</td>\n",
       "      <td>0.014796</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.041832</td>\n",
       "      <td>0.148116</td>\n",
       "      <td>0.008693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.090658</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.029377</td>\n",
       "      <td>0.069507</td>\n",
       "      <td>0.183646</td>\n",
       "      <td>0.095169</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.013619</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.001571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.266518</td>\n",
       "      <td>0.072537</td>\n",
       "      <td>0.151274</td>\n",
       "      <td>0.131176</td>\n",
       "      <td>0.159583</td>\n",
       "      <td>0.064325</td>\n",
       "      <td>0.012214</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.039225</td>\n",
       "      <td>0.100064</td>\n",
       "      <td>0.086502</td>\n",
       "      <td>0.008364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.126867</td>\n",
       "      <td>0.018983</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.149204</td>\n",
       "      <td>0.066643</td>\n",
       "      <td>0.020265</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.007683</td>\n",
       "      <td>0.092507</td>\n",
       "      <td>0.062234</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>0.012076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.089198</td>\n",
       "      <td>0.019292</td>\n",
       "      <td>0.064408</td>\n",
       "      <td>0.271573</td>\n",
       "      <td>0.123992</td>\n",
       "      <td>0.043398</td>\n",
       "      <td>0.017654</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.012178</td>\n",
       "      <td>0.021733</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>0.006076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.127436</td>\n",
       "      <td>0.008553</td>\n",
       "      <td>0.037755</td>\n",
       "      <td>0.084579</td>\n",
       "      <td>0.029504</td>\n",
       "      <td>0.056464</td>\n",
       "      <td>0.006620</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.006833</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.113823</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.043901</td>\n",
       "      <td>0.012436</td>\n",
       "      <td>0.056403</td>\n",
       "      <td>0.086985</td>\n",
       "      <td>0.179277</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.017277</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.077173</td>\n",
       "      <td>0.013615</td>\n",
       "      <td>0.004486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.112005</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.169651</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>0.038004</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>0.024223</td>\n",
       "      <td>0.107050</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>0.007660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.075884</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.029550</td>\n",
       "      <td>0.120509</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.049722</td>\n",
       "      <td>0.040821</td>\n",
       "      <td>0.183076</td>\n",
       "      <td>0.009958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.172255</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.073867</td>\n",
       "      <td>0.209784</td>\n",
       "      <td>0.174143</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.018220</td>\n",
       "      <td>0.021184</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.044326</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>0.010207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.288846</td>\n",
       "      <td>0.023425</td>\n",
       "      <td>0.219445</td>\n",
       "      <td>0.314111</td>\n",
       "      <td>0.129780</td>\n",
       "      <td>0.065010</td>\n",
       "      <td>0.070463</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>0.006451</td>\n",
       "      <td>0.010899</td>\n",
       "      <td>0.062045</td>\n",
       "      <td>0.010348</td>\n",
       "      <td>0.005106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.027265</td>\n",
       "      <td>0.071332</td>\n",
       "      <td>0.129028</td>\n",
       "      <td>0.172884</td>\n",
       "      <td>0.047575</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>0.008293</td>\n",
       "      <td>0.014505</td>\n",
       "      <td>0.037821</td>\n",
       "      <td>0.042391</td>\n",
       "      <td>0.067736</td>\n",
       "      <td>0.010332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.176904</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>0.095462</td>\n",
       "      <td>0.302188</td>\n",
       "      <td>0.088456</td>\n",
       "      <td>0.079101</td>\n",
       "      <td>0.012116</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>0.034706</td>\n",
       "      <td>0.136725</td>\n",
       "      <td>0.015292</td>\n",
       "      <td>0.003911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.044926</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.017259</td>\n",
       "      <td>0.051252</td>\n",
       "      <td>0.057826</td>\n",
       "      <td>0.009985</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.006195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.061313</td>\n",
       "      <td>0.005052</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.056553</td>\n",
       "      <td>0.018764</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.003644</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>0.012014</td>\n",
       "      <td>0.020010</td>\n",
       "      <td>0.005356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.135888</td>\n",
       "      <td>0.019852</td>\n",
       "      <td>0.085141</td>\n",
       "      <td>0.194923</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.027055</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.017733</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.019051</td>\n",
       "      <td>0.004118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.095507</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.111227</td>\n",
       "      <td>0.031639</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>0.289102</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.224268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.213274</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>0.044987</td>\n",
       "      <td>0.056162</td>\n",
       "      <td>0.086015</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.002782</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.193059</td>\n",
       "      <td>0.026891</td>\n",
       "      <td>0.050224</td>\n",
       "      <td>0.167565</td>\n",
       "      <td>0.049721</td>\n",
       "      <td>0.046105</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.061720</td>\n",
       "      <td>0.051581</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>0.004259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.103743</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.086286</td>\n",
       "      <td>0.111026</td>\n",
       "      <td>0.048438</td>\n",
       "      <td>0.049903</td>\n",
       "      <td>0.048594</td>\n",
       "      <td>0.006302</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.021364</td>\n",
       "      <td>0.039796</td>\n",
       "      <td>0.001376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.096233</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.055244</td>\n",
       "      <td>0.097785</td>\n",
       "      <td>0.073930</td>\n",
       "      <td>0.041620</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.009778</td>\n",
       "      <td>0.038274</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.018260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.179024</td>\n",
       "      <td>0.015268</td>\n",
       "      <td>0.096274</td>\n",
       "      <td>0.195495</td>\n",
       "      <td>0.061545</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.015358</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>0.018844</td>\n",
       "      <td>0.030816</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>0.007091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.131213</td>\n",
       "      <td>0.011207</td>\n",
       "      <td>0.031342</td>\n",
       "      <td>0.076775</td>\n",
       "      <td>0.080091</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.070456</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.012274</td>\n",
       "      <td>0.035541</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.010837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.160797</td>\n",
       "      <td>0.033629</td>\n",
       "      <td>0.065971</td>\n",
       "      <td>0.208971</td>\n",
       "      <td>0.061821</td>\n",
       "      <td>0.016902</td>\n",
       "      <td>0.008365</td>\n",
       "      <td>0.016117</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.011929</td>\n",
       "      <td>0.049216</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.012546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.648245</td>\n",
       "      <td>0.082499</td>\n",
       "      <td>0.208519</td>\n",
       "      <td>0.430048</td>\n",
       "      <td>0.091672</td>\n",
       "      <td>0.239653</td>\n",
       "      <td>0.029099</td>\n",
       "      <td>0.015121</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.018023</td>\n",
       "      <td>0.077873</td>\n",
       "      <td>0.029871</td>\n",
       "      <td>0.004523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.288893</td>\n",
       "      <td>0.328532</td>\n",
       "      <td>0.098680</td>\n",
       "      <td>0.207167</td>\n",
       "      <td>0.620405</td>\n",
       "      <td>0.388302</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.011916</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>0.004386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.106409</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>0.057460</td>\n",
       "      <td>0.122503</td>\n",
       "      <td>0.057820</td>\n",
       "      <td>0.043998</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>0.035399</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.066948</td>\n",
       "      <td>0.009748</td>\n",
       "      <td>0.003765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.124193</td>\n",
       "      <td>0.058428</td>\n",
       "      <td>0.078131</td>\n",
       "      <td>0.165491</td>\n",
       "      <td>0.149048</td>\n",
       "      <td>0.041766</td>\n",
       "      <td>0.024136</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.042836</td>\n",
       "      <td>0.107097</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>0.004156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96455</th>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.022851</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.012418</td>\n",
       "      <td>0.014701</td>\n",
       "      <td>0.080458</td>\n",
       "      <td>0.002309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96456</th>\n",
       "      <td>0.100828</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.111878</td>\n",
       "      <td>0.081956</td>\n",
       "      <td>0.039853</td>\n",
       "      <td>0.021610</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.017537</td>\n",
       "      <td>0.054956</td>\n",
       "      <td>0.241967</td>\n",
       "      <td>0.003983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96457</th>\n",
       "      <td>0.055190</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.023584</td>\n",
       "      <td>0.084495</td>\n",
       "      <td>0.016307</td>\n",
       "      <td>0.034529</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.008776</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.077974</td>\n",
       "      <td>0.026258</td>\n",
       "      <td>0.008356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96458</th>\n",
       "      <td>0.078085</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.077335</td>\n",
       "      <td>0.154360</td>\n",
       "      <td>0.121988</td>\n",
       "      <td>0.029239</td>\n",
       "      <td>0.050551</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.037135</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.088334</td>\n",
       "      <td>0.006752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96459</th>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.059477</td>\n",
       "      <td>0.105412</td>\n",
       "      <td>0.011788</td>\n",
       "      <td>0.019832</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.018042</td>\n",
       "      <td>0.012604</td>\n",
       "      <td>0.005551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96460</th>\n",
       "      <td>0.162488</td>\n",
       "      <td>0.029440</td>\n",
       "      <td>0.211075</td>\n",
       "      <td>0.189530</td>\n",
       "      <td>0.070380</td>\n",
       "      <td>0.029964</td>\n",
       "      <td>0.050147</td>\n",
       "      <td>0.023580</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>0.208233</td>\n",
       "      <td>0.042613</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.001641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96461</th>\n",
       "      <td>0.084114</td>\n",
       "      <td>0.007861</td>\n",
       "      <td>0.035721</td>\n",
       "      <td>0.042248</td>\n",
       "      <td>0.028850</td>\n",
       "      <td>0.023190</td>\n",
       "      <td>0.019254</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.071533</td>\n",
       "      <td>0.001820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96462</th>\n",
       "      <td>0.131416</td>\n",
       "      <td>0.008456</td>\n",
       "      <td>0.117133</td>\n",
       "      <td>0.454376</td>\n",
       "      <td>0.107708</td>\n",
       "      <td>0.038559</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>0.058450</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.012598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96463</th>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.009157</td>\n",
       "      <td>0.259776</td>\n",
       "      <td>0.292472</td>\n",
       "      <td>0.071204</td>\n",
       "      <td>0.037703</td>\n",
       "      <td>0.025146</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.236645</td>\n",
       "      <td>0.157444</td>\n",
       "      <td>0.008808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96464</th>\n",
       "      <td>0.339494</td>\n",
       "      <td>0.070778</td>\n",
       "      <td>0.144121</td>\n",
       "      <td>0.232484</td>\n",
       "      <td>0.062182</td>\n",
       "      <td>0.106777</td>\n",
       "      <td>0.011331</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.039483</td>\n",
       "      <td>0.039038</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96465</th>\n",
       "      <td>0.259846</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>0.076901</td>\n",
       "      <td>0.164975</td>\n",
       "      <td>0.066760</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>0.012775</td>\n",
       "      <td>0.016426</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.044546</td>\n",
       "      <td>0.002376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96466</th>\n",
       "      <td>0.076998</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.113796</td>\n",
       "      <td>0.216172</td>\n",
       "      <td>0.067206</td>\n",
       "      <td>0.071570</td>\n",
       "      <td>0.007279</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.030116</td>\n",
       "      <td>0.015425</td>\n",
       "      <td>0.007184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96467</th>\n",
       "      <td>0.029697</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.070445</td>\n",
       "      <td>0.108971</td>\n",
       "      <td>0.103451</td>\n",
       "      <td>0.019171</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.914535</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96468</th>\n",
       "      <td>0.125997</td>\n",
       "      <td>0.010058</td>\n",
       "      <td>0.095175</td>\n",
       "      <td>0.217537</td>\n",
       "      <td>0.100306</td>\n",
       "      <td>0.050595</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.024971</td>\n",
       "      <td>0.166416</td>\n",
       "      <td>0.017266</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96469</th>\n",
       "      <td>0.144709</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.105790</td>\n",
       "      <td>0.230870</td>\n",
       "      <td>0.059233</td>\n",
       "      <td>0.020035</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>0.068827</td>\n",
       "      <td>0.041950</td>\n",
       "      <td>0.006663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96470</th>\n",
       "      <td>0.092860</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.117124</td>\n",
       "      <td>0.385942</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.053264</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.005394</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.004579</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>0.042763</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96471</th>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.272577</td>\n",
       "      <td>0.446513</td>\n",
       "      <td>0.151988</td>\n",
       "      <td>0.190867</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>0.026337</td>\n",
       "      <td>0.025921</td>\n",
       "      <td>0.010939</td>\n",
       "      <td>0.385818</td>\n",
       "      <td>0.312074</td>\n",
       "      <td>0.018841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96472</th>\n",
       "      <td>0.118523</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>0.103764</td>\n",
       "      <td>0.142064</td>\n",
       "      <td>0.041914</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>0.021962</td>\n",
       "      <td>0.011431</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.102482</td>\n",
       "      <td>0.113253</td>\n",
       "      <td>0.010329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96473</th>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.129827</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.074117</td>\n",
       "      <td>0.055041</td>\n",
       "      <td>0.011288</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>0.094312</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.010591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96474</th>\n",
       "      <td>0.197135</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>0.122630</td>\n",
       "      <td>0.264209</td>\n",
       "      <td>0.128880</td>\n",
       "      <td>0.034262</td>\n",
       "      <td>0.029311</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>0.012283</td>\n",
       "      <td>0.057888</td>\n",
       "      <td>0.080602</td>\n",
       "      <td>0.016641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96475</th>\n",
       "      <td>0.045430</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.039211</td>\n",
       "      <td>0.055469</td>\n",
       "      <td>0.020506</td>\n",
       "      <td>0.030371</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.021226</td>\n",
       "      <td>0.039056</td>\n",
       "      <td>0.130307</td>\n",
       "      <td>0.007631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96476</th>\n",
       "      <td>0.403847</td>\n",
       "      <td>0.041947</td>\n",
       "      <td>0.182834</td>\n",
       "      <td>0.683317</td>\n",
       "      <td>0.057793</td>\n",
       "      <td>0.059243</td>\n",
       "      <td>0.013792</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.012442</td>\n",
       "      <td>0.074096</td>\n",
       "      <td>0.038957</td>\n",
       "      <td>0.146171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96477</th>\n",
       "      <td>0.052230</td>\n",
       "      <td>0.040830</td>\n",
       "      <td>0.031276</td>\n",
       "      <td>0.051278</td>\n",
       "      <td>0.022352</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.238392</td>\n",
       "      <td>0.164396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96478</th>\n",
       "      <td>0.122340</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>0.071325</td>\n",
       "      <td>0.148980</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.036480</td>\n",
       "      <td>0.010618</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.006622</td>\n",
       "      <td>0.021643</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.161209</td>\n",
       "      <td>0.005599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96479</th>\n",
       "      <td>0.036959</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.037593</td>\n",
       "      <td>0.144861</td>\n",
       "      <td>0.072356</td>\n",
       "      <td>0.022032</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.023158</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96480</th>\n",
       "      <td>0.109544</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>0.097144</td>\n",
       "      <td>0.144152</td>\n",
       "      <td>0.139650</td>\n",
       "      <td>0.117369</td>\n",
       "      <td>0.005962</td>\n",
       "      <td>0.016857</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>0.037282</td>\n",
       "      <td>0.103445</td>\n",
       "      <td>0.045452</td>\n",
       "      <td>0.009075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96481</th>\n",
       "      <td>0.094099</td>\n",
       "      <td>0.005725</td>\n",
       "      <td>0.058384</td>\n",
       "      <td>0.188585</td>\n",
       "      <td>0.068799</td>\n",
       "      <td>0.055878</td>\n",
       "      <td>0.024599</td>\n",
       "      <td>0.012818</td>\n",
       "      <td>0.005378</td>\n",
       "      <td>0.008196</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.012676</td>\n",
       "      <td>0.002659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96482</th>\n",
       "      <td>0.177797</td>\n",
       "      <td>0.010534</td>\n",
       "      <td>0.092779</td>\n",
       "      <td>0.160535</td>\n",
       "      <td>0.071541</td>\n",
       "      <td>0.048665</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.007503</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.050779</td>\n",
       "      <td>0.105569</td>\n",
       "      <td>0.007703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96483</th>\n",
       "      <td>0.133153</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.105485</td>\n",
       "      <td>0.205019</td>\n",
       "      <td>0.043391</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>0.011659</td>\n",
       "      <td>0.006609</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.023823</td>\n",
       "      <td>0.014509</td>\n",
       "      <td>0.001388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96484</th>\n",
       "      <td>0.025321</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.014991</td>\n",
       "      <td>0.030430</td>\n",
       "      <td>0.031688</td>\n",
       "      <td>0.032249</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.026995</td>\n",
       "      <td>0.013520</td>\n",
       "      <td>0.003666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96485</th>\n",
       "      <td>0.130408</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.151921</td>\n",
       "      <td>0.157597</td>\n",
       "      <td>0.119185</td>\n",
       "      <td>0.017441</td>\n",
       "      <td>0.015597</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>0.034586</td>\n",
       "      <td>0.054697</td>\n",
       "      <td>0.013853</td>\n",
       "      <td>0.012776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96486</th>\n",
       "      <td>0.072063</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.047503</td>\n",
       "      <td>0.182637</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>0.008020</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>0.015297</td>\n",
       "      <td>0.003280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96487</th>\n",
       "      <td>0.291261</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>0.086485</td>\n",
       "      <td>0.168126</td>\n",
       "      <td>0.024663</td>\n",
       "      <td>0.021354</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.032602</td>\n",
       "      <td>0.017670</td>\n",
       "      <td>0.007870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96488</th>\n",
       "      <td>0.199091</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>0.096251</td>\n",
       "      <td>0.153075</td>\n",
       "      <td>0.141469</td>\n",
       "      <td>0.052932</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.012810</td>\n",
       "      <td>0.042710</td>\n",
       "      <td>0.082239</td>\n",
       "      <td>0.012839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96489</th>\n",
       "      <td>0.079683</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.059411</td>\n",
       "      <td>0.177855</td>\n",
       "      <td>0.022559</td>\n",
       "      <td>0.038207</td>\n",
       "      <td>0.015093</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.042665</td>\n",
       "      <td>0.009960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96490</th>\n",
       "      <td>0.096914</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.049603</td>\n",
       "      <td>0.087801</td>\n",
       "      <td>0.035788</td>\n",
       "      <td>0.011566</td>\n",
       "      <td>0.010449</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.019129</td>\n",
       "      <td>0.083872</td>\n",
       "      <td>0.072764</td>\n",
       "      <td>0.007668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96491</th>\n",
       "      <td>0.094880</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.073259</td>\n",
       "      <td>0.166182</td>\n",
       "      <td>0.057913</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>0.011037</td>\n",
       "      <td>0.008549</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.018706</td>\n",
       "      <td>0.036295</td>\n",
       "      <td>0.014187</td>\n",
       "      <td>0.027587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96492</th>\n",
       "      <td>0.082763</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>0.100109</td>\n",
       "      <td>0.279134</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.015373</td>\n",
       "      <td>0.015913</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96493</th>\n",
       "      <td>0.148817</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.075586</td>\n",
       "      <td>0.074171</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>0.016385</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.049311</td>\n",
       "      <td>0.052394</td>\n",
       "      <td>0.008050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96494</th>\n",
       "      <td>0.054083</td>\n",
       "      <td>0.005801</td>\n",
       "      <td>0.029631</td>\n",
       "      <td>0.066616</td>\n",
       "      <td>0.037535</td>\n",
       "      <td>0.060031</td>\n",
       "      <td>0.006378</td>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.055668</td>\n",
       "      <td>0.077533</td>\n",
       "      <td>0.022255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96495</th>\n",
       "      <td>0.085397</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.073916</td>\n",
       "      <td>0.162073</td>\n",
       "      <td>0.040694</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>0.041213</td>\n",
       "      <td>0.011619</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.059575</td>\n",
       "      <td>0.027447</td>\n",
       "      <td>0.013842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96496</th>\n",
       "      <td>0.197707</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.124933</td>\n",
       "      <td>0.158012</td>\n",
       "      <td>0.132633</td>\n",
       "      <td>0.020660</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>0.020183</td>\n",
       "      <td>0.012324</td>\n",
       "      <td>0.001364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96497</th>\n",
       "      <td>0.048417</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>0.055606</td>\n",
       "      <td>0.012315</td>\n",
       "      <td>0.017074</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.006902</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>0.024881</td>\n",
       "      <td>0.004168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96498</th>\n",
       "      <td>0.083290</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.039505</td>\n",
       "      <td>0.188802</td>\n",
       "      <td>0.041454</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.008239</td>\n",
       "      <td>0.038397</td>\n",
       "      <td>0.022753</td>\n",
       "      <td>0.004964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96499</th>\n",
       "      <td>0.062126</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.037749</td>\n",
       "      <td>0.067867</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.001705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96500</th>\n",
       "      <td>0.067225</td>\n",
       "      <td>0.013179</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.094097</td>\n",
       "      <td>0.026133</td>\n",
       "      <td>0.026046</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96501</th>\n",
       "      <td>0.111815</td>\n",
       "      <td>0.005823</td>\n",
       "      <td>0.046987</td>\n",
       "      <td>0.296217</td>\n",
       "      <td>0.112525</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.288713</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.002316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96502</th>\n",
       "      <td>0.075847</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.058082</td>\n",
       "      <td>0.120682</td>\n",
       "      <td>0.048806</td>\n",
       "      <td>0.014919</td>\n",
       "      <td>0.011828</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.011403</td>\n",
       "      <td>0.051934</td>\n",
       "      <td>0.027272</td>\n",
       "      <td>0.003768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96503</th>\n",
       "      <td>0.068047</td>\n",
       "      <td>0.005803</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.097637</td>\n",
       "      <td>0.015018</td>\n",
       "      <td>0.011306</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.024994</td>\n",
       "      <td>0.036939</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.009986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96504</th>\n",
       "      <td>0.054334</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>0.054952</td>\n",
       "      <td>0.057805</td>\n",
       "      <td>0.025957</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.013535</td>\n",
       "      <td>0.025197</td>\n",
       "      <td>0.090526</td>\n",
       "      <td>0.008842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96505 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         130123    130125    130129    130131    140307    140313    140316  \\\n",
       "0      0.040232  0.006860  0.025696  0.067452  0.032766  0.009906  0.004553   \n",
       "1      0.076402  0.013645  0.045617  0.084973  0.040603  0.011788  0.010483   \n",
       "2      0.241824  0.030543  0.132799  0.228855  0.078287  0.054810  0.008997   \n",
       "3      0.109612  0.009601  0.064301  0.294879  0.040440  0.044881  0.010482   \n",
       "4      0.148123  0.009579  0.065051  0.177052  0.119795  0.041511  0.011092   \n",
       "5      0.110663  0.021188  0.080869  0.116072  0.106700  0.028532  0.011348   \n",
       "6      0.157866  0.010783  0.074565  0.174603  0.192951  0.029467  0.011864   \n",
       "7      0.137579  0.035408  0.085052  0.300336  0.137301  0.021337  0.004689   \n",
       "8      0.109786  0.008904  0.089285  0.179042  0.183370  0.035798  0.004544   \n",
       "9      0.107374  0.025556  0.051660  0.219676  0.158542  0.139714  0.013758   \n",
       "10     0.101286  0.039677  0.022023  0.160185  0.036040  0.033209  0.010502   \n",
       "11     0.109366  0.010478  0.073806  0.163040  0.032267  0.039163  0.016528   \n",
       "12     0.183691  0.012937  0.086621  0.170772  0.070020  0.095257  0.015437   \n",
       "13     0.088039  0.010157  0.064675  0.064894  0.074808  0.037319  0.020142   \n",
       "14     0.183376  0.023416  0.084722  0.225365  0.057707  0.040954  0.007004   \n",
       "15     0.171143  0.071667  0.031954  0.046897  0.031246  0.059392  0.002912   \n",
       "16     0.100105  0.032906  0.038482  0.147695  0.079915  0.057597  0.013058   \n",
       "17     0.136375  0.031044  0.076937  0.179066  0.161210  0.047519  0.015653   \n",
       "18     0.064761  0.034348  0.021603  0.047098  0.031167  0.017307  0.005540   \n",
       "19     0.122729  0.010555  0.047288  0.078212  0.184213  0.009748  0.004230   \n",
       "20     0.106310  0.014948  0.047447  0.115014  0.080254  0.052485  0.019982   \n",
       "21     0.163807  0.026799  0.142801  0.219892  0.158371  0.033492  0.013032   \n",
       "22     0.079120  0.026704  0.055886  0.050065  0.036738  0.014796  0.014660   \n",
       "23     0.090658  0.007189  0.029377  0.069507  0.183646  0.095169  0.004438   \n",
       "24     0.266518  0.072537  0.151274  0.131176  0.159583  0.064325  0.012214   \n",
       "25     0.126867  0.018983  0.069767  0.149204  0.066643  0.020265  0.011676   \n",
       "26     0.089198  0.019292  0.064408  0.271573  0.123992  0.043398  0.017654   \n",
       "27     0.127436  0.008553  0.037755  0.084579  0.029504  0.056464  0.006620   \n",
       "28     0.043901  0.012436  0.056403  0.086985  0.179277  0.009242  0.004045   \n",
       "29     0.112005  0.012982  0.029891  0.169651  0.102212  0.038004  0.007546   \n",
       "30     0.075884  0.009787  0.029550  0.120509  0.032183  0.021030  0.006403   \n",
       "31     0.172255  0.021657  0.073867  0.209784  0.174143  0.042269  0.018220   \n",
       "32     0.288846  0.023425  0.219445  0.314111  0.129780  0.065010  0.070463   \n",
       "33     0.123718  0.027265  0.071332  0.129028  0.172884  0.047575  0.017604   \n",
       "34     0.176904  0.012741  0.095462  0.302188  0.088456  0.079101  0.012116   \n",
       "35     0.044926  0.007939  0.017259  0.051252  0.057826  0.009985  0.003474   \n",
       "36     0.061313  0.005052  0.019307  0.056553  0.018764  0.015900  0.003644   \n",
       "37     0.135888  0.019852  0.085141  0.194923  0.027036  0.027055  0.010966   \n",
       "38     0.095507  0.004824  0.016304  0.111227  0.031639  0.004929  0.002034   \n",
       "39     0.213274  0.011737  0.044987  0.056162  0.086015  0.029514  0.013585   \n",
       "40     0.193059  0.026891  0.050224  0.167565  0.049721  0.046105  0.011356   \n",
       "41     0.103743  0.014118  0.086286  0.111026  0.048438  0.049903  0.048594   \n",
       "42     0.096233  0.013986  0.055244  0.097785  0.073930  0.041620  0.009806   \n",
       "43     0.179024  0.015268  0.096274  0.195495  0.061545  0.085000  0.015358   \n",
       "44     0.131213  0.011207  0.031342  0.076775  0.080091  0.098938  0.070456   \n",
       "45     0.160797  0.033629  0.065971  0.208971  0.061821  0.016902  0.008365   \n",
       "46     0.648245  0.082499  0.208519  0.430048  0.091672  0.239653  0.029099   \n",
       "47     0.288893  0.328532  0.098680  0.207167  0.620405  0.388302  0.007948   \n",
       "48     0.106409  0.029525  0.057460  0.122503  0.057820  0.043998  0.010160   \n",
       "49     0.124193  0.058428  0.078131  0.165491  0.149048  0.041766  0.024136   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "96455  0.031513  0.002777  0.009627  0.022851  0.013877  0.004696  0.002274   \n",
       "96456  0.100828  0.008161  0.111878  0.081956  0.039853  0.021610  0.006304   \n",
       "96457  0.055190  0.002317  0.023584  0.084495  0.016307  0.034529  0.004684   \n",
       "96458  0.078085  0.010685  0.077335  0.154360  0.121988  0.029239  0.050551   \n",
       "96459  0.225500  0.007309  0.059477  0.105412  0.011788  0.019832  0.006517   \n",
       "96460  0.162488  0.029440  0.211075  0.189530  0.070380  0.029964  0.050147   \n",
       "96461  0.084114  0.007861  0.035721  0.042248  0.028850  0.023190  0.019254   \n",
       "96462  0.131416  0.008456  0.117133  0.454376  0.107708  0.038559  0.007665   \n",
       "96463  0.261364  0.009157  0.259776  0.292472  0.071204  0.037703  0.025146   \n",
       "96464  0.339494  0.070778  0.144121  0.232484  0.062182  0.106777  0.011331   \n",
       "96465  0.259846  0.007723  0.076901  0.164975  0.066760  0.012808  0.012775   \n",
       "96466  0.076998  0.008593  0.113796  0.216172  0.067206  0.071570  0.007279   \n",
       "96467  0.029697  0.003266  0.070445  0.108971  0.103451  0.019171  0.007919   \n",
       "96468  0.125997  0.010058  0.095175  0.217537  0.100306  0.050595  0.010953   \n",
       "96469  0.144709  0.017474  0.105790  0.230870  0.059233  0.020035  0.009098   \n",
       "96470  0.092860  0.004136  0.117124  0.385942  0.015300  0.053264  0.003109   \n",
       "96471  0.151465  0.008281  0.272577  0.446513  0.151988  0.190867  0.081001   \n",
       "96472  0.118523  0.008940  0.103764  0.142064  0.041914  0.031463  0.021962   \n",
       "96473  0.157355  0.007977  0.129827  0.240968  0.074117  0.055041  0.011288   \n",
       "96474  0.197135  0.019378  0.122630  0.264209  0.128880  0.034262  0.029311   \n",
       "96475  0.045430  0.004489  0.039211  0.055469  0.020506  0.030371  0.017257   \n",
       "96476  0.403847  0.041947  0.182834  0.683317  0.057793  0.059243  0.013792   \n",
       "96477  0.052230  0.040830  0.031276  0.051278  0.022352  0.011303  0.003059   \n",
       "96478  0.122340  0.009163  0.071325  0.148980  0.040268  0.036480  0.010618   \n",
       "96479  0.036959  0.005927  0.037593  0.144861  0.072356  0.022032  0.005027   \n",
       "96480  0.109544  0.018876  0.097144  0.144152  0.139650  0.117369  0.005962   \n",
       "96481  0.094099  0.005725  0.058384  0.188585  0.068799  0.055878  0.024599   \n",
       "96482  0.177797  0.010534  0.092779  0.160535  0.071541  0.048665  0.011154   \n",
       "96483  0.133153  0.012127  0.105485  0.205019  0.043391  0.013711  0.011659   \n",
       "96484  0.025321  0.001773  0.014991  0.030430  0.031688  0.032249  0.011594   \n",
       "96485  0.130408  0.008242  0.151921  0.157597  0.119185  0.017441  0.015597   \n",
       "96486  0.072063  0.005116  0.047503  0.182637  0.020089  0.020751  0.007908   \n",
       "96487  0.291261  0.005590  0.086485  0.168126  0.024663  0.021354  0.004834   \n",
       "96488  0.199091  0.016473  0.096251  0.153075  0.141469  0.052932  0.006745   \n",
       "96489  0.079683  0.008843  0.059411  0.177855  0.022559  0.038207  0.015093   \n",
       "96490  0.096914  0.005068  0.049603  0.087801  0.035788  0.011566  0.010449   \n",
       "96491  0.094880  0.006797  0.073259  0.166182  0.057913  0.019266  0.011037   \n",
       "96492  0.082763  0.011113  0.038269  0.100109  0.279134  0.036009  0.004897   \n",
       "96493  0.148817  0.007703  0.075586  0.074171  0.023098  0.016385  0.012125   \n",
       "96494  0.054083  0.005801  0.029631  0.066616  0.037535  0.060031  0.006378   \n",
       "96495  0.085397  0.005188  0.073916  0.162073  0.040694  0.088479  0.041213   \n",
       "96496  0.197707  0.003429  0.124933  0.158012  0.132633  0.020660  0.015099   \n",
       "96497  0.048417  0.003868  0.031540  0.055606  0.012315  0.017074  0.003558   \n",
       "96498  0.083290  0.006811  0.039505  0.188802  0.041454  0.018790  0.009490   \n",
       "96499  0.062126  0.008029  0.037749  0.067867  0.008404  0.004988  0.001932   \n",
       "96500  0.067225  0.013179  0.034371  0.094097  0.026133  0.026046  0.005376   \n",
       "96501  0.111815  0.005823  0.046987  0.296217  0.112525  0.023625  0.004802   \n",
       "96502  0.075847  0.005322  0.058082  0.120682  0.048806  0.014919  0.011828   \n",
       "96503  0.068047  0.005803  0.048828  0.097637  0.015018  0.011306  0.002686   \n",
       "96504  0.054334  0.007735  0.054952  0.057805  0.025957  0.015214  0.007692   \n",
       "\n",
       "         140317    140321    140501    140505    140641    140691  \n",
       "0      0.008393  0.009025  0.018505  0.030068  0.025426  0.007609  \n",
       "1      0.005316  0.005111  0.254897  0.153382  0.037649  0.003275  \n",
       "2      0.007156  0.008335  0.028201  0.031096  0.022021  0.004850  \n",
       "3      0.011213  0.015895  0.030360  0.030806  0.018678  0.006798  \n",
       "4      0.006895  0.010336  0.011060  0.061648  0.048735  0.036030  \n",
       "5      0.006263  0.029676  0.095509  0.040977  0.013331  0.005558  \n",
       "6      0.004071  0.006119  0.010505  0.054553  0.019914  0.006475  \n",
       "7      0.007803  0.020294  0.044519  0.060856  0.035409  0.009408  \n",
       "8      0.005758  0.021637  0.172807  0.124942  0.056038  0.004552  \n",
       "9      0.013936  0.006682  0.020089  0.038128  0.021894  0.003825  \n",
       "10     0.006677  0.006061  0.088070  0.097055  0.030676  0.014967  \n",
       "11     0.008318  0.003053  0.067530  0.077171  0.013175  0.014167  \n",
       "12     0.015920  0.015065  0.011036  0.013599  0.015663  0.008305  \n",
       "13     0.004402  0.004211  0.026477  0.126996  0.017219  0.003256  \n",
       "14     0.005920  0.021510  0.080025  0.058221  0.020870  0.027794  \n",
       "15     0.002606  0.001583  0.007547  0.005599  0.003078  0.001415  \n",
       "16     0.011692  0.003714  0.016884  0.030483  0.692458  0.016825  \n",
       "17     0.010080  0.014234  0.030943  0.116676  0.026162  0.020893  \n",
       "18     0.012677  0.002736  0.008842  0.023566  0.009264  0.004829  \n",
       "19     0.005037  0.004298  0.008574  0.014068  0.011820  0.005137  \n",
       "20     0.007745  0.006829  0.023467  0.058822  0.057344  0.018216  \n",
       "21     0.015696  0.013648  0.019365  0.129354  0.050691  0.008672  \n",
       "22     0.003539  0.002877  0.004244  0.041832  0.148116  0.008693  \n",
       "23     0.004933  0.001478  0.013619  0.012300  0.010842  0.001571  \n",
       "24     0.017071  0.007328  0.039225  0.100064  0.086502  0.008364  \n",
       "25     0.007965  0.007683  0.092507  0.062234  0.036790  0.012076  \n",
       "26     0.008663  0.006799  0.012178  0.021733  0.020251  0.006076  \n",
       "27     0.002312  0.006833  0.002365  0.113823  0.013769  0.001378  \n",
       "28     0.017277  0.002358  0.009963  0.077173  0.013615  0.004486  \n",
       "29     0.005227  0.007167  0.024223  0.107050  0.040179  0.007660  \n",
       "30     0.008223  0.010641  0.049722  0.040821  0.183076  0.009958  \n",
       "31     0.021184  0.005455  0.014200  0.044326  0.008698  0.010207  \n",
       "32     0.006102  0.006451  0.010899  0.062045  0.010348  0.005106  \n",
       "33     0.008293  0.014505  0.037821  0.042391  0.067736  0.010332  \n",
       "34     0.007816  0.007024  0.034706  0.136725  0.015292  0.003911  \n",
       "35     0.005327  0.003742  0.023629  0.029349  0.014200  0.006195  \n",
       "36     0.004533  0.006490  0.006878  0.012014  0.020010  0.005356  \n",
       "37     0.009553  0.010116  0.017733  0.023008  0.019051  0.004118  \n",
       "38     0.004013  0.009579  0.289102  0.020268  0.013800  0.224268  \n",
       "39     0.005903  0.002782  0.002241  0.007911  0.005095  0.005952  \n",
       "40     0.014451  0.003099  0.061720  0.051581  0.013646  0.004259  \n",
       "41     0.006302  0.003511  0.011335  0.021364  0.039796  0.001376  \n",
       "42     0.007390  0.013220  0.009778  0.038274  0.007068  0.018260  \n",
       "43     0.007604  0.011711  0.018844  0.030816  0.021552  0.007091  \n",
       "44     0.004985  0.001660  0.012274  0.035541  0.013703  0.010837  \n",
       "45     0.016117  0.004285  0.011929  0.049216  0.017461  0.012546  \n",
       "46     0.015121  0.006127  0.018023  0.077873  0.029871  0.004523  \n",
       "47     0.004195  0.000812  0.002350  0.011916  0.012299  0.004386  \n",
       "48     0.035399  0.002957  0.013707  0.066948  0.009748  0.003765  \n",
       "49     0.008605  0.003467  0.042836  0.107097  0.022022  0.004156  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "96455  0.002006  0.001070  0.012418  0.014701  0.080458  0.002309  \n",
       "96456  0.006018  0.003758  0.017537  0.054956  0.241967  0.003983  \n",
       "96457  0.008776  0.001708  0.024865  0.077974  0.026258  0.008356  \n",
       "96458  0.012984  0.006861  0.037135  0.023833  0.088334  0.006752  \n",
       "96459  0.004212  0.001868  0.002781  0.018042  0.012604  0.005551  \n",
       "96460  0.023580  0.003365  0.208233  0.042613  0.014961  0.001641  \n",
       "96461  0.003866  0.008546  0.009081  0.045781  0.071533  0.001820  \n",
       "96462  0.012185  0.003115  0.007040  0.058450  0.010685  0.012598  \n",
       "96463  0.007817  0.008163  0.015870  0.236645  0.157444  0.008808  \n",
       "96464  0.005894  0.001151  0.004214  0.039483  0.039038  0.007302  \n",
       "96465  0.016426  0.002765  0.002744  0.014684  0.044546  0.002376  \n",
       "96466  0.005455  0.002751  0.012707  0.030116  0.015425  0.007184  \n",
       "96467  0.003565  0.001185  0.003253  0.914535  0.016863  0.001543  \n",
       "96468  0.009820  0.003784  0.024971  0.166416  0.017266  0.019494  \n",
       "96469  0.010350  0.003149  0.017313  0.068827  0.041950  0.006663  \n",
       "96470  0.005394  0.000972  0.004579  0.010410  0.042763  0.001288  \n",
       "96471  0.026337  0.025921  0.010939  0.385818  0.312074  0.018841  \n",
       "96472  0.011431  0.003109  0.011806  0.102482  0.113253  0.010329  \n",
       "96473  0.005250  0.003071  0.011777  0.094312  0.108400  0.010591  \n",
       "96474  0.012482  0.012579  0.012283  0.057888  0.080602  0.016641  \n",
       "96475  0.011161  0.006753  0.021226  0.039056  0.130307  0.007631  \n",
       "96476  0.008011  0.006487  0.012442  0.074096  0.038957  0.146171  \n",
       "96477  0.006438  0.002058  0.005011  0.006478  0.238392  0.164396  \n",
       "96478  0.008116  0.006622  0.021643  0.020912  0.161209  0.005599  \n",
       "96479  0.006860  0.002830  0.004293  0.023158  0.005063  0.001028  \n",
       "96480  0.016857  0.003989  0.037282  0.103445  0.045452  0.009075  \n",
       "96481  0.012818  0.005378  0.008196  0.021201  0.012676  0.002659  \n",
       "96482  0.007503  0.002754  0.015064  0.050779  0.105569  0.007703  \n",
       "96483  0.006609  0.001509  0.012263  0.023823  0.014509  0.001388  \n",
       "96484  0.003570  0.001718  0.003117  0.026995  0.013520  0.003666  \n",
       "96485  0.010827  0.004315  0.034586  0.054697  0.013853  0.012776  \n",
       "96486  0.008020  0.001774  0.002889  0.022264  0.015297  0.003280  \n",
       "96487  0.005581  0.002443  0.004012  0.032602  0.017670  0.007870  \n",
       "96488  0.029843  0.006222  0.012810  0.042710  0.082239  0.012839  \n",
       "96489  0.014110  0.004388  0.006597  0.022525  0.042665  0.009960  \n",
       "96490  0.007560  0.005514  0.019129  0.083872  0.072764  0.007668  \n",
       "96491  0.008549  0.001671  0.018706  0.036295  0.014187  0.027587  \n",
       "96492  0.006181  0.006291  0.004042  0.015373  0.015913  0.001832  \n",
       "96493  0.004849  0.008757  0.012300  0.049311  0.052394  0.008050  \n",
       "96494  0.005456  0.002960  0.017490  0.055668  0.077533  0.022255  \n",
       "96495  0.011619  0.006470  0.033678  0.059575  0.027447  0.013842  \n",
       "96496  0.007492  0.003419  0.007070  0.020183  0.012324  0.001364  \n",
       "96497  0.006902  0.000995  0.003608  0.021559  0.024881  0.004168  \n",
       "96498  0.005372  0.002529  0.008239  0.038397  0.022753  0.004964  \n",
       "96499  0.002788  0.000755  0.006864  0.003033  0.004897  0.001705  \n",
       "96500  0.004495  0.004675  0.006997  0.009403  0.012321  0.003750  \n",
       "96501  0.004014  0.001174  0.008956  0.288713  0.008323  0.002316  \n",
       "96502  0.004264  0.002299  0.011403  0.051934  0.027272  0.003768  \n",
       "96503  0.002916  0.002401  0.024994  0.036939  0.016970  0.009986  \n",
       "96504  0.007138  0.001674  0.013535  0.025197  0.090526  0.008842  \n",
       "\n",
       "[96505 rows x 13 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.rename(columns=lambda s: s+\"_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plus_pred = pd.concat([test,y_pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_id</th>\n",
       "      <th>130123_pred</th>\n",
       "      <th>130125_pred</th>\n",
       "      <th>130129_pred</th>\n",
       "      <th>130131_pred</th>\n",
       "      <th>140307_pred</th>\n",
       "      <th>140313_pred</th>\n",
       "      <th>140316_pred</th>\n",
       "      <th>140317_pred</th>\n",
       "      <th>140321_pred</th>\n",
       "      <th>140501_pred</th>\n",
       "      <th>140505_pred</th>\n",
       "      <th>140641_pred</th>\n",
       "      <th>140691_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3rcdjjRyw9qSh6NcZMKSX</td>\n",
       "      <td>0.040232</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>0.067452</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.018505</td>\n",
       "      <td>0.030068</td>\n",
       "      <td>0.025426</td>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y56uzwqQzynHYZ4bDfLPp5</td>\n",
       "      <td>0.076402</td>\n",
       "      <td>0.013645</td>\n",
       "      <td>0.045617</td>\n",
       "      <td>0.084973</td>\n",
       "      <td>0.040603</td>\n",
       "      <td>0.011788</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.254897</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.037649</td>\n",
       "      <td>0.003275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xDMdFERmC7CD9yFvyvKJnh</td>\n",
       "      <td>0.241824</td>\n",
       "      <td>0.030543</td>\n",
       "      <td>0.132799</td>\n",
       "      <td>0.228855</td>\n",
       "      <td>0.078287</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.031096</td>\n",
       "      <td>0.022021</td>\n",
       "      <td>0.004850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzZENdjz7SvUQkGZV45afF</td>\n",
       "      <td>0.109612</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.294879</td>\n",
       "      <td>0.040440</td>\n",
       "      <td>0.044881</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.030360</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.006798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zFWkhHbLYJ9Fh5kUvCrx4g</td>\n",
       "      <td>0.148123</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>0.177052</td>\n",
       "      <td>0.119795</td>\n",
       "      <td>0.041511</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.061648</td>\n",
       "      <td>0.048735</td>\n",
       "      <td>0.036030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              purchase_id  130123_pred  130125_pred  130129_pred  130131_pred  \\\n",
       "0  C3rcdjjRyw9qSh6NcZMKSX     0.040232     0.006860     0.025696     0.067452   \n",
       "1  Y56uzwqQzynHYZ4bDfLPp5     0.076402     0.013645     0.045617     0.084973   \n",
       "2  xDMdFERmC7CD9yFvyvKJnh     0.241824     0.030543     0.132799     0.228855   \n",
       "3  zzZENdjz7SvUQkGZV45afF     0.109612     0.009601     0.064301     0.294879   \n",
       "4  zFWkhHbLYJ9Fh5kUvCrx4g     0.148123     0.009579     0.065051     0.177052   \n",
       "\n",
       "   140307_pred  140313_pred  140316_pred  140317_pred  140321_pred  \\\n",
       "0     0.032766     0.009906     0.004553     0.008393     0.009025   \n",
       "1     0.040603     0.011788     0.010483     0.005316     0.005111   \n",
       "2     0.078287     0.054810     0.008997     0.007156     0.008335   \n",
       "3     0.040440     0.044881     0.010482     0.011213     0.015895   \n",
       "4     0.119795     0.041511     0.011092     0.006895     0.010336   \n",
       "\n",
       "   140501_pred  140505_pred  140641_pred  140691_pred  \n",
       "0     0.018505     0.030068     0.025426     0.007609  \n",
       "1     0.254897     0.153382     0.037649     0.003275  \n",
       "2     0.028201     0.031096     0.022021     0.004850  \n",
       "3     0.030360     0.030806     0.018678     0.006798  \n",
       "4     0.011060     0.061648     0.048735     0.036030  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_plus_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_plus_oof_pred.columns.tolist()\n",
    "\n",
    "for i in target_list:\n",
    "    features.remove(i)\n",
    "    \n",
    "features.remove('purchase_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['130123_pred',\n",
       " '130125_pred',\n",
       " '130129_pred',\n",
       " '130131_pred',\n",
       " '140307_pred',\n",
       " '140313_pred',\n",
       " '140316_pred',\n",
       " '140317_pred',\n",
       " '140321_pred',\n",
       " '140501_pred',\n",
       " '140505_pred',\n",
       " '140641_pred',\n",
       " '140691_pred']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### 1 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.711365\tvalid_1's auc: 0.490449\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.525443\tvalid_1's auc: 0.504114\n",
      "Partial score of fold 0 is: 0.504113595358015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.716301\tvalid_1's auc: 0.494107\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.521273\tvalid_1's auc: 0.500291\n",
      "Partial score of fold 1 is: 0.5002914793586253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.714438\tvalid_1's auc: 0.502063\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.568916\tvalid_1's auc: 0.507051\n",
      "Partial score of fold 2 is: 0.5070506701606304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.706436\tvalid_1's auc: 0.501584\n",
      "[200]\ttraining's auc: 0.775717\tvalid_1's auc: 0.504797\n",
      "[300]\ttraining's auc: 0.816386\tvalid_1's auc: 0.506274\n",
      "Early stopping, best iteration is:\n",
      "[298]\ttraining's auc: 0.815829\tvalid_1's auc: 0.506515\n",
      "Partial score of fold 3 is: 0.5065153953050734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.710432\tvalid_1's auc: 0.495071\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.607684\tvalid_1's auc: 0.505632\n",
      "Partial score of fold 4 is: 0.5056322558443189\n",
      "AUC score is:  0.50543818752728\n",
      "################### 2 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.892483\tvalid_1's auc: 0.480301\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.603733\tvalid_1's auc: 0.50409\n",
      "Partial score of fold 0 is: 0.5040903801931698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.89922\tvalid_1's auc: 0.521537\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.596196\tvalid_1's auc: 0.528399\n",
      "Partial score of fold 1 is: 0.5283993702973002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.88819\tvalid_1's auc: 0.472313\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.620415\tvalid_1's auc: 0.502167\n",
      "Partial score of fold 2 is: 0.5021672245549826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.889111\tvalid_1's auc: 0.48885\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.571918\tvalid_1's auc: 0.498683\n",
      "Partial score of fold 3 is: 0.49868290428978673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.895182\tvalid_1's auc: 0.519331\n",
      "[200]\ttraining's auc: 0.949559\tvalid_1's auc: 0.517291\n",
      "Early stopping, best iteration is:\n",
      "[154]\ttraining's auc: 0.930526\tvalid_1's auc: 0.525878\n",
      "Partial score of fold 4 is: 0.5258784287229796\n",
      "AUC score is:  0.5122395653804042\n",
      "################### 3 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.772663\tvalid_1's auc: 0.497731\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.552522\tvalid_1's auc: 0.510818\n",
      "Partial score of fold 0 is: 0.5108179709223913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.764986\tvalid_1's auc: 0.500078\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.563551\tvalid_1's auc: 0.506977\n",
      "Partial score of fold 1 is: 0.5069771813832511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.77197\tvalid_1's auc: 0.501968\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.567136\tvalid_1's auc: 0.509642\n",
      "Partial score of fold 2 is: 0.5096421021800185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.774771\tvalid_1's auc: 0.497694\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.623593\tvalid_1's auc: 0.511952\n",
      "Partial score of fold 3 is: 0.5119518688240565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.772736\tvalid_1's auc: 0.49291\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.55714\tvalid_1's auc: 0.509632\n",
      "Partial score of fold 4 is: 0.5096318722779319\n",
      "AUC score is:  0.5097079125981342\n",
      "################### 4 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.707566\tvalid_1's auc: 0.503899\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's auc: 0.660956\tvalid_1's auc: 0.510036\n",
      "Partial score of fold 0 is: 0.5100355815995469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.707534\tvalid_1's auc: 0.494026\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.524777\tvalid_1's auc: 0.500272\n",
      "Partial score of fold 1 is: 0.5002720193242941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.705813\tvalid_1's auc: 0.501008\n",
      "[200]\ttraining's auc: 0.765682\tvalid_1's auc: 0.506392\n",
      "[300]\ttraining's auc: 0.805485\tvalid_1's auc: 0.507636\n",
      "Early stopping, best iteration is:\n",
      "[268]\ttraining's auc: 0.794171\tvalid_1's auc: 0.509111\n",
      "Partial score of fold 2 is: 0.5091110363408639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.710414\tvalid_1's auc: 0.501077\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's auc: 0.665164\tvalid_1's auc: 0.50575\n",
      "Partial score of fold 3 is: 0.5057502561975864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.705961\tvalid_1's auc: 0.503898\n",
      "[200]\ttraining's auc: 0.767095\tvalid_1's auc: 0.503087\n",
      "Early stopping, best iteration is:\n",
      "[139]\ttraining's auc: 0.736818\tvalid_1's auc: 0.504736\n",
      "Partial score of fold 4 is: 0.5047359098372359\n",
      "AUC score is:  0.5063788485452988\n",
      "################### 5 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.736654\tvalid_1's auc: 0.503892\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.628487\tvalid_1's auc: 0.517724\n",
      "Partial score of fold 0 is: 0.5177242173317859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.735617\tvalid_1's auc: 0.491153\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.537819\tvalid_1's auc: 0.504001\n",
      "Partial score of fold 1 is: 0.5040013540457696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.733813\tvalid_1's auc: 0.504541\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.574351\tvalid_1's auc: 0.514353\n",
      "Partial score of fold 2 is: 0.514353089753891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.741001\tvalid_1's auc: 0.506421\n",
      "[200]\ttraining's auc: 0.80644\tvalid_1's auc: 0.50697\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's auc: 0.791414\tvalid_1's auc: 0.509634\n",
      "Partial score of fold 3 is: 0.5096341073659089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.741577\tvalid_1's auc: 0.495392\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's auc: 0.663085\tvalid_1's auc: 0.501372\n",
      "Partial score of fold 4 is: 0.501372156692168\n",
      "AUC score is:  0.5091186741319937\n",
      "################### 6 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.79601\tvalid_1's auc: 0.501047\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's auc: 0.680864\tvalid_1's auc: 0.513825\n",
      "Partial score of fold 0 is: 0.513824882947299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.798991\tvalid_1's auc: 0.488755\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.56472\tvalid_1's auc: 0.507225\n",
      "Partial score of fold 1 is: 0.5072252952502565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.799464\tvalid_1's auc: 0.503091\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.602733\tvalid_1's auc: 0.506345\n",
      "Partial score of fold 2 is: 0.506345415222311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.788967\tvalid_1's auc: 0.502785\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's auc: 0.711539\tvalid_1's auc: 0.509614\n",
      "Partial score of fold 3 is: 0.5096135599283352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.798554\tvalid_1's auc: 0.51186\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's auc: 0.714292\tvalid_1's auc: 0.515263\n",
      "Partial score of fold 4 is: 0.5152630886132453\n",
      "AUC score is:  0.5111818597138876\n",
      "################### 7 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.900429\tvalid_1's auc: 0.487637\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.632616\tvalid_1's auc: 0.516863\n",
      "Partial score of fold 0 is: 0.5168627972031646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.88953\tvalid_1's auc: 0.489451\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's auc: 0.658092\tvalid_1's auc: 0.509008\n",
      "Partial score of fold 1 is: 0.5090081924049759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.891928\tvalid_1's auc: 0.496854\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.57982\tvalid_1's auc: 0.507078\n",
      "Partial score of fold 2 is: 0.5070779915783081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.892124\tvalid_1's auc: 0.511407\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's auc: 0.765857\tvalid_1's auc: 0.522615\n",
      "Partial score of fold 3 is: 0.5226146468825532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.898976\tvalid_1's auc: 0.514773\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's auc: 0.836715\tvalid_1's auc: 0.527406\n",
      "Partial score of fold 4 is: 0.5274059298991591\n",
      "AUC score is:  0.5189006261926927\n",
      "################### 8 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.898955\tvalid_1's auc: 0.529612\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's auc: 0.884504\tvalid_1's auc: 0.533481\n",
      "Partial score of fold 0 is: 0.533480907581627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.902533\tvalid_1's auc: 0.513926\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's auc: 0.78927\tvalid_1's auc: 0.528974\n",
      "Partial score of fold 1 is: 0.5289744054948052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.894304\tvalid_1's auc: 0.525654\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's auc: 0.805101\tvalid_1's auc: 0.534405\n",
      "Partial score of fold 2 is: 0.5344048161051507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.905243\tvalid_1's auc: 0.507912\n",
      "[200]\ttraining's auc: 0.95613\tvalid_1's auc: 0.516863\n",
      "[300]\ttraining's auc: 0.978764\tvalid_1's auc: 0.507384\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttraining's auc: 0.957192\tvalid_1's auc: 0.517449\n",
      "Partial score of fold 3 is: 0.5174485598431025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.898955\tvalid_1's auc: 0.526817\n",
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's auc: 0.888031\tvalid_1's auc: 0.530245\n",
      "Partial score of fold 4 is: 0.5302448919458876\n",
      "AUC score is:  0.5269429767941406\n",
      "################### 9 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.934198\tvalid_1's auc: 0.47716\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's auc: 0.686632\tvalid_1's auc: 0.513829\n",
      "Partial score of fold 0 is: 0.5138290583215179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.929895\tvalid_1's auc: 0.490784\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.65959\tvalid_1's auc: 0.511417\n",
      "Partial score of fold 1 is: 0.5114167652222155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.931313\tvalid_1's auc: 0.499427\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.873647\tvalid_1's auc: 0.506732\n",
      "Partial score of fold 2 is: 0.506732494564344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.937615\tvalid_1's auc: 0.489025\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.619614\tvalid_1's auc: 0.534007\n",
      "Partial score of fold 3 is: 0.5340074119704136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.943907\tvalid_1's auc: 0.481163\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.720323\tvalid_1's auc: 0.515942\n",
      "Partial score of fold 4 is: 0.5159423031521088\n",
      "AUC score is:  0.5157846813429187\n",
      "################### 10 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.795178\tvalid_1's auc: 0.484418\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.539031\tvalid_1's auc: 0.507307\n",
      "Partial score of fold 0 is: 0.5073067949673336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.798431\tvalid_1's auc: 0.496873\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.655639\tvalid_1's auc: 0.504995\n",
      "Partial score of fold 1 is: 0.5049949682513837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.803775\tvalid_1's auc: 0.49514\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's auc: 0.590779\tvalid_1's auc: 0.50934\n",
      "Partial score of fold 2 is: 0.5093397618551321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.798409\tvalid_1's auc: 0.50555\n",
      "[200]\ttraining's auc: 0.872596\tvalid_1's auc: 0.502102\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's auc: 0.80173\tvalid_1's auc: 0.506287\n",
      "Partial score of fold 3 is: 0.5062870409137918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.791173\tvalid_1's auc: 0.490305\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.550741\tvalid_1's auc: 0.509118\n",
      "Partial score of fold 4 is: 0.5091180056224021\n",
      "AUC score is:  0.5067210042557714\n",
      "################### 11 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.749166\tvalid_1's auc: 0.498972\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's auc: 0.676754\tvalid_1's auc: 0.505547\n",
      "Partial score of fold 0 is: 0.5055467918393024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.749732\tvalid_1's auc: 0.49313\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.536928\tvalid_1's auc: 0.4961\n",
      "Partial score of fold 1 is: 0.4961003643312207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.750991\tvalid_1's auc: 0.499894\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.633412\tvalid_1's auc: 0.505505\n",
      "Partial score of fold 2 is: 0.5055048637650791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.753551\tvalid_1's auc: 0.505077\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's auc: 0.615843\tvalid_1's auc: 0.509022\n",
      "Partial score of fold 3 is: 0.509021647011549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.75615\tvalid_1's auc: 0.501273\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.627514\tvalid_1's auc: 0.505473\n",
      "Partial score of fold 4 is: 0.5054731858718272\n",
      "AUC score is:  0.5059236309990233\n",
      "################### 12 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.846732\tvalid_1's auc: 0.510504\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's auc: 0.786514\tvalid_1's auc: 0.516613\n",
      "Partial score of fold 0 is: 0.5166128257878814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.846894\tvalid_1's auc: 0.497846\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.56624\tvalid_1's auc: 0.506208\n",
      "Partial score of fold 1 is: 0.506208043278192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.851311\tvalid_1's auc: 0.492923\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.630193\tvalid_1's auc: 0.518178\n",
      "Partial score of fold 2 is: 0.5181782823533782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.850683\tvalid_1's auc: 0.520127\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.628813\tvalid_1's auc: 0.523982\n",
      "Partial score of fold 3 is: 0.5239824629831064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.85126\tvalid_1's auc: 0.512161\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.712418\tvalid_1's auc: 0.519067\n",
      "Partial score of fold 4 is: 0.5190671317127424\n",
      "AUC score is:  0.5147655776912725\n",
      "################### 13 ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.866355\tvalid_1's auc: 0.487111\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.709123\tvalid_1's auc: 0.510422\n",
      "Partial score of fold 0 is: 0.5104218357616972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.866613\tvalid_1's auc: 0.501435\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's auc: 0.790698\tvalid_1's auc: 0.508412\n",
      "Partial score of fold 1 is: 0.508412334323906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.867368\tvalid_1's auc: 0.495368\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.660654\tvalid_1's auc: 0.510554\n",
      "Partial score of fold 2 is: 0.5105543428037324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.85557\tvalid_1's auc: 0.492854\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.551834\tvalid_1's auc: 0.515653\n",
      "Partial score of fold 3 is: 0.5156528211272262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.864942\tvalid_1's auc: 0.511347\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.648858\tvalid_1's auc: 0.516193\n",
      "Partial score of fold 4 is: 0.5161926971726096\n",
      "AUC score is:  0.5095297519169797\n"
     ]
    }
   ],
   "source": [
    "train_lgb = train_plus_oof_pred.copy()\n",
    "test_lgb = test_plus_pred.copy()\n",
    "\n",
    "print(\"################### 1 ###################\")\n",
    "lgb_model_130123 = Lgb_Model(train_lgb, test_lgb, features, '130123')\n",
    "print(\"################### 2 ###################\")\n",
    "lgb_model_130125 = Lgb_Model(train_lgb, test_lgb, features, '130125')\n",
    "print(\"################### 3 ###################\")\n",
    "lgb_model_130129 = Lgb_Model(train_lgb, test_lgb, features, '130129')\n",
    "print(\"################### 4 ###################\")\n",
    "lgb_model_130131 = Lgb_Model(train_lgb, test_lgb, features, '130131')\n",
    "print(\"################### 5 ###################\")\n",
    "lgb_model_140307 = Lgb_Model(train_lgb, test_lgb, features, '140307')\n",
    "print(\"################### 6 ###################\")\n",
    "lgb_model_140313 = Lgb_Model(train_lgb, test_lgb, features, '140313')\n",
    "print(\"################### 7 ###################\")\n",
    "lgb_model_140316 = Lgb_Model(train_lgb, test_lgb, features, '140316')\n",
    "print(\"################### 8 ###################\")\n",
    "lgb_model_140317 = Lgb_Model(train_lgb, test_lgb, features, '140317')\n",
    "print(\"################### 9 ###################\")\n",
    "lgb_model_140321 = Lgb_Model(train_lgb, test_lgb, features, '140321')\n",
    "print(\"################### 10 ###################\")\n",
    "lgb_model_140501 = Lgb_Model(train_lgb, test_lgb, features, '140501')\n",
    "print(\"################### 11 ###################\")\n",
    "lgb_model_140505 = Lgb_Model(train_lgb, test_lgb, features, '140505')\n",
    "print(\"################### 12 ###################\")\n",
    "lgb_model_140641 = Lgb_Model(train_lgb, test_lgb, features, '140641')\n",
    "print(\"################### 13 ###################\")\n",
    "lgb_model_140691 = Lgb_Model(train_lgb, test_lgb, features, '140691')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"130123_AUC: {lgb_model_130123.score}\")\n",
    "# print(f\"130125_AUC: {lgb_model_130125.score}\")\n",
    "# print(f\"130129_AUC: {lgb_model_130129.score}\")\n",
    "# print(f\"130131_AUC: {lgb_model_130131.score}\")\n",
    "# print(f\"140307_AUC: {lgb_model_140307.score}\")\n",
    "# print(f\"140313_AUC: {lgb_model_140313.score}\")\n",
    "# print(f\"140316_AUC: {lgb_model_140316.score}\")\n",
    "# print(f\"140317_AUC: {lgb_model_140317.score}\")\n",
    "# print(f\"140321_AUC: {lgb_model_140321.score}\")\n",
    "# print(f\"140501_AUC: {lgb_model_140501.score}\")\n",
    "# print(f\"140505_AUC: {lgb_model_140505.score}\")\n",
    "# print(f\"140641_AUC: {lgb_model_140641.score}\")\n",
    "# print(f\"140691_AUC: {lgb_model_140691.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130123_AUC: 0.50543818752728\n",
      "130125_AUC: 0.5122395653804042\n",
      "130129_AUC: 0.5097079125981342\n",
      "130131_AUC: 0.5063788485452988\n",
      "140307_AUC: 0.5091186741319937\n",
      "140313_AUC: 0.5111818597138876\n",
      "140316_AUC: 0.5189006261926927\n",
      "140317_AUC: 0.5269429767941406\n",
      "140321_AUC: 0.5157846813429187\n",
      "140501_AUC: 0.5067210042557714\n",
      "140505_AUC: 0.5059236309990233\n",
      "140641_AUC: 0.5147655776912725\n",
      "140691_AUC: 0.5095297519169797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_w = f'/mnt/batch/tasks/shared/LS_root/mounts/clusters/sinchir02/code/Users/td017/kaggle-pipeline/model/{CASE}_lgb_score_second.txt'\n",
    "\n",
    "s1 = f\"130123_AUC: {lgb_model_130123.score}\"\n",
    "s2 = f\"130125_AUC: {lgb_model_130125.score}\"\n",
    "s3 = f\"130129_AUC: {lgb_model_130129.score}\"\n",
    "s4 = f\"130131_AUC: {lgb_model_130131.score}\"\n",
    "s5 = f\"140307_AUC: {lgb_model_140307.score}\"\n",
    "s6 = f\"140313_AUC: {lgb_model_140313.score}\"\n",
    "s7 = f\"140316_AUC: {lgb_model_140316.score}\"\n",
    "s8 = f\"140317_AUC: {lgb_model_140317.score}\"\n",
    "s9 = f\"140321_AUC: {lgb_model_140321.score}\"\n",
    "s10 = f\"140501_AUC: {lgb_model_140501.score}\"\n",
    "s11 = f\"140505_AUC: {lgb_model_140505.score}\"\n",
    "s12 = f\"140641_AUC: {lgb_model_140641.score}\"\n",
    "s13 = f\"140691_AUC: {lgb_model_140691.score}\"\n",
    "\n",
    "with open(path_w, mode='w') as f:\n",
    "    f.write(s1+\"\\n\")\n",
    "    f.write(s2+\"\\n\")\n",
    "    f.write(s3+\"\\n\")\n",
    "    f.write(s4+\"\\n\")\n",
    "    f.write(s5+\"\\n\")\n",
    "    f.write(s6+\"\\n\")\n",
    "    f.write(s7+\"\\n\")\n",
    "    f.write(s8+\"\\n\")\n",
    "    f.write(s9+\"\\n\")\n",
    "    f.write(s10+\"\\n\")\n",
    "    f.write(s11+\"\\n\")\n",
    "    f.write(s12+\"\\n\")\n",
    "    f.write(s13+\"\\n\")\n",
    "\n",
    "with open(path_w) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"130123\"] = lgb_model_130123.y_pred\n",
    "sample_submission[\"130125\"] = lgb_model_130125.y_pred\n",
    "sample_submission[\"130129\"] = lgb_model_130129.y_pred\n",
    "sample_submission[\"130131\"] = lgb_model_130131.y_pred\n",
    "sample_submission[\"140307\"] = lgb_model_140307.y_pred\n",
    "sample_submission[\"140313\"] = lgb_model_140313.y_pred\n",
    "sample_submission[\"140316\"] = lgb_model_140316.y_pred\n",
    "sample_submission[\"140317\"] = lgb_model_140317.y_pred\n",
    "sample_submission[\"140321\"] = lgb_model_140321.y_pred\n",
    "sample_submission[\"140501\"] = lgb_model_140501.y_pred\n",
    "sample_submission[\"140505\"] = lgb_model_140505.y_pred\n",
    "sample_submission[\"140641\"] = lgb_model_140641.y_pred\n",
    "sample_submission[\"140691\"] = lgb_model_140691.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>130123</th>\n",
       "      <th>130125</th>\n",
       "      <th>130129</th>\n",
       "      <th>130131</th>\n",
       "      <th>140307</th>\n",
       "      <th>140313</th>\n",
       "      <th>140316</th>\n",
       "      <th>140317</th>\n",
       "      <th>140321</th>\n",
       "      <th>140501</th>\n",
       "      <th>140505</th>\n",
       "      <th>140641</th>\n",
       "      <th>140691</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125916</td>\n",
       "      <td>0.018495</td>\n",
       "      <td>0.065802</td>\n",
       "      <td>0.151879</td>\n",
       "      <td>0.087172</td>\n",
       "      <td>0.044379</td>\n",
       "      <td>0.016416</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>0.047892</td>\n",
       "      <td>0.076854</td>\n",
       "      <td>0.027323</td>\n",
       "      <td>0.025388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.123771</td>\n",
       "      <td>0.017051</td>\n",
       "      <td>0.064819</td>\n",
       "      <td>0.149858</td>\n",
       "      <td>0.083424</td>\n",
       "      <td>0.044674</td>\n",
       "      <td>0.016716</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.046444</td>\n",
       "      <td>0.073613</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.024517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131856</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.065081</td>\n",
       "      <td>0.157233</td>\n",
       "      <td>0.086959</td>\n",
       "      <td>0.045294</td>\n",
       "      <td>0.015598</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>0.010406</td>\n",
       "      <td>0.045974</td>\n",
       "      <td>0.075453</td>\n",
       "      <td>0.026174</td>\n",
       "      <td>0.024202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.128921</td>\n",
       "      <td>0.019875</td>\n",
       "      <td>0.064486</td>\n",
       "      <td>0.153029</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>0.046551</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>0.011894</td>\n",
       "      <td>0.043774</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>0.025893</td>\n",
       "      <td>0.025456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130108</td>\n",
       "      <td>0.018902</td>\n",
       "      <td>0.064229</td>\n",
       "      <td>0.153422</td>\n",
       "      <td>0.085195</td>\n",
       "      <td>0.046187</td>\n",
       "      <td>0.015740</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.045064</td>\n",
       "      <td>0.075573</td>\n",
       "      <td>0.025852</td>\n",
       "      <td>0.024684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     130123    130125    130129    130131    140307    140313    140316  \\\n",
       "0  0.125916  0.018495  0.065802  0.151879  0.087172  0.044379  0.016416   \n",
       "1  0.123771  0.017051  0.064819  0.149858  0.083424  0.044674  0.016716   \n",
       "2  0.131856  0.018427  0.065081  0.157233  0.086959  0.045294  0.015598   \n",
       "3  0.128921  0.019875  0.064486  0.153029  0.086791  0.046551  0.015550   \n",
       "4  0.130108  0.018902  0.064229  0.153422  0.085195  0.046187  0.015740   \n",
       "\n",
       "     140317    140321    140501    140505    140641    140691  \n",
       "0  0.019307  0.012002  0.047892  0.076854  0.027323  0.025388  \n",
       "1  0.014424  0.009615  0.046444  0.073613  0.025516  0.024517  \n",
       "2  0.011010  0.010406  0.045974  0.075453  0.026174  0.024202  \n",
       "3  0.013263  0.011894  0.043774  0.074882  0.025893  0.025456  \n",
       "4  0.016352  0.009917  0.045064  0.075573  0.025852  0.024684  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(f\"{SAME_PATH}\" + \"/Users/td017/kaggle-pipeline/submission/sub_\" + f\"{CASE}\"+\"_second.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack_df = pd.DataFrame({\n",
    "#   \"130123\" :  lgb_model_130123.oof_pred,\n",
    "#   \"130125\" :  lgb_model_130125.oof_pred,\n",
    "#   \"130129\" :  lgb_model_130129.oof_pred,\n",
    "#   \"130131\" :  lgb_model_130131.oof_pred,\n",
    "#   \"140307\" :  lgb_model_140307.oof_pred,\n",
    "#   \"140313\" :  lgb_model_140313.oof_pred,\n",
    "#   \"140316\" :  lgb_model_140316.oof_pred,\n",
    "#   \"140317\" :  lgb_model_140317.oof_pred,\n",
    "#   \"140321\" :  lgb_model_140321.oof_pred,\n",
    "#   \"140501\" :  lgb_model_140501.oof_pred,\n",
    "#   \"140505\" :  lgb_model_140505.oof_pred,\n",
    "#   \"140641\" :  lgb_model_140641.oof_pred,\n",
    "#   \"140691\" :  lgb_model_140691.oof_pred,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case1_oof_pred = pd.read_csv(f\"{SAME_PATH}\" + \"Users/td017/kaggle-pipeline/submission/sub_case1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
